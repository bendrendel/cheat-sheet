<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithms</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Algorithms</h1>
    <h2>Asymptotic Notation</h2>
    <ul>
        <li>Runtime or time complexity is used to describe how efficient a program is. Since processor speeds vary or perhaps the efficiency of different programming languages, time is not is not usually used, but rather the number of operations the porgram has to perform based on the input.</li>
        <li>Execution Count is a precise count of how many operations/instructions are performed by a function or program expressed as a function of the input size N.</li>
        <li>Asymptotic Notation describes the asymptotic behavior of the execution count is generally an easier and just as useful way to describe runtime.  It eliminates all constants and all but the largest term from the execution count function. So, if a program has a complex polynomial execution count, only the largest power termm will be used with no constant factor.  Or, if a program has multiple sections with different runtimes, only the section with the slowest runtime will be counted since it will have the largest term, i.e. we only care about the part of the program that runs the slowest when using asympmtotic notation.  e.g. if a program prints all numbers from 1 to N (linear runtime N) and then divides N by 2 until 1 is reached (logarithmic runtime log N), the execution count would be N + log N, but the asymptotic runtime will be N only since that is the larger term.  There are three variations of asymptotic notation:</li>
        <ul>
            <li>Big Theta (Θ): Θ(N) is used when there is no variablity in the runtime for a given input size N.  For example looping through a list of N items and printing each one always takes N operations and the runtime would be described as Θ(N).  Or, taking an integer N and dividing it in half until you get to 1, will take log2(N) steps, and the runtime would be described as Θ(log N). (Consider the reverse direction, where you multiply 1 by 2, then by 2 again, by 2 again, etc until you hit the input N)</li>
            <li>Big Omega (Ω): Ω(N) is used to describe the best case runtime for input size N, when there is potential variability in the runtime. Instead of Ω(N), sometimes a program will be described as "having a best-case runtime of Θ(N)". For example searcing for a value in a list one item at a time could have best case runtime of 1 (constant runtime) if the item is first in the list.</li>
            <li>Big O (O): O(N) is used to describe the worst case runtime for input size N, when there is potential variability in the runtime. Instead of O(N), sometimes a program will be described as "having a worst-case runtime of Θ(N)". For example searcing for a value in a list one item at a time could have worst case runtime of N (linear runtime) if the item is last in the list.  Most of the time people will only discuss O(N) instead of Θ(N) or Ω(N), because if runtime is variable then the worst case is what matters most, and if runtime is constant then O(N) is the same as Θ(N)</li>
        </ul>
        <li>Common Runtimes from fastest to slowest</li>
        <ul>
            <li>Θ(1) is constant runtime when the number of operations is the same regardless of input size</li>
            <li>Θ(log N) is logarithmic runtime common in good search algorithms</li>
            <li>Θ(N) is linear runtime common when iterating through data</li>
            <li>Θ(N*logN) is common in sorting algorithms</li>
            <li>Θ(N^2) is quadratic or polynomial for higher powers runtime common when iterating multi-dimensional data sets and nested iterations</li>
            <li>Θ(2^N) is exponential runtime common with recursive functions</li>
            <li>Θ(N!) is factorial runtime common when generating all permutations of some input</li>
        </ul>
        <li>Space Complexity is used to describe how much memory is needed for a program or function and is also described using asymptotic notation.  It describes the amount of space needed in relation to input size, and generally does not include counting the input itself.  So if a function takes 10 arrays as input but simply prints "Hello world" in the body, it has a space complexity of O(1).  A function that takes an array as input and outputs a new array that doubles each value in the input array would have a space complexity of O(N) because the output array size grows linearly as the input size grows. However, if that same function mutated the input array instead of creating a new array it would have space complexity of O(1) because no new spaces is needed to perform the function.  Time complexity is generally more important to consider, but second to that it is important to optimize for space.  Space complexity might be considered when breaking a tie between two programs that have the same time complexity.</li>
    </ul>

    <h2>Recursion</h2>
    <ul>
        <li>A method of problem-solving that defines a problem in terms of itself.  A function definition that invokes itself in the body of the function.</li>
        <li>A recursive function always has two parts, one that runs when the base case condition is statisfied, and on that runs when the recursive case condition is satisfied:</li>
        <ul>
            <li>Base Case Check: Checking if the input is simplified enough to take base case action.  There may be multiple conditions that make up the base case: Base Case 1, Base Case 2, etc.  E.g. if searching for something in a list, the base case could be that it is found, or that the end of the list was reached.</li>
            <ul>
                <li>Base Case Action: Some action taken, often returning a value. Could be multiple different actions if there are multiple base case conditions, e.g. returning a found value vs returning null if the end of the list is reached.</li>
            </ul>
            <li>Recursive Case Check: Checking if recursive case is met, may be done implicitly simply by the fact that none of the base cases were met.</li>
            <ul>
                <li>Recursive Case Action: The function is invoked with input that is closer to the base case.  Some action is taken often returning a value, which may depend on getting output from the recursively called function</li>
            </ul>
        </ul>
        <li>Each recusive call adds to the call stack, calling the function with a different execution context that gets closer and closer to the base case.  When the base case is reached, the last function call returns, allowing earlier calls to return one by one.</li>
    </ul>

    <h2>Bubble Sort</h2>
    <ul>
        <li>Bubble Sort is a common simple sorting algorithm.  It compares subsequent pairs of adjacent elements in an unsorted list starting with the first two elements and swaps them if the first element is larger than the second.  This forces the largest element in the list
            to end up at the end of the list.  This is then repeated on the remaining unsorted elements, moving the second largest element to the second to last position in the list.  This continues until no more swaps occur indicating the list is sorted.
        </li>
        <li>To swap elements, you'll need a temporary variable to hold one of the elements or a language that supports multiple assignment in one line</li>
        <li>The runtime is O(n^2).  Consider this requires you to make n-1 comparisons (and possible swaps) when you go through a list of length n once.  You have to go through the list making swaps up to n-1 times to get everything ordered.  But each time you go through, the number of comparisons (and possible swaps) needed decreases by one.  Calculating the actual execution count is something like below. Up to n-1 times through the list making comparisons and swaps (x2 operations), but decreasing the number of items by one each round:
        </li>
        <ul>
            <li>
                <pre>
                                    n-1 terms
                    2[(n-1)+(n-2)+(n-3)+...+(n-(n-2))+(n-(n-1))]
                    =2[(n-1)+(n-2)+(n-3)+...+n-(n-2)+n-(n-1)]

                    n even
                    2[(n-1)+(n-2)+...+(n-n/2)+...+n-(n-2)+n-(n-1)]
                    =2[n-n/2+n*(n/2-1)]
                    =2[n(1-1/2+n/2-1)]
                    =2[n(n/2-1/2)]
                    =2[n((n-1)/2)]
                    =n^2-n

                    n odd
                    2[((n-1)/2)*n]
                    =n^2-n                    
                </pre>
            </li>
        </ul>
        <li>Bubble sort doesn't have a great big O, but it has a good big omega (Ω) of Ω(n), because in the best case scenario, the list is already sorted, so the algorithm just loops once through the list making comparisons, realizes no swaps were made and quits.  This makes bubble sort a good choice if you know a list is nearly fully sorted.</li>
        <li>Example implementation with swap helper function:
            <code><pre>
                const swap = (arr, indexOne, indexTwo) => {
                    const temp = arr[indexTwo];
                    arr[indexTwo] = arr[indexOne];
                    arr[indexOne] = temp;
                };

                const bubbleSort = input => {
                    let swapCount = 0
                    let swapping = true;
                    
                    while (swapping) {
                        swapping = false;
                        for (let i = 0; i &lt input.length - 1; i++) {
                            if (input[i] > input[i + 1]) {
                                swap(input, i, i + 1);
                                swapCount++;
                                swapping = true;
                            }
                        }
                    }
                    console.log(`Swapped ${swapCount} times`);
                    return input;
                };
            </pre></code>
        </li>
    </ul>

    <h2>Merge Sort</h2>
    <ul>
        <li>Merge sort is a sorting algorithm that breaks the input down and then rebuilds it</li>
        <ul>
            <li>1. Split: Cut the list in half repeatedly until you are down to single elements</li>
            <li>2. Merge: Now merge the neighboring single elements into two element lists in order from smallest to largest. Continue merging the lists by comparing the leftmost elements and adding the smallest to the merged list, then comparing the remaining leftmost elements and adding the smallest to the merged list, and so on.</li>
        </ul>
        <li>It has runtime O(n*log n) with no variability (doesn't matter if list is already nearly sorted).  It has space complexity of up to O(n)</li>
        <li>Example implementation: 
            <code><pre>
                const mergeSort = (startArray) => {
                    const length = startArray.length;
                    if (length === 1) {
                        return startArray;
                    }
                    
                    const mid = Math.floor(length / 2);
                    const leftArray = startArray.slice(0, mid);
                    const rightArray = startArray.slice(mid, length);

                    return merge(mergeSort(leftArray), mergeSort(rightArray))
                }

                const merge = (leftArray, rightArray) => {
                    const sortedArray = [];
                    while (leftArray.length > 0 && rightArray.length > 0) {
                        if (leftArray[0] &lt rightArray[0]) {
                            sortedArray.push(leftArray.shift());
                        } else {
                            sortedArray.push(rightArray.shift());
                        }
                    }
                
                    return sortedArray.concat(leftArray).concat(rightArray);
                }
            </pre></code>
        </li>
    </ul>

    <h2>Quicksort</h2>
    <ul>
        <li>A recursive sorting method</li>
        <ul>
            <li>1. Partition: Pick a 'pivot element' from the list, then reorder the list into two partitions, one with elements less than the pivot element, and one with elements greater than the pivot.  Elements equal to the pivot could go in either partition.</li>
            <li>2. Recurse: Now partition the partitions from the step above, reordering elements in the list, and continue until the partitions are only one element, meaning the whole list will be sorted</li>

        </ul>
        <li>Picking the pivot element:</li>
        <ul>
            <li>One good way to pick the pivot element is at random instead of say, always picking the first element.  If you always pick the first element, the algorithm would perform badly on lists that are already sorted because one partition would have all the elements and the other would have just one.  Picking at random will mean that no particular data set will perform badly.</li>
            <li>Another good method is to take the median of the first, middle, and last elements in the list, which tends to create uniform partitions</li>
        </ul>
        <li>Runtime</li>
        <ul>
            <li>The worst case runtime is O(n^2) and the average case is O(n*log n).  The worst case is so uncommon that usually it is thought of as O(n*log n)</li>    
            <li>The worst case occurs when the pivot element is always the min or max of the list, and thus each time you partition you end up with everything in one partition, this would require n partitioning steps, and for each partition you would have n comparison and swap steps, making for n^2</li>
            <li>The best case occurs when the pivot element is always the median so you end up with equal partitions each time you partition, this would cut the list in half with each partition and get down to 1 element in log2 N steps.  Then the merge step at each level would be n operations, making for n*log n</li>
        </ul>
        <li>Example implementation:
            <code><pre>
                const quicksort = (array, leftBound = 0, rightBound = array.length - 1) => {
                    if (leftBound &lt rightBound) {
                        const pivotIndex = partition(array, leftBound, rightBound);
                        quicksort(array, leftBound, pivotIndex - 1);
                        quicksort(array, pivotIndex, rightBound);
                    }
                    return array;
                }

                const partition = (array, leftIndex, rightIndex) => {
                    const pivot = array[Math.floor((rightIndex + leftIndex) / 2)];
                    while (leftIndex $lt= rightIndex) {
                        while (array[leftIndex] &lt pivot) {
                            leftIndex++;
                        }
                        while (array[rightIndex] > pivot) {
                            rightIndex--;
                        }
                        if (leftIndex &lt= rightIndex) {
                            swap(array, leftIndex, rightIndex);
                            leftIndex++;
                            rightIndex--;
                        }
                    }
                    return leftIndex;
                }

                const swap = (arr, indexOne, indexTwo) => {
                    const temp = arr[indexTwo];
                    arr[indexTwo] = arr[indexOne];
                    arr[indexOne] = temp;
                }
            </pre></code>
        </li>
    </ul>

    <h2>Binary Search</h2>
    <ul>
        <li>This is a way of searching through a sorted data set.  It gives a runtime of log n by cutting the list in half with each iteration, meaning at worst it would have to cut it in half again and again until it gets down to 1 element which would take log n iterations.  This is instead of a linear search method that would have a runtime of n, when the order of the list is not known or known to be out of order.</li>
        <li>Binary Search Trees are a binary tree data structure, where every left child node is less than or equal to the parent, and every right child node is greater than or equal to the parent.</li>
        <li>It is a recursive algorithm that looks at the middle value of a sorted list.  If it matches what is searched for then it is done, otherwise the same algorithm is applied to the left half of the list if the searched for value is less than the middle value, or it is applied to the right half of the list if the search for value is greater than the middle value.  This is repeated until the search for value is found.</li>
        <li>Example implementation:
            <code><pre>
                const binarySearch = (arr, target) => {
                    let left = 0;
                    let right = arr.length;
                    
                    while (right > left) {
                        const indexToCheck = Math.floor((left + right) / 2);
                        const checking = arr[indexToCheck];
                        console.log(indexToCheck);
                    
                        if (checking === target) {
                            return indexToCheck;
                        } else if (checking &lt target) {
                            left = indexToCheck + 1;
                        } else {
                            right = indexToCheck;
                        }
                    }
                    
                    return null;
                }
            </pre></code>
        </li>
    </ul>

    <h2>Graph Traversal</h2>
    <ul>
        <li>There are many ways to traverse/search through a graph, but depth-first search (DFS) and breadth-first search (BFS) are the two most common general approaches</li>
        <li>The worst-case O(n) runtime for both is that every single vertex is visited and every edge is traversed between those vertices, thus O(vertices+edges).</li>
    </ul>

    <h3>Depth-First Search</h3>
    <ul>
        <li>DFS Continues down a single path until reaching the end.</li>
        <li>It is helpful when determining if a path exists at all between two vertices. Can be used to solve problems that have a singular correct answer (a path between the start state and end state) such as a Soduko puzzle. It is best for long graphs with few connections.</li>
        <li>Perhaps the most common implementation uses recursion (see example implementation below).</li>
        <li>Another implementation uses a list of all visited vertices and a stack of vertices in the current path.  When a vertex is visited it is added to the list of visited vertices.  With every step, we add the visited vertex to the stack. (The stack starts with the vertex we are starting from.) The vertex at the top of the stack is our current vertex, and we pick an adjacent vertex that isn't in the visited list to move to next.  We add that next vertex to the stack (and the list of visited vertices) and repeat until the current vertex has no adjacent unvisited vertices (i.e. all of its adjacent vertices are in the visited list). At this point, we pop off the current vertex from the stack, making the previous vertex the current one, and then try to visit an unvisited vertex from there.  If there are none, then we pop that off the stack and try again.  This is repeated, until some target vertex is found, or the stack is completely empty indicating all vertices have been visited.</li>
        <li>DFS can be used to get a list of all vertices in a graph, but there are several ways of ordering this, commonly:</li>
        <ul>
            <li>Preorder: This is the order in which vertices are added to the visited vertices list.  It lists the vertices as they were visited by the algorithm, starting at the origin and going down each path explored.</li>
            <li>Postorder: This is the order in which vertices are popped from the stack.  So, the first vertex will be at the end of the first path visited by the algorithm, when the algorithm first starts to back up and pop vertices off the stack. The last element will be the vertex you started the algorithm from.  This is the order in which vertices are "fully explored" by the algorithm.</li>
            <li>Reverse-postorder: AKA Topological sort, this is simply the reverse of the postorder list. So the first element of this list will be the vertex the algorithm started from.</li>
        </ul>
        <li>Example implementation, this could be a method on the Graph data structure developed on the data structure page. It uses a recursive implementation instead of the stack implementation. Start is a vertex from a graph (e.g. myGraph.vertices[0])
            <code><pre>
                const depthFirstTraversal = (start, callback, visitedVertices = [start]) => {
                    callback(start);

                    start.edges.forEach((edge) => {
                        const neighbor = edge.end;

                        if (!visitedVertices.includes(neighbor)) {
                            visitedVertices.push(neighbor);
                            depthFirstTraversal(neighbor, callback, visitedVertices);
                        }
                    });
                };
            </pre></code>
        </li>
    </ul>

    <h3>Breadth-First Search</h3>
    <ul>
        <li>BFS checks every neighboring vertex before moving down another level of depth.</li>
        <li>It is helpful when looking for the shortest path between two vertices. It's inefficient if just looking for whether a path exists at all. It is better for densley connected graphs.</li>
        <li>One implementation uses a list of all visited vertices and a queue of vertices.  Whenever a vertex is visited it is add to the list of visited vertices.  With every step, the vertex is added to the queue (the queue starts with the vertex we start from).  The vertex at the head of the queue is the current vertex and each of its adjacent vertices are checked against the visited vertex list, and if they haven't been visited yet, are added to the queue and the list of visited vertices.  Once the current vertex has no more unvisited adjacent vertices, it is dequeued.  Now the process repeats for the new head of the queue.  This repeats until the target vertex is found or the queue is completely empty indicating all vertices have been visited.</li>
        <li>Example, implementation, relying on the graph data structure and the queue data structure developed on the data strucuture page.  This simply prints out vertices as they are visited, but could be modified to accept a callback instead.
            <code><pre>
                const breadthFirstTraversal = (start) => {
                    const visitedVertices = [start];
                    const visitQueue = new Queue();
                    visitQueue.enqueue(start);

                    while(!visitQueue.isEmpty()) {
                        const current = visitQueue.dequeue();

                        console.log(current.data);

                        current.edges.forEach((edge) => {
                            const neighbor = edge.end;

                            if (!visitedVertices.includes(neighbor)) {
                                visitedVertices.push(neighbor);
                                visitQueue.enqueue(neighbor);
                            }
                        });
                    }
                };
            </pre></code>
        </li>
    </ul>

    <h3>Dijkstra's Algorithm</h3>
    <ul>
        <li>This finds the shortest path from a given vertex to every other vertex in a weighted graph, by using BFS and keeping track of the total weight of paths as it goes.</li>
        <li>Helpful for finding the quickest route, or least costly weighted route from one vertex to another in a weighted graph.</li>
    </ul>

    <h2>Sieve of Eratosthenes</h2>
    <ul>
        <li>This is an algorithm to find all prime numbers up to a given limit n.  The algorithm makes a list of all numbers from 2 to n.  Starting with the smallest number, it marks all numbers in the list that are multiples of the smallest number, then moves to the next number and marks all numbers that are multiples of that, until it has gone through the whole list.  Whatever isn't marked is prime.</li>
    </ul>

</body>
</html>