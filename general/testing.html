<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Testing</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Testing</h1>

    <h2>Terminology</h2>
    <ul>
        <li><strong>Software Testing </strong>Assessing the completeness and quality of computer software,
            usually by comparing actual behavior/outcome to expected behavior/outcome.</li>
        <li><strong>Bugs </strong>An error, fault, or flaw making software behave in an unexpected way.</li>
        <li><strong>Manual Testing </strong>Testing done by a human interacting with the system</li>
        <li><strong>Automated Testing </strong>Testing done by software interacting with the system.
            This is faster and more reliable than manual testing and can be done more frequently, 
            often after every significant change to the software.  Developers use test frameworks to organize
            automated tests.</li>
        <li><strong>Test Code </strong>The code written to test software, in contrast to implementation
            code that is the code of the software itself.  Often stored along with implementation code.
            e.g. index.js is the implementation code and in the same folder is index-test.js with test code</li>
        <li><strong>Test Suite </strong>A collection of tests for an application.  A good test suite is maintainable, complete, fast, isolated, reliable, and expressive (MC-FIRE)</li>
        <ul>
            <li>Fast: Fast tests encourage the developer to run tests more often and save time, integration tests generally take longer than unit tests, so a test suite that is heavy on integration tests will be slower</li>
            <li>Complete: A test suite that cover 100% of your code base will give you confidence the program is working correctly</li>
            <li>Reliable: The test suite gives the same output for the same version of the program, i.e. it doesn't fail one time and then pass the next</li>
            <li>Isolated: Tests don't make changes that persist when the test is done running, which could impact other tests or have other unexpected results</li>
            <li>Maintainable: Well-organized so it is easy to add, edit, and remove tests, encouraging completeness</li>
            <li>Expressive: Tests should be descriptive of functionality tested, which effectively documents the purpose of the software</li>
        </ul>
        <li><strong>Documentation </strong>Any content separate from implementation code explaining how it
            works or how to use it.  Tests can be a form of documentation that is unique in that it is
            both readable by humans and computers to confirm it works as described.</li>
        <li><strong>Coverage </strong>Refers to the scope of what is tested. Often testing only covers functionality - i.e. meeting the requirements.  But testing could expand coverage to security, ease of use, and speed</li>
        <ul>
            <li>Code Coverage: This refers to automated tests, and the extent to which they test every area of code</li>
            <li>Test Coverage: This refers to the QA team testing that the software meets the requirements, and the extent to which it tests all the requirements</li>
            <ul>
                <li>Product Coverage: The extent to which all areas of the product are tested</li>
                <li>Risk Coverage: The extent to which possible risks have been tested for</li>
                <li>Requirements Coverage: The extent to which all requirements have been tested</li>
            </ul>
        </ul>
        <li><strong>Regression </strong>When previously tested functionality no longer passes the tests. Can
            happen when new features are implemented.</li>
        <li><strong>Mock Testing </strong>This is when you use a fake version of an internal or external service (e.g. JSON from an API), for testing purposes. This is good to use when possible to save time when running the test.</li>
        <li><strong>Stub Testing </strong>Similar to mock testing, except you only fake some portion of the internal or external service that is specifically needed for the test.  Like mock testing, this is good to use to save time when running the test.  An example is instead of creating an actual error condition by e.g. creating a corrupt file programmatically and then testing what happens when your program tries to read it, you could stub the file system with some functionality that returns the error code for a corrupt file and feed that to your test, instead of actually creating the corrupt file.</li>
    </ul>

    <h2>Test Driven Development (TDD)</h2>
    <p>A development method that writes test code to define program behavior before implementation code, and only writes implementation code in response to failed tests, addressing one failure at a time.</p>
    <p>Phases of test-drive development:</p>
    <ol>
        <li><strong>Add a test</strong> Convert requirements to tests, before code is written</li>
        <li><strong>Run all tests - fail</strong> The new tests should fail because the requirements haven't been met yet</li>
        <li><strong>Write simplest code to pass test</strong> Inelegant, hard-coded is fine</li>
        <li><strong>Run all tests - pass</strong> Revise code until all tests pass</li>
        <li><strong>Refactor</strong> Now make the code more logical, etc.</li>
        <li><strong>Repeat</strong> Repeat this process for every new piece of functionality, generally using unit tests</li>
    </ol>
    <p>This is often simplified as:</p>
    <ol>
        <li><strong>Red: </strong>Write tests to describe how code should behave, tests fail (red) because code doesn't meet requirement</li>
        <li><strong>Green: </strong>Write the minimum possible code to address the test's error message.  Repeat until all tests pass (green).  e.g. the object you are testing doesn't exist (reference error) so you create the object.  Then the method in the object doesn't exist so you create the method.  Then the method returns undefined, so you make it return the hard-coded expected result, now the test passes.</li>
        <li><strong>Refactor: </strong>Refactor code while testing often to ensure it stays green, and backtracking if it doesn't. This includes refactoring both test code and implementation code.  e.g. you may refactor your test code to be broken into setup/exercise/verify phases. You may add an additional test that feeds your method a different input expecting a different result and see that your method fails the test because it is returning the hard-coded expected result of the previous test. This forces you to refactor the implementation code to return a dynamic value based on input.  You might then write a test for edge cases, like providing the wrong data type as input or an empty array as input, and let that fail and guide refactoring of implementation code.  You may also simply refactor implementation code to make it more concise, reduce duplication, use more efficient methods, etc.  You may or may not need to refactor at all in this step, depending on the complexity of the functionality and testing.</li>
    </ol>

    <h2>Testing Heirarchy</h2>
    <p>Testing generally breaks down to Unit Testing, Integration Testing, and UI Testing.  Unit/Component Testing is the fastest and cheapest testing and is done often.  You move up the heirarchy only as previous tests are exhuasted, as higher up tests are slower and more resource expensive.</p>
    <h3>Unit/Component Testing</h3>
    <ul>
        <li>Automated tests written and run by developers, testing small units of code, such as a single class, a function, a module, a method, etc.</li>
        <li>Needs to be used in conjuntion with integration testing, and other tests that test functionality of units working together</li>
        <li>Expect Unit Tests to evolve as the implementation code evolves and implementation details change, unit test code is not as static as higher level tests that test overall functionality that doesn't change as much.</li>
        <li>Can be used as a design-specification, essentially they define what needs to be built and how it should work and can simultaneously be
            used to test if requirements are met.
        </li>
        <li>Unit Testing is fast and cheap, and you should rely heavily on these in your test suite</li>
    </ul>

    <h3>Contract Testing</h3>
    <ul>
        <li>When two services integrate with each other they have expectations about what is received and returned, which can be thought of as a contract between integrated endpoints.</li>
        <li>You can test internal services (microservices architecture) this way also, and use mocks and stubs for the expected returned values from the other system.</li>
    </ul>

    <h3>Integration Testing</h3>
    <ul>
        <li>This tests modules which have been unit tested, and aggregates them to test as a group.  Happens after unit testing,
            but before validation and system testing.
        </li>
        <li>Different methods can be used such as big-bang testing which combines large aggregates of modules, or can systematically
            test smaller aggregates first, building up to larger aggregates.
        </li>
        <li>Integration Testing is slower and more expensive on resources than unit testing, and should only be done once unit tests have been exhuasted.</li>
    </ul>

    <h3>UI Layer Testing</h3>
    <ul>
        <li></li>
    </ul>

    <h2>Mocha</h2>
    <ul>
        <li>Mocha is a javascript test framework, which should be installed using <code>npm install mocha -D</code> in the root directory of a project, to save it to the dev dependcies of the project (-D)</li>
        <li>Once installed, it can be executed from the project root directory with <code>./node_modules/mocha/bin/mocha</code>, however it is more common to set it as the test script
        in the project's package.json by setting <code>"test": "mocha"</code> in the scripts object, so mocha can be executed using <code>npm test</code>. This will run the full test suite instead of having
        to run each test in your test folder individually.  You can put <code>"test": "mocha test/**/*_test.js"</code> to specifically run anything in any folder of the test directory that ends in _test.</li>
        <li>Typically the architecture is you'll have a test folder in the root of your project, with one or more js files containing tests.  You might have an index.js file, and a corresponding index_test.js file in the test folder.  In the index_test.js you'll require in index.js at the top (and index.js will need a module.exports statement) so you can test the functionality in index.js.  You'll also typically require in the assert core module.  You don't have to require in Mocha if its been installed as described above.</li>
        <li>The <code>describe()</code> and <code>it()</code> are mocha methods that provide structure to your test code.  They both accept two arguments, a descriptive string and a callback function.</li>
        <ul>
            <li><code>describe()</code> groups related tests together. the first argument should describe the grouping, and the callback function contains the test code and may include nested
            describe() functions to create subgroups of tests</li>
            <li><code>it()</code> defines the actual tests.  the first argument should describes the functionality tested such as 'returns argument with highest value', and the callback function should
            define the test</li>
            <li>Example code: 
                <code><pre>
                    describe('Math', () => {
                        describe('.max', () => {
                            it('returns the argument with the highest value', () => {
                                // Your test goes here
                            });
                            it('returns -Infinity when no arguments are provided', () => {
                                // Your test goes here
                            });
                        });
                    });
                </pre></code>
            </li>
        </ul>
        <li>The <code>assert</code> library is a core node module, and is commonly used with Mocha to perform a test. It must be imported to the test file with <code>const assert = require('assert');</code></li>
        <ul>
            <li><code>assert.ok()</code> accepts a single conditional statement such as <code>assert.ok(myVar === 3)</code> and throws an AssertionError if condition is false.  This communciates to mocha that a test has failed and mocha then logs it to the console.</li>
            <li><code>assert.equal()</code> and <code>assert.strictEqual()</code> can and should (for a more expressive, easier to read test) be used instead of .ok() when you just want to test if two items are equal, .equal does a loose == comparison and .strictEqual does a strict === comparison. e.g. <code>assert.strictEqual(result, expected)</code> where typically the result/actual is put first and the expected second</li>
            <li><code>assert.deepEqual()</code> should be used when comparing equality between two objects or two arrays. Comparing distinct but identical objects or arrays with == or === will result in false, because the objects/arrays are not in the same location in memory.</li>
            <li>There are other assert methods for specific circumstances that should generally be used instead of the more generic ok() method for more expressive tests that are easier to read</li>     
            <li>Example Code:
                <code><pre>
                    describe('+', () => {
                        it('returns the sum of its arguments', () => {
                            // Write assertion here
                            assert.ok(3 + 4 === 7);
                        });
                    });
                </pre></code>
            </li>
        </ul>
        <li>Test Phases are a good way to organize the test code inside an it statement callback function, to make it more maintainable, readable, and expressive</li>
        <ul>
            <li>Setup: create any variables, conditions, etc. needed to perform your test</li>
            <li>Exercise: using your setup, execute the actual functionality being tested</li>
            <li>Verify: using the outcome of the excerise, compare to expected outcome, can write an if statement to make this comparison and throw error if it fails, but the assert library provides a more compact way of doing that, making the test more expressive.</li>
            <li>Teardown: not always needed, this resets any conditions changed during the test such as altering files or directory structure, changing read/write permissions for a file, editing database records.
                This ensures your test is isolated.
            </li>
            <li>Example Code:
                <code><pre>
                    describe('appendFileSync', () => {
                        it('writes a string to text file at given path name', () => {

                            // Setup
                            const path = './message.txt';
                            const str = 'Hello Node.js';
                            
                            // Exercise: write to file
                            fs.appendFileSync(path, str);

                            // Verify: compare file contents to string
                            const contents = fs.readFileSync(path);
                            assert.ok(contents.toString() === str);

                            // Teardown: delete path
                            fs.unlinkSync(path);
                        });
                    });
                </pre></code>
            </li>
        </ul>
        <li>Hooks are generally a better way to write setup and teardown code instead of including it directly in your it callback code.  Hooks are included inside the describe callback function along with the
            it function calls.  Hooks include <code>beforeEach(), afterEach(), before(), after()</code> and accept a single callback function as an argument.  For example, the provided callback function for afterEach() will
            run after every time an it callback runs in the same describe callback.  This is especially important to use for teardown code because it ensures the teardown code will run even if the it callback ends early
            due to an assertError or other error being thrown.  The teardown code would not run after the error if it was included in the it callback directly.
        </li>
        <ul>
            <li>Example code:
                <code><pre>
                    describe('example', () => {
                    
                        afterEach(() => {
                            // teardown goes here
                        });
                        
                        it('.sample', () => {
                            // test goes here
                        });
                    });
                </pre></code>
            </li>
        </ul>
    </ul>

    <h2>Jest</h2>
    <ul>
        <li>Jest is another javascript test framework and the default one used for React applications.  It needs to be installed for a project by initiliazing the project with npm (npm init) if it isn't already, and then running <code>npm install jest -D</code> to install to dev dependencies.  Then in your package.json file, under scripts, make sure you have <code>"scripts" : { "test": "jest" }</code> so that jest runs when you use the command <code>npm test</code></li>
        <li>By default, jest expects all tests to be saved in a folder in your root directory called <code>__tests__</code>.  By some conventions, you name each test file as <code>functionalityToTest.spec.js</code> where functionalityToTest is any descriptive name of the functionality to be tested in the file and .spec is included as an old Ruby convention indicating the file is a specification of functionality.</li>
        <li>As with Mocha, the typical usage is that you would import the functionality with <code>myFunction = require('../myFile/myCode.js')</code> to test from some javascript file in your project (and export the functionality in that file using module.exports = myFunction), and then you can use that funcitonality in your test file to exercise/test it. You shouldn't need to import Jest in your test files in any way if you've installed it for the project as described above</li>
        <li>Basic test syntax is similar to Mocha, except it use test() instead of it(), and provides a special syntax for exercising and verifying, namely <code>expect().toEqual()</code>, where expect() accepts an argument like a call to a function being tested that give some output, and toEqual() accepts an argument that that output should be equal to.  There are other 'matcher' functions besides .toEqual() like .toThrowError(Error('expected error msg here')) depending on the nature of the verification</li>
        <li>Example:
            <code><pre>
                describe("Filter function", () => {
                    test("it should filter by a search term (link)", () => {

                        // Setup
                        const input = [
                            { id: 1, url: "https://www.url1.dev" },
                            { id: 2, url: "https://www.url2.dev" },
                            { id: 3, url: "https://www.link3.dev" }
                        ];

                        const output = [{ id: 3, url: "https://www.link3.dev" }];

                        //Exercise and Verify
                        expect(filterByTerm(input, "link")).toEqual(output);
                    });
                });
            </pre></code>
        </li>
        <li>Jest provides a code coverage analysis feature.  This provides a report of how well your test files covers the file that is imported and tested.  If there are lines in the file that are never tested by your test file, then the report will indicate those lines and give percentage of lines covered, and some other info.  This coverage analysis can be run in a few ways.  You can run <code>npm test -- --coverage</code> to get a report in the command line after the test runs.  You can change your package.json scripts.test property from <code>"jest"</code> to <code>"jest --coverage</code> to always run the coverage report when you run <code>npm test</code>.  You can also instead add a separate new property to your package.json file outside of the scripts property that is <code>"jest": { "collectCoverage": true, "coverageReporters": ["html"] }</code> which will also always run the coverage report when you run <code>npm test</code> and the coverageReporters property can be left out to simply run reports in the command line, or included to instead print the reports to html files in a folder called 'coverage' in your root directory and which contain even more info than the command line report.</li>

    </ul>
</body>
</html>