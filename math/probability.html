<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Probability</h1>

    <h2>Set Theory</h2>
    <ul>
        <li><strong>Set</strong> Sets are a collection of elements where each element is unique, and order doesn't matter. Represented by curly braces and often denoted by a capital letter e.g. A = {Book, Folder, Pen, Paper, Hat}</li>
        <li><strong>Subset</strong> One set is a subset of another if each element of the set is contained in the other. e.g. A is a subset of B if A = {1, 3, 2} and B = {4, 2, 1, 5, 3}</li>
        <li><strong>Union</strong> The union of two sets is the set of all elements that appear in at least one of the two sets, written as (A or B)</li>
        <li><strong>Intersection</strong> The intersection of two sets is the set of all elements that appear in both of the two sets, written as (A and B)</li>
        <li><strong>Complement</strong> The complement of a set is all the elements not in the set, but in some superset, written A<sup>C</sup>. In the context of probability, your superset would be the sample space, and the set would be an event, and its complement would be all sample points not in the event.  So, the event and its complement cover the entire sample space.</li>
    </ul>

    <h2>Basic Probability</h2>
    <ul>
        <li><strong>Experiment/Trial</strong> An Experiment is any procedure that can be repeated infinitely and has a well-defined set of possible outcomes. e.g. flipping a coin once is an experiment.  Often, an experiment is repeated many times to be subjected to statistical analysis.  In this case, there is still just a single experiment but it can be thought of as an experiment composed of other experiments which we call trials.  Mathematically an experiment consists of 1) a Sample Space Ω or S of all possible outcomes, 2) a set of events F where each event is a set containing 0 or more outcomes, and 3) A probability measure function P that maps events to probabilities.</li>
        <li><strong>Sample Point or Outcome and Sample Space</strong> A Sample Point or Outcome of an Experiment is the result of a single execution of the experiment.  The Sample Space of an Experiment Ω is the set of all possible outcomes of the experiment.  The outcomes of a sample space should be mutually exclusive (if one happens then another does not) and collectively exhaustive (no matter what happens, the outcome of the experiment is in the sample space) </li>
        <li><strong>Event</strong> An event is a set of outcomes and is a subset of the sample space.  It is often more convenient to consider events than every possible outcome.  An event can consistent of a single outcome which is called an elementary or atomic event.</li>
        <ul>
            <li><strong>Dependence/Independence</strong> Independent events means the outcome of one event does not effect the probability of the other event.  Event B is not dependent on event A means P(B|A) = P(B), because A occuring has no effect on the probability of B occuring.  Since P(B|A) = P(B and A)/P(A) (P(A) not zero) by definition of conditional probability, we have P(B and A) = P(A)P(B) when event B is not dependent on event A.  If P(B) is not zero, then P(B and A)/P(B) = P(A and B)/P(B) = P(A|B) = P(A), so A is not dependent on B.  So we say A and B are independent events and the definition of independence is taken to be P(A and B) = P(A)P(B).  If either P(A) or P(B) is 0, then they are considered independent, and in fact any event with probability 0 is independent of every other event.  More generally, n events are independent only if the probability of the intersection of every combination of those events is equal to the product of those events' probabilities occuring on their own.  This means not only every pair of events, but every combination of three events, four events, up to n events.  e.g. flipping a coin twice, the outcome of the first flip does not effect the outcome of the second flip.  Dependent events means one event does effect the probability of the other, e.g. pulling marbles out of a bag that are either blue or red, the first time you pull a marble out is going to change the probability of what you pull out after that.  For more than two events, consider flipping a coin twice and event A is the first flip is heads, event B is the second flip is heads, and event C is both flips are the same.  Events A and B are independent, Events A and C are independnet, and events B and C are independent because the first event occuring in each of those does not change the probability of the second event occuring, or said another way P(A and B)=P(A)P(B), P(A and C)=P(A)P(C), and P(B and C) = P(B)P(C).  However, A,B,C are not independent events because P(A and B and C) does not equal P(A)P(B)P(C).  This is because P(C|A and B) = 1, i.e. if you know both A and B occured, then you know for sure that C occured.  So C is not independent of (A and B).  Likewise if you know C and A occurred then you know for sure B occurred or if you know C and B occured then you know for sure A occured.</li>
            <li><strong>Mutually Exclusive</strong> Two events are mutually exclusive if their intersection is empty.  e.g. flipping a coin the event of getting heads is mutually exclusive from the even of getting tails.  But rolling a die the event of getting an even number is not mutually exclusive frmo the event of getting a number greater than three. An event and its complement are always mutually exclusive.</li>
        </ul>
        <li><strong>Probability</strong> If we run an experiment an infinite number of times, the probability of a given event is the proportion of times it occurs.  This is known as the frequentist interpretation of probability.  This is denoted mathematically as P(Event) = (Number of times event occurred)/(Number of Trials).  A probabilistic model of something is a mathematical description of an uncertain situation that consists of a sample space Ω of all possible outcomes, and a probability law that assigns (ideally) every possible event A of the sample space a probability P(A).  The axioms for a probability rule are: P(A) >= 0 for all A, P(Ω)=1, and if A and B are disjoint events then P(A and B) = P(A) + P(B) (more generally this is true for the union of a countably infinite sequence of disjoint events)</li>
        <ul>
            <li><strong>Conditional Probability</strong> This is the probabiity of an event A occuring given than another event B occurs, and is written P(A|B). By definition P(A|B) = P(A and B)/P(B), given P(B) is not zero.  If the events are independent, then P(A|B) = P(A) and P(B|A) = P(B).  However this is not true if the events are dependent, e.g. if B is first picking a blue marble out of a bag of red and blue marbles, and A is second picking a red marble, then A is dependent on B because the ratio of red to blue marbles changes after the first pick.</li>
            <li><strong>Addition Rule</strong> The probability of event A happening or event B happening (or both) is P(A or B) = P(A) + P(B) - P(A and B).  If A and B are mutually exclusive, then P(A and B) = 0, but if they are not, then you have to subtract the probability of their intersection or else you'll be double counting.</li>
            <li><strong>Multiplication Rule</strong> The probability of event A happening and event event B happening is P(A and B) = P(A) * P(B|A).  If A and B are independent, the P(B|A) = P(B), but if they are not, then you have to take into account the probability of B occuring given A occuring.  This provides a definition of conditional probability as P(B|A) = P(A and B) / P(A).  </li>
            <li><strong>Bayes' Theorem</strong> This states that P(B|A) = P(A|B)*P(B) / P(A) = P(A and B)/P(A).  This is true because the set A and B = B and A.  Thus P(A and B) = P(B and A), thus P(A) * P(B|A) = P(B) * P(A|B), and thus P(B|A) = P(B) * P(A|B) / P(A)</li>
            <li><strong>Tree Diagrams</strong> This is a way of visually diagramming multiple experiments and their sample space, and you can see how the multiplaction rule works and Bayes' Theorem.</li>
            <ul>
                <li>Here is a diagram for independent events:</li><img src="./coin-tree-diagram.svg" alt="">
                <li>And here is a diagram for dependent events.  The first experiment is whether someone has strep throat, and P(ST) is the event that they do and has a 20% probability.  The second experiment is running a test for strep throat and the events are either a positive or negative test result.  The probabilities of the events are dependent on whether the person has strep throat or not.  If you wanted to know the probability of having strep throat given you had a positive test, then you could use Bayes' theorem: where P(ST|+) = P(+|ST) * P(ST) / P(+)</li>
                <img src="./Conditional_Probability_Application.svg" alt="" style="width: 800px">
            </ul>
        </ul>
        <li><strong>Law of Large Numbers</strong> You can't perform an infinite number of trials of an experiment, but the law of large numbers is that as you perform more and more trials, the probability of any given event will converge to its true probability.</li>
        <li>For example, say the experiment is flipping a coin twice and the observation is which side of the coin lands up on each flip.
        <ul>
            <li>There are four possible sample points for this experiment depending on the side of the first flip and the side of the second flip.  The full sample space could be represented as S = {HH, HT, TH, TT}</li>
            <li>Possible events you might be interested in are getting two heads, A = {HH}, getting two tails, B = {TT}, or getting a heads and a tails, C = {HT, TH}</li>
            <li>You might run 1000 trials of the experiment, and notice that {HH} occurred 252 times. Thus you could estimate the probability of getting two heads with two coin flips is P({HH}) = 252/1000 = .252 = 25.2%</li>
        </ul>
        </li>
    </ul>

    <h2>Combinatorics</h2>
    <ul>
        <li>Combinatorics is the mathematics of counting, you often need to be able to count outcomes in an event or in a sample space.  Counting is really needed in the case of uniform probability.</li>
        <li><strong>Discrete Uniform Probability</strong> When calculating the probability of events in a sample space where every outcome has the same probability, then you simply need to count the number of outcomes in the event and divide by the number of outcomes in the sample space to get the probability.  The hard part can be counting the outcomes in the event of interest and outcomes in the sample space, when you have a large events/sample space, which is where combinatorics comes in handy.  e.g. using the basic counting principles below you can calculate the probability that 6 rolls of a die will each roll a different number as 6! / 6^6.</li>
        <li><strong>Basic Counting principles</strong> Consider multiple stages each with multiple possible choices.  e.g. 1st stage has 2 choices, 2nd stage has 4 choices, 3rd stage has 3 choices.  Then the total number of outcomes across all 3 stages is 2 * 4 * 3.  Think of a tree diagram of this, and starting from the leaves of the tree, you have 3 outcomes repeated 4 times, and then you have that repeates 2 times.  An example of using this counting principle is picking letters/digits for a license plate, say you pick 6 and there are 26 letters and 10 digits for each choice, then you get 36*36*36*36*36*36 possible outcomes.  When you are picking items from a set of items and cannot reuse the items, then e.g. for the license plate you get 36*35*34*33*32*31.  If you go through every item in the set, then each outcome is called a permutation of the n items, and permutations refer to all the possible orderings of n items, and is given by n!.  Finally, an important counting principle is going through a list of n items and choosing to pick each one or not to create a subset, for this you have n trials with 2 outcomes on each, so the number of subsets of n items is 2^n (includes possibility of picking none).</li>
        <li><strong>Combinations</strong> Represented as (n k) except n is stacked on top of k, and said as n choose k.  It is for determining the number of possible subsets of k elements from a set of n elements.  Since they are sets, the order of the k elements does not matter.  If it did matter, then there would be k!/(n-k)! possibilities (example of basic counting principles above).  But we know also from basic counting principles that each subset of k elements has k! different orderings.  So (n k) = k! / ((n-k)!k!).  Note 0! is defined to equal 1 because it makes this formula work in the case that k=n or k=0.  n and k are the binomial coefficients. </li>
        <li><strong>Binomial Probabilities</strong> Consider the experiment of n coin tosses, and the event that you get k heads.  If the probability of getting heads is p, then for any given outcome, the probability for that outcome is p^k * (1-p)^(n-k) you want to know how many outcomes have k heads.  The problem is now counting how many outcomes have k heads and multiplying that by the probability of each outcome above.  Counting how many outcomes have k heads is equivalent to counting how many ways you can pick out k items out the n tosses to pick to be heads, i.e. the number of possible subsets of k elements out of n.  This is simply n choose k, so the probability of k heads in n coin tosses is (n k) * p^k * (1-p)^(n-k).  This applies to any binomial distribution with n independent events.  Consider summing up the probability of k heads in n tosses from k=0 to k=n.  This would be equal to 1 since it covers the whole sample space. Additionally, consider the question out of 10 coin tosses, you know 3 heads occurred, and you want to know what is the probability that the first two tosses were heads, so given event A (3 heads) what is the chance of event B (first two are heads)?  Within event A the probability of all outcomes is the same since every outcome has the same number of heads and it is p^3 * (1-p)^(n-3).  So, it is uniform probability inside event A and all you need to do is count the ways B can occur and divide by the number of ways A can occur.  A can occur (n 3) * p^3 * (1-p)^(n-3) ways, and given A, B can occur 8 ways since the only uncertainty in B is where the third head goes, which given the first two are heads, leaves 8 possible spots the third head could occur.  So the answer is 8 divided by the probability of A above.</li>
        <li><strong>Partitioning</strong> A more general combination is partitioning a set into a given number of subsets of given cardinalities.  A combination can be though of as how many ways can I split a set of n elements into 2 subsets, one a subset of k elements and one a subset of n-k elements.  A partion asks how many ways can a split a set of n elements into m subsets of cardinality k<sub>1</sub>, k<sub>2</sub>, ..., k<sub>m</sub>.  This is (n k<sub>1</sub>) * (n-k <sub>1</sub> k<sub>2</sub>) * ... * (n-k<sub>1</sub>-...-k<sub>m-1</sub> k<sub>m</sub>). The last term is always equal to 1, and algebraically this works out to n!/(k<sub>1</sub>! * k<sub>2</sub>! * ... * k<sub>m</sub>!).  e.g. if we want to know how many ways there are to deal 52 cards into 4, 13-card bridge hands, then it would be (52 13)*(39 13)*(26 13)*(13 13) = 52! / (13! * 13! * 13! * 13!).   Now consider the question what is the probability of dealing one ace to each person?  Well, You have the demoninator you need of how many 13-card hands there are total.  The numerator comes from, how many ways are there to distribute 4 aces to 4 hands? There is 4! since you have 4 hands to choose from for the first ace, 3 hands for the second ace, and so on.  Now how many ways are there to distribute the remaining 48 cards to 4 hands? It comes from partioning 48 cards into 4, 12-card partitions.  So, the answer is (4! * (48 12) * (36 12) * (24 12) * (12 12)) / (52 13)*(39 13)*(26 13)*(13 13)</li>
    </ul>

    <h2>Probability Distributions</h2>
    <ul>
        <li><strong>Random Variable</strong> A random variable X is a function from a sample space to generally the real numbers.  It divides the sample space up into a series of mutually exclusive events that cover the entire sample space, assigning each event a numeric value.  It is useful because it allows you to forget the particular sample space you are using, and focus on just the value of the random variable and probabilities associated with it.  Many different sample spaces can be assigned to useful random variables in a way that ends up having the same probability distributions, so by studying one probability distribution you can understand many different sample spaces.  These common probability distributions are given names and often parameters that influence the exact distribution.</li>
        <ul>
            <li><strong>Discrete Random Variable</strong> A discrete random variable has a countable range often the integers.  E.g. you could flip a coin four times, creating a sample space of {HHHH, HHHT, HHTH, HHTT, HTHH, HTHT, HTTH, HTTT, THHH, THHT, THTH, THTT, TTHH, TTHT, TTTH, TTTT}.  You could create a random variable that maps this sample space onto the number of heads in each outcome, which will have a range of {0, 1, 2, 3, 4}</li>
            <li><strong>Continous Random Variable</strong> A continuous random variable has an uncountably infinite range, e.g. the temperature measured at a given time is a continuous random variable, or the time it takes a person to run a mile.</li>
            <li>Python's numpy library has a convenient method for simulating a random event a given number of times random.choice(list to choose from, number of times to choose, replace after choosing), where the first arg could be a list 1-6 for a die, the second arg is 5 for rolling it 5 times, and the third arg is TRUE because you can roll the same value again.  It outputs a list of the results of its random choices.</li>
        </ul>
        <li><strong>Expected Value</strong> This represents the average value of a random variable, E(X).  If you sum together two independent random variables, then E(X+Y) = E(X) + E(Y).  If you multiply a random variable by a constant, then E(aX) = aE(X).  Finally if you add a constant to a random variable, then E(X+a) = E(X)+a.  For example consider an experiment where you pick an employee's salary at random, there is a sample space of values with an associated probability distribution, and you define a random variable X that is the employee's salary, which will thus have a probability distribution and an Expected Value E(X) (average if you ran the experiment many times). If you gave each employee a 5% raise, then you could define a new random variable that is 0.05*X, and the expected value of the raise would be E(0.05X) = 0.05E(X)</li>
        <li><strong>Variance</strong> This represents the spread of a random variable Var(X).  For a constant a, Var(X+a) = Var(X), Var(aX) = a^2 * Var(X), and Var(X+Y) = Var(X) + Var(Y) if X and Y are independent random variables. </li>
        <li><strong>Probability Distribution</strong> A general discription of a probability distribution is a function P: A->R, where A is related to the sample space S and the output R (reals) assigns a probability.  If S is the sample space, then A is the set of all subsets of S whose probability can be measured, i.e. all possible events.  Often, we use a random variable X to transform a sample space to a number (e.g Reals or Integers), and speak of the probability distribution of the random variable, which replaces the sample space S in the description above, and the arguments to the probability function are subsets of X.  Once a random variable is defined, you can ask what the probabiltiy of a given output value of the random variable.  Mapping all possible values of the random variable to their probability is the probability distribution of the random variable.</li>
        <ul>
            <li><strong>Probability Mass Function (PMF)</strong> This is a type of probability distribution that defines the probability of observing a particular value of a discrete sample space or random variable and encodes the probabilities as a discrete list of outcomes and probabilities.  Often denoted P(X=?). In the discrete case, each value of the random variable is a mutually exclusive event and the entire sample space is covered, so the sum of all probabilities in the distribution is 1.  You can also look at the PMF over a range of X values, as in P(1&lt=X&lt=3), which equals P(X=1) + P(X=2) + P(X=3). Sometimes it is easier when looking at a range to calculate the probabilities of the X values not in that range and subtract that from 1.  e.g. for the flip a coin four times example, you would map the output range {0, 1, 2, 3, 4} to their probabilities, which are 1/16, 4/16, 6/16, 4/16, 1/16 respectively, thus P(X=1) = 4/16, and P(1&lt=X&lt=3) = P(X=1)+P(X=2)+P(X=3) = 4/16+6/16+4/16 = 14/16</li>
            <ul>
                <li><strong>Binomial Distribution</strong> This is a common type of PMF that has parameters of the number of trials n, and the probability of success in each trial p. The expected value E(X), or average if you conducted many experiments is E(X) = n*p.  The variance for a single trial is p(p-1), and the variance var(X) for the entire experiment is n*p(p-1). e.g. the flip a coin four times example has a binomial distribution, the n=4 and p=0.5 (say succes is getting heads), and E(X)=2 adn Var(X)=1.  Another example is randomly guessing 20 questions on a multiple choice test with four options for each question where each option is equally likely to be right, then you would have a binomial distribution for your score with n=20 and p=0.25, and E(X)=5.</li>
                <li><strong>Poisson Distribution</strong> This is a PMF for discrete random variables, common for counting the number of occurences of something in a given interval of time, like the number of cars through an intersection during a given hour of the day. It has a parameter λ that is the expected value or average value of the distribution.  This is the result of making the observation many times, e.g. 10 cars one day, 7 cars the next, 15 the next day, etc. - the average value of those counts as the number of observations approaces infinity (law of large numbers) is the expected value.  The variance or spread of the distribution is always equal to λ, thus bigger expected values always have a bigger variance.</li>
                <li>Python's scipy.stats library has a method to calculate the probability mass function value for a given random variable value of a binomial distribution and poisson distribution.  For the binomial distribution it is binom.pmf(value to calculate, number of trials, probability of success on each trial).  E.g. with the flip a coin four times example, you could get the probability of flipping 2 heads with binom.pmf(2, 4, .5).  For the poisson distribution, it is poisson.pmf(value to calculate, expected value). E.g. if the expected value of rainy days in the next month is 10, and you wanted to know the probability of having 6 you could use poisson.pmf(6, 10) </li>
                <li>The scipy.stats library also has a method for generating a list of random values that follow the poisson distribution (probably one for binomial too).  You can use poisson.rvs(expected value, size = num of values), e.g. poisson.rvs(10, size=1000) would generate a list of 1000 values following a poisson distribution with expected value 10.  You could then plot this or find the mean, or min and max values, etc.</li>
            </ul>
            <li><strong>Probability Density Function</strong> These define the probability distribution of a continuous random variable, by definition the probability distribution of a continuous random variable is the integral of the probability density function, which is the cumulative ditribution function.  Instead of mapping discrete values of a discrete random variable to probabilities, you take the area under the curve to get the probability of a range of values of the random variable.  Since the range of a continuous random variable is infinite, any single value of the random variable has zero probability, but a range of values is also a continuum and this can be assigned a probability within the range of the random variable.</li>
            <ul>
                <li>The normal distribution is common for use in probability density functions and is parameterized by the mean μ (mu) of the distribution and standard deviation σ (sigma).</li>
            </ul>
            <li><strong>Cumulative Distribution Function (CDF)</strong> This can be derived from the Probability Mass Function or Probability density function and gives the probability of observing a specific value or less, i.e. CDF(X=n) = PMF(X &lt= n).  The CDF is always increasing, and the highest value of X will equal 1. The CDF is handy for calculating ranges of a discrete random variable, e.g. if you want to know P(3&lt=X&lt=6) = CDF(X=6) - CDF(X=2).  Or if you want to know P(X>6) = 1 - CDF(X=6).  This applies to continuous random variables also, where the CDF is helpful for getting the area under the curve.  You can get 1)the probability that the random variable is less than a certain amount by getting the value of the CDF at that point, 2)the probability of a range of the random variable by subtracting two values of the CDF at the endpoints of the range, or 3) the probability that the random variable is greater than a certain amount by subtracting the value of the CDF at that point from 1.</li>
            <ul>
                <li>Python's scipy.stats library has a method to calculate the cdf for a given random variable value of a binomial distribution or a poisson distribution. For binomial, it is binom.cdf(value to calculate, number of trials, probability of success on each trial).  For poisson, it is poisson.cdf(value to calculate, expected value).</li>
                <li>The scipy.stats library also has method to calculate the cdf for a normal distribution.  norm.cdf(value to calculate, mean of distribution, standard deviation)</li>
            </ul>
        </ul>
    </ul>

    <h2>Sampling Distributions</h2>
    <ul>
        <li>We often cannot calculate statistics for an entire population, and thus calculate a statistic from a randomly selected sample of the population, and want to know how certain we can be that we know the true statistic for the entire population.</li>
        <li><strong>Sampling Distribution</strong> If you take a statistic like the mean, median, the min or max value, variance, etc. for a sample of the population, across many samples, you will create a probability distbution for that value which is called a sampling distribution.</li>
        <li><strong>Unbiased and Biased Estimators</strong> If the mean of a sampling distribution for some statistic is equal (approximately) to the actual value of that statistic for the whole population, then the sample statistic is an unbiased estimator.  This is true for the mean of a sample/population.  If the mean of a sampling distribution for some statistic is not centered around the actual value fo that statistic for the whole population, then it is a biased estimator.  This is true for the max and min value of a sample/population.</li>
        <li><strong>Central Limit Theorem</strong> This states that the sampling distribution of the mean for a population will be normally distributed as long as the population is not too skewed, or the sample size is large enough (at least 30 is a good rule of thumb).  You can have a smaller sample size if the population is normally distributed and the central limit theorem will still hold.  CLT says 1) The sampling distribution of the mean will have a mean x that will be close to the true population mean μ. And 2) The sampling distribution of the mean will have standard deviation equal to the population standard deviation over the square root of the sample size σ / sqrt(n). The standard deviation of a sampling distribution is called the standard error of the estimate of the mean. We may not know the population's standard deviation so we often use the standard deviation of a sample in its place.</li>
        <li><strong>Confidence Intervals</strong> Usually we only have one sample, not many that we can make a sampling distribution with.  We want to say how sure we are that the statistic for that sample that we're interested in is close to the actual population statistic.  We can use the standard deviation of the sample as an estimate for the standard deviation of that population.  Then per CLT, we can calculate the standard error of our sample, using the estimate of the standard deviation divided by the square root of the sample size.  Also using CLT, we know that the sampling distribution is normally distributed and centered on the actual population mean. Since a normal distribution has 95% of its values fall with 1.96 standard deviations of the mean, we know that 95% of sample statistics will thus be less than 1.96 * standard error away from the actual mean.  Thus using our standard error estimate and the mean of our one sample, we can say that the mean of our sample is within 1.96 * standard error of the actual population mean.  This interval is called a 95% confidence interval.</li>
        <li>Python's numpy library has a convenient function for creating a random sample from a population.  It is random.choice(population array, sample size, replace), where pop array is an array to choose from, sample size is an integer, and replace should be FALSE because once an item is added to the sample it should not be able to be picked again.  It returns a list of the sample.</li>
    </ul>
</body>
</html>