<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Probability</h1>

    <h2>Set Theory</h2>
    <ul>
        <li><strong>Set</strong> Sets are a collection of elements where each element is unique, and order doesn't matter. Represented by curly braces and often denoted by a capital letter e.g. A = {Book, Folder, Pen, Paper, Hat}</li>
        <li><strong>Subset</strong> One set is a subset of another if each element of the set is contained in the other. e.g. A is a subset of B if A = {1, 3, 2} and B = {4, 2, 1, 5, 3}</li>
        <li><strong>Union</strong> The union of two sets is the set of all elements that appear in at least one of the two sets, written as (A or B)</li>
        <li><strong>Intersection</strong> The intersection of two sets is the set of all elements that appear in both of the two sets, written as (A and B)</li>
        <li><strong>Complement</strong> The complement of a set is all the elements not in the set, but in some superset, written A<sup>C</sup>. In the context of probability, your superset would be the sample space, and the set would be an event, and its complement would be all sample points not in the event.  So, the event and its complement cover the entire sample space.</li>
    </ul>

    <h2>Basic Probability</h2>
    <ul>
        <li><strong>Experiment/Trial</strong> An Experiment is any procedure that can be repeated infinitely and has a well-defined set of possible outcomes. e.g. flipping a coin once is an experiment.  Often, an experiment is repeated many times to be subjected to statistical analysis.  In this case, there is still just a single experiment but it can be thought of as an experiment composed of other experiments which we call trials.  Mathematically an experiment consists of 1) a Sample Space Ω or S of all possible outcomes, 2) a set of events F where each event is a set containing 0 or more outcomes, and 3) A probability measure function P that maps events to probabilities.</li>
        <li><strong>Sample Point or Outcome and Sample Space</strong> A Sample Point or Outcome of an Experiment is the result of a single execution of the experiment.  The Sample Space of an Experiment Ω is the set of all possible outcomes of the experiment.  The outcomes of a sample space should be mutually exclusive (if one happens then another does not) and collectively exhaustive (no matter what happens, the outcome of the experiment is in the sample space) </li>
        <li><strong>Event</strong> An event is a set of outcomes and is a subset of the sample space.  It is often more convenient to consider events than every possible outcome.  An event can consistent of a single outcome which is called an elementary or atomic event.</li>
        <ul>
            <li><strong>Dependence/Independence</strong> Independent events means the outcome of one event does not effect the probability of the other event.  Event B is not dependent on event A means P(B|A) = P(B), because A occuring has no effect on the probability of B occuring.  Since P(B|A) = P(B and A)/P(A) (P(A) not zero) by definition of conditional probability, we have P(B and A) = P(A)P(B) when event B is not dependent on event A.  If P(B) is not zero, then P(B and A)/P(B) = P(A and B)/P(B) = P(A|B) = P(A), so A is not dependent on B.  So we say A and B are independent events and the definition of independence is taken to be P(A and B) = P(A)P(B).  If either P(A) or P(B) is 0, then they are considered independent, and in fact any event with probability 0 is independent of every other event.  More generally, n events are independent only if the probability of the intersection of every combination of those events is equal to the product of those events' probabilities occuring on their own.  This means not only every pair of events, but every combination of three events, four events, up to n events.  e.g. flipping a coin twice, the outcome of the first flip does not effect the outcome of the second flip.  Dependent events means one event does effect the probability of the other, e.g. pulling marbles out of a bag that are either blue or red, the first time you pull a marble out is going to change the probability of what you pull out after that.  For more than two events, consider flipping a coin twice and event A is the first flip is heads, event B is the second flip is heads, and event C is both flips are the same.  Events A and B are independent, Events A and C are independnet, and events B and C are independent because the first event occuring in each of those does not change the probability of the second event occuring, or said another way P(A and B)=P(A)P(B), P(A and C)=P(A)P(C), and P(B and C) = P(B)P(C).  However, A,B,C are not independent events because P(A and B and C) does not equal P(A)P(B)P(C).  This is because P(C|A and B) = 1, i.e. if you know both A and B occured, then you know for sure that C occured.  So C is not independent of (A and B).  Likewise if you know C and A occurred then you know for sure B occurred or if you know C and B occured then you know for sure A occured.</li>
            <li><strong>Mutually Exclusive</strong> Two events are mutually exclusive if their intersection is empty.  e.g. flipping a coin the event of getting heads is mutually exclusive from the even of getting tails.  But rolling a die the event of getting an even number is not mutually exclusive frmo the event of getting a number greater than three. An event and its complement are always mutually exclusive.</li>
        </ul>
        <li><strong>Probability</strong> If we run an experiment an infinite number of times, the probability of a given event is the proportion of times it occurs.  This is known as the frequentist interpretation of probability.  This is denoted mathematically as P(Event) = (Number of times event occurred)/(Number of Trials).  A probabilistic model of something is a mathematical description of an uncertain situation that consists of a sample space Ω of all possible outcomes, and a probability law that assigns (ideally) every possible event A of the sample space a probability P(A).  The axioms for a probability rule are: P(A) >= 0 for all A, P(Ω)=1, and if A and B are disjoint events then P(A and B) = P(A) + P(B) (more generally this is true for the union of a countably infinite sequence of disjoint events)</li>
        <ul>
            <li><strong>Conditional Probability</strong> This is the probabiity of an event A occuring given than another event B occurs, and is written P(A|B). By definition P(A|B) = P(A and B)/P(B), given P(B) is not zero.  If the events are independent, then P(A|B) = P(A) and P(B|A) = P(B).  However this is not true if the events are dependent, e.g. if B is first picking a blue marble out of a bag of red and blue marbles, and A is second picking a red marble, then A is dependent on B because the ratio of red to blue marbles changes after the first pick.</li>
            <li><strong>Addition Rule</strong> The probability of event A happening or event B happening (or both) is P(A or B) = P(A) + P(B) - P(A and B).  If A and B are mutually exclusive, then P(A and B) = 0, but if they are not, then you have to subtract the probability of their intersection or else you'll be double counting.</li>
            <li><strong>Multiplication Rule</strong> The probability of event A happening and event event B happening is P(A and B) = P(A) * P(B|A).  If A and B are independent, the P(B|A) = P(B), but if they are not, then you have to take into account the probability of B occuring given A occuring.  This provides a definition of conditional probability as P(B|A) = P(A and B) / P(A).  </li>
            <li><strong>Bayes' Theorem</strong> This states that P(B|A) = P(A|B)*P(B) / P(A) = P(A and B)/P(A).  This is true because the set A and B = B and A.  Thus P(A and B) = P(B and A), thus P(A) * P(B|A) = P(B) * P(A|B), and thus P(B|A) = P(B) * P(A|B) / P(A)</li>
            <li><strong>Tree Diagrams</strong> This is a way of visually diagramming multiple experiments and their sample space, and you can see how the multiplaction rule works and Bayes' Theorem.</li>
            <ul>
                <li>Here is a diagram for independent events:</li><img src="./coin-tree-diagram.svg" alt="">
                <li>And here is a diagram for dependent events.  The first experiment is whether someone has strep throat, and P(ST) is the event that they do and has a 20% probability.  The second experiment is running a test for strep throat and the events are either a positive or negative test result.  The probabilities of the events are dependent on whether the person has strep throat or not.  If you wanted to know the probability of having strep throat given you had a positive test, then you could use Bayes' theorem: where P(ST|+) = P(+|ST) * P(ST) / P(+)</li>
                <img src="./Conditional_Probability_Application.svg" alt="" style="width: 800px">
            </ul>
        </ul>
        <li><strong>Law of Large Numbers</strong> You can't perform an infinite number of trials of an experiment, but the law of large numbers is that as you perform more and more trials, the probability of any given event will converge to its true probability.</li>
        <li>For example, say the experiment is flipping a coin twice and the observation is which side of the coin lands up on each flip.
        <ul>
            <li>There are four possible sample points for this experiment depending on the side of the first flip and the side of the second flip.  The full sample space could be represented as S = {HH, HT, TH, TT}</li>
            <li>Possible events you might be interested in are getting two heads, A = {HH}, getting two tails, B = {TT}, or getting a heads and a tails, C = {HT, TH}</li>
            <li>You might run 1000 trials of the experiment, and notice that {HH} occurred 252 times. Thus you could estimate the probability of getting two heads with two coin flips is P({HH}) = 252/1000 = .252 = 25.2%</li>
        </ul>
        </li>
    </ul>

    <h2>Combinatorics</h2>
    <ul>
        <li>Combinatorics is the mathematics of counting, you often need to be able to count outcomes in an event or in a sample space.  Counting is really needed in the case of uniform probability.</li>
        <li><strong>Discrete Uniform Probability</strong> When calculating the probability of events in a sample space where every outcome has the same probability, then you simply need to count the number of outcomes in the event and divide by the number of outcomes in the sample space to get the probability.  The hard part can be counting the outcomes in the event of interest and outcomes in the sample space, when you have a large events/sample space, which is where combinatorics comes in handy.  e.g. using the basic counting principles below you can calculate the probability that 6 rolls of a die will each roll a different number as 6! / 6^6.</li>
        <li><strong>Basic Counting principles</strong> Consider multiple stages each with multiple possible choices.  e.g. 1st stage has 2 choices, 2nd stage has 4 choices, 3rd stage has 3 choices.  Then the total number of outcomes across all 3 stages is 2 * 4 * 3.  Think of a tree diagram of this, and starting from the leaves of the tree, you have 3 outcomes repeated 4 times, and then you have that repeates 2 times.  An example of using this counting principle is picking letters/digits for a license plate, say you pick 6 and there are 26 letters and 10 digits for each choice, then you get 36*36*36*36*36*36 possible outcomes.  When you are picking items from a set of items and cannot reuse the items, then e.g. for the license plate you get 36*35*34*33*32*31.  If you go through every item in the set, then each outcome is called a permutation of the n items, and permutations refer to all the possible orderings of n items, and is given by n!.  Finally, an important counting principle is going through a list of n items and choosing to pick each one or not to create a subset, for this you have n trials with 2 outcomes on each, so the number of subsets of n items is 2^n (includes possibility of picking none).</li>
        <li><strong>Combinations</strong> Represented as (n k) except n is stacked on top of k, and said as n choose k.  It is for determining the number of possible subsets of k elements from a set of n elements.  Since they are sets, the order of the k elements does not matter.  If it did matter, then there would be k!/(n-k)! possibilities (example of basic counting principles above).  But we know also from basic counting principles that each subset of k elements has k! different orderings.  So (n k) = k! / ((n-k)!k!).  Note 0! is defined to equal 1 because it makes this formula work in the case that k=n or k=0.  n and k are the binomial coefficients. </li>
        <li><strong>Binomial Probabilities</strong> Consider the experiment of n coin tosses, and the event that you get k heads.  If the probability of getting heads is p, then for any given outcome, the probability for that outcome is p^k * (1-p)^(n-k).  The problem is now counting how many outcomes have k heads and multiplying that by the probability of each outcome above.  Counting how many outcomes have k heads is equivalent to counting how many ways you can pick out k items out the n tosses to be heads, i.e. the number of possible subsets of k elements out of n.  This is simply n choose k, so the probability of k heads in n coin tosses is (n k) * p^k * (1-p)^(n-k).  This applies to any binomial distribution with n independent trials and probability p of success on each trial.  Consider summing up the probability of k heads in n tosses from k=0 to k=n.  This would be equal to 1 since it covers the whole sample space. Additionally, consider the question out of 10 coin tosses, you know 3 heads occurred, and you want to know what is the probability that the first two tosses were heads, so given event A (3 heads) what is the chance of event B (first two are heads)?  Within event A the probability of all outcomes is the same since every outcome has the same number of heads and it is p^3 * (1-p)^(n-3).  So, it is uniform probability inside event A and all you need to do is count the ways B can occur and divide by the number of ways A can occur.  A can occur (n 3) * p^3 * (1-p)^(n-3) ways, and given A, B can occur 8 ways since the only uncertainty in B is where the third head goes, which given the first two are heads, leaves 8 possible spots the third head could occur.  So the answer is 8 divided by the probability of A above.</li>
        <li><strong>Partitioning</strong> A more general combination is partitioning a set into a given number of subsets of given cardinalities.  A combination can be though of as how many ways can I split a set of n elements into 2 subsets, one a subset of k elements and one a subset of n-k elements.  A partion asks how many ways can a split a set of n elements into m subsets of cardinality k<sub>1</sub>, k<sub>2</sub>, ..., k<sub>m</sub>.  This is (n k<sub>1</sub>) * (n-k <sub>1</sub> k<sub>2</sub>) * ... * (n-k<sub>1</sub>-...-k<sub>m-1</sub> k<sub>m</sub>). The last term is always equal to 1, and algebraically this works out to n!/(k<sub>1</sub>! * k<sub>2</sub>! * ... * k<sub>m</sub>!).  e.g. if we want to know how many ways there are to deal 52 cards into 4, 13-card bridge hands, then it would be (52 13)*(39 13)*(26 13)*(13 13) = 52! / (13! * 13! * 13! * 13!).   Now consider the question what is the probability of dealing one ace to each person?  Well, You have the demoninator you need of how many 13-card hands there are total.  The numerator comes from, how many ways are there to distribute 4 aces to 4 hands? There is 4! since you have 4 hands to choose from for the first ace, 3 hands for the second ace, and so on.  Now how many ways are there to distribute the remaining 48 cards to 4 hands? It comes from partioning 48 cards into 4, 12-card partitions.  So, the answer is (4! * (48 12) * (36 12) * (24 12) * (12 12)) / (52 13)*(39 13)*(26 13)*(13 13)</li>
    </ul>

    <h2>Probability Distributions</h2>
    <ul>
        <li><strong>Random Variable</strong> A random variable is a function from a sample space to a set of numbers, generally the reals or integers.  It is generally represented by a capital letter like X, and its values are represented by a lower case letter like x.  A single experiment that generates a sample space could have multiple random variables, e.g. say your experiment is picking a student out of a class, your sample space would consist of one outcome for each student in the class.  You could have a random variable H from each outcome (a student) to the height of that student in inches (a real number or integer) and a second random variable W from each outcome to the weight of that student.  Additionally, you can have random variables defined on other random variables, say Hbar = 2.54*H and gives the height of a student in centimeters, Hbar is still a random variable because each outcome in the sample space maps to a single value of Hbar, and thus Hbar is a function from outcomes in a sample space to numbers.  It divides the sample space up into a series of mutually exclusive events that cover the entire sample space, assigning each event a numeric value.  It is useful because it allows you to forget the particular sample space you are using, and focus on just the value of the random variable and probabilities associated with it.  Many different sample spaces can be assigned to useful random variables in a way that ends up having the same probability distributions, so by studying one probability distribution you can understand many different sample spaces.  These common probability distributions are given names and often parameters that influence the exact distribution.</li>
        <ul>
            <li><strong>Discrete Random Variable</strong> A discrete random variable has a countable range often the integers.  E.g. you could flip a coin four times, creating a sample space of {HHHH, HHHT, HHTH, HHTT, HTHH, HTHT, HTTH, HTTT, THHH, THHT, THTH, THTT, TTHH, TTHT, TTTH, TTTT}.  You could create a random variable that maps this sample space onto the number of heads in each outcome, which will have a range of {0, 1, 2, 3, 4}</li>
            <li><strong>Continous Random Variable</strong> A continuous random variable has an uncountably infinite range, e.g. the temperature measured at a given time is a continuous random variable, or the time it takes a person to run a mile.</li>
            <li>Python's numpy library has a convenient method for simulating a random event a given number of times random.choice(list to choose from, number of times to choose, replace after choosing), where the first arg could be a list 1-6 for a die, the second arg is 5 for rolling it 5 times, and the third arg is TRUE because you can roll the same value again.  It outputs a list of the results of its random choices.</li>
        </ul>

        <li><strong>Probability Distribution</strong> A general discription of a probability distribution is a function P: A->R, where A is related to the sample space S and the output R (reals) assigns a probability.  If S is the sample space, then A is the set of all subsets of S whose probability can be measured, i.e. all possible events.  Often, we use a random variable X to transform a sample space to a number (e.g Reals or Integers), and speak of the probability distribution of the random variable, which replaces the sample space S in the description above, and the arguments to the probability function are subsets of X.  Once a random variable is defined, you can ask what the probabiltiy of a given output value of the random variable.  Mapping all possible values of the random variable to their probability is the probability distribution of the random variable.</li>
        <ul>
            <li><strong>Probability Mass Function (PMF)</strong> This is how probabilities are assigned for discrete random variables.  Consider a random variable X, with values x mapped from each outcome in a sample space.  You are interested in the probability of each value of x, based on the probability of outcomes in the sample space.  This probability is a function from x to a probability, often denoted in a few different ways, p<sub>X</sub>(x) = P(X=x) = P({ω∈Ω s.t. X(ω)=x}), where p<sub>X</sub>(x)≥0 and the sum over all values of x of p<sub>X</sub>(x) equals 1.  The last representation makes explicit you want the total probability of all outcomes in the sample space that map to x under the random variable X.  In general, a random variable X can map many outcomes to the same value, and your task is to add up the probabilities of all those outcomes for each x to determine the probability of x.  e.g. you roll a die twice and could have a random variable F that maps to the first value of the die roll, a random variable S that maps to the second value of the die roll, and a random variable M = min(F, S).  If you want to know what is P(M=2) for example, you know the outcomes (2,2), (2,3), (2,4), (2,5), (2,6), (3,2), (4,2), (5,2), (6,2) are what map to 2 under M.  Since the outcomes have uniform probability of 1/(6*6), you count them and get that P(M=2) = 9/36. You can also look at the PMF over a range of X values, as in P(1&lt=X&lt=3), which equals P(X=1) + P(X=2) + P(X=3). Sometimes it is easier when looking at a range to calculate the probabilities of the X values not in that range and subtract that from 1.  e.g. for the flip a coin four times example, you would map the output range {0, 1, 2, 3, 4} to their probabilities, which are 1/16, 4/16, 6/16, 4/16, 1/16 respectively, thus P(X=1) = 4/16, and P(1&lt=X&lt=3) = P(X=1)+P(X=2)+P(X=3) = 4/16+6/16+4/16 = 14/16</li>
            <ul>
                <li><strong>Conditional PMF</strong> A conditional PMF is the PMF of a random variable X given that some event A has occurred.  It is written as p<sub>X|A</sub>(x) = P(X=x|A).  For instance you may have a random variable X that can take values of 1, 2, 3, or 4 with equal probability of 1/4 for each value, so for k in {1, 2, 3, 4}, P(k) = 1/4. You might know that event A = (X >= 2) has occurred, so you know X doesn't equal 1.  The conditional PMF would have P(X=1|A)=0, whereas P(X=2|A) = 1/3, P(X=3|A) = 1/3, and P(X=4|A) = 1/3.</li>
                <li><strong>Joint PMFs</strong> Given two random variables X and Y of the same experiment, we might be interested in the probability of events where X has some value and Y has some value, simultaneously.  So we want to know the probabilities of all pairs of (x, y) that arise from each outcome in the sample space.  This is the join probability of X and Y, written p<sub>X,Y</sub>(x, y) = P(X=x and Y=y).  Think of X taking on value along the x axis, and Y taking on values along the y axis, and every outcome in the sample space gets mapped to some (x, y) under X and Y.  We could plot the probability of getting (x,y) as a value on the z axis.  Joint PMFs must have sum over all x, sum over all y, of P(X=x and Y=y) = 1, so it is a normalized probability distribution.  This extends to 3 or more random variables, e.g. with three random variables X, Y, Z we write the join probability p<sub>X, Y, Z</sub>(x, y, z) = P(X=x and Y=y and Z=z).</li>
                <ul>
                    <li><strong>Marginal PMF</strong> This is when you look at only one of the random variables at a time, p<sub>X</sub>(x) = sum over all y of p<sub>X, Y</sub>(x,y).  Which gives you the probability of getting the value x, regardless of the value of y.  This extends to three or more random variables, written as p<sub>X</sub>(x) = sum over all y and sum over all z of p<sub>X, Y, Z</sub>(x, y, z), which you can think of as fix x, and sum up the PMF over all y and z for that fixed x.  Do that for each x to get the marginal PMF of x.</li>
                    <li><strong>Conditional PMF</strong> This is when you give a condition to one of the random variables, and you want to know the probability of getting values of the other random variable given that condition.  Think of fixing Y to be some value y, and you want to know the probability of getting each value of x given that Y=y, then you have p<sub>X, Y</sub>(x|y) = P(X=x|Y=y) = p<sub>X, Y</sub>(x, y) / p<sub>Y</sub>(y).  You must have sum over all x of p<sub>X,Y</sub>(x|y) = 1 so the conditional PMF is still a normalized probability distribution.</li>
                    <li><strong>Multiplication Rule</strong> This applies the probability multiplication rule of P(A and B) = P(A)*P(B|A) to joint PMFs, as p<sub>X, Y</sub>(x, y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x).  This extends to three or more random variables, as p<sub>X, Y, Z</sub>(x, y, z) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) * p<sub>Z|X,Y</sub>(z|x,y)</li>
                    <li><strong>Independence</strong> Multiple PMFs are independent if their joint PMF equals the product of their marginal PMFs for each value of each random variable.  e.g. for random variables X, Y, and Z, they are independent if p<sub>X, Y, Z</sub>(x,y,z) = p<sub>X</sub>(x) * p<sub>Y</sub>(y) * p<sub>Z</sub>(z) for all x, y, z.  Intuitively, independence means that e.g. for two random variables, the outcome of Y does not change your beliefs about the outcome of X, p<sub>X|Y</sub>(x|y) = p<sub>X</sub>(x) (note this equation only works for p<sub>Y</sub>(y) nonzero so the prior definition of independence is more general).  Or for three random variables, p(X=x|Y=y, Z=z) = p(X=x). </li>
                </ul>
                <li><strong>Geometric PMF</strong> This PMF describes a discrete random variable with probabilities for successive values that decrease geometrically.  Consider the experiment of repeating a coin toss until you get heads, where you have probability p of getting heads on each toss.  Consider the random variable X from the outcomes in the sample space (e.g. H, TH, TTH, TTTH, etc.) to the number of tosses made.  The probability of k tosses will be (1-p)^(k-1) * p.  So p<sub>X</sub>(k) = P(X=k) = p * (1-p)^(k-1), where k=1,2,3,...  Thus P(X=1) = p, P(X=2) = p(1-p), P(X=3) = p(1-p)^2, and so on, where you are adding another factor of (1-p) for each successive value of X.  The PMF is highest at 1 and decreases with each value of X by a factor of (1-p), so this random variable has a geometric PMF and is called a geometric random variable.</li>
                <ul>
                    <li><strong>Memorylessness</strong> The geometric PMF is memoryless, meaning P(X>m+n | X>=m) = P(X>n) for all m and n in {0,1,2,...}.  The only memoryless discrete probability distribution or PMF is the geometric PMF.  If you consider the coin tossing example above, it means that if you for example toss two tails to start with, you want to know what is the probability of getting heads on subsequent tosses given you had two tails to start.  If X represents the number of tosses to get heads, then you are given the condition that X>2, and you are interested in P(X|X>2).  Well, since the past coin flips don't influence future ones, you know that P(X=3|X>2)=P(X=1), and P(X=4|X>2)=P(X=2).  Think of a tree of all possible outcomes for X, and you know the first two branches from the root are both tails, you are asking what is the probability model for the tree after that point, and it is a copy of the whole tree starting at that point.  You might define a random variable Y=X-2, which gives you the number of tosses it takes to get heads after the first two tosses.  So you can say P(Y=k|X>2) = P(X-2=k|X>2) = P(X=k).</li>
                    <li><strong>Expected Value</strong> The expected value of a geometric random variable X with probability of success p is E[X] and can be calculated using a few tricks.  Using the total expectation theorem, we can create two disjoint events, X=1 and X>1.  So, E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1].  First of all we know P(X=1)=p and P(X=1)+P(X>1)=1,so P(X>1)=1-p.  Next we know E[X|X=1] is the sum over x of x*P(X=x|x=1) = 1*1 = 1, because P(X=x|x=1) will be 0 for all terms except when x=1 for which it will be 1.  Finally, we need to know E[X|X>1].  Consider E[X-1], which is the expected value of a linear function of the random variable X, so we know E[X-1]=E[X]-1.  This is also true for the conditional expectation, E[X-1|X>1] = E[X|X>1]-1 (shown using the definition of expectation and using conditional probabilities). From this we get E[X|X>1] = E[X-1|X>1]+1.  Well, E[X-1|X>1] is the expected value of the number of trials after the first trial to get success, given the first trial was a failure and due to memoryless this should be equal to E[X] itself.  Thus E[X|X>1] = E[X]+1, which is basically saying that the expected value of the number of trials until success given the first trial was a failure, is the same as the expected value of the number of trails until success if you were starting over, plus one.  Putting everythign together we get E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1] = p + (1-p)*(E[X]+1)= p + E[X] + 1 - pE[X] - p = E[X] + 1 - pE[X], so 0 = 1 - pE[X], so E[X] = 1/p.</li>
                </ul>
                <li><strong>Binomial Distribution</strong> This PMF describes a discrete random variable that is the number of successes out of an n-trial experiment where each trial has a boolean outcome with probability p of success and 1-p of failure.  For example, tossing a coin n times, with probability p of heads and 1-p of tails, you get p<sub>X</sub>(k) = P(X = k) = (n k)*p^k*(1-p)^(n-k),  The expected value E(X), or average if you conducted many experiments is E(X) = n*p.  The variance for a single trial is p(p-1), and the variance var(X) for the entire experiment is n*p(p-1). e.g. the flip a coin four times example has a binomial distribution, the n=4 and p=0.5 (say succes is getting heads), and E(X)=2 adn Var(X)=1.  Another example is randomly guessing 20 questions on a multiple choice test with four options for each question where each option is equally likely to be right, then you would have a binomial distribution for your score with n=20 and p=0.25, and E(X)=5.</li>
                <li><strong>Poisson Distribution</strong> This is a PMF for discrete random variables, common for counting the number of occurences of something in a given interval of time, like the number of cars through an intersection during a given hour of the day. It has a parameter λ that is the expected value or average value of the distribution.  This is the result of making the observation many times, e.g. 10 cars one day, 7 cars the next, 15 the next day, etc. - the average value of those counts as the number of observations approaces infinity (law of large numbers) is the expected value.  The variance or spread of the distribution is always equal to λ, thus bigger expected values always have a bigger variance.</li>
                <li>Python's scipy.stats library has a method to calculate the probability mass function value for a given random variable value of a binomial distribution and poisson distribution.  For the binomial distribution it is binom.pmf(value to calculate, number of trials, probability of success on each trial).  E.g. with the flip a coin four times example, you could get the probability of flipping 2 heads with binom.pmf(2, 4, .5).  For the poisson distribution, it is poisson.pmf(value to calculate, expected value). E.g. if the expected value of rainy days in the next month is 10, and you wanted to know the probability of having 6 you could use poisson.pmf(6, 10) </li>
                <li>The scipy.stats library also has a method for generating a list of random values that follow the poisson distribution (probably one for binomial too).  You can use poisson.rvs(expected value, size = num of values), e.g. poisson.rvs(10, size=1000) would generate a list of 1000 values following a poisson distribution with expected value 10.  You could then plot this or find the mean, or min and max values, etc.</li>
            </ul>
            <li><strong>Probability Density Function</strong> These define the probability distribution of a continuous random variable, by definition the probability distribution of a continuous random variable is the integral of the probability density function, which is the cumulative ditribution function.  Instead of mapping discrete values of a discrete random variable to probabilities, you take the area under the curve to get the probability of a range of values of the random variable.  Since the range of a continuous random variable is infinite, any single value of the random variable has zero probability, but a range of values is also a continuum and this can be assigned a probability within the range of the random variable.</li>
            <ul>
                <li>The normal distribution is common for use in probability density functions and is parameterized by the mean μ (mu) of the distribution and standard deviation σ (sigma).</li>
            </ul>
            <li><strong>Cumulative Distribution Function (CDF)</strong> This can be derived from the Probability Mass Function or Probability density function and gives the probability of observing a specific value or less, i.e. CDF(X=n) = PMF(X &lt= n).  The CDF is always increasing, and the highest value of X will equal 1. The CDF is handy for calculating ranges of a discrete random variable, e.g. if you want to know P(3&lt=X&lt=6) = CDF(X=6) - CDF(X=2).  Or if you want to know P(X>6) = 1 - CDF(X=6).  This applies to continuous random variables also, where the CDF is helpful for getting the area under the curve.  You can get 1)the probability that the random variable is less than a certain amount by getting the value of the CDF at that point, 2)the probability of a range of the random variable by subtracting two values of the CDF at the endpoints of the range, or 3) the probability that the random variable is greater than a certain amount by subtracting the value of the CDF at that point from 1.</li>
            <ul>
                <li>Python's scipy.stats library has a method to calculate the cdf for a given random variable value of a binomial distribution or a poisson distribution. For binomial, it is binom.cdf(value to calculate, number of trials, probability of success on each trial).  For poisson, it is poisson.cdf(value to calculate, expected value).</li>
                <li>The scipy.stats library also has method to calculate the cdf for a normal distribution.  norm.cdf(value to calculate, mean of distribution, standard deviation)</li>
            </ul>
        </ul>
        <li><strong>Expected Value</strong> Represented as E[X], the expected value of a discrete random variable X is E[X] = Sum over x of x * p<sub>X</sub>(x).  Loosely speaking, if you interpret the probability for each value x of a random variable X given by a PMF as the frequency of getting x if you run the experiment infinite times, then the expected value would give you the average value of x you would get.  E[X] could also be interpreted as the center of gravity of the PMF of X, which can make it easy to visualize the value of E[X].  If you interpret the probability of each value of x as a weight at point x, the center of gravity is the point along the line x where it would balance perfectly.  So a uniform discrete probability from 1 to n, would have a center of mass right in the middle at n/2 which is the expected value.</li>
        <ul>
            <li><strong>Conditional Expectation</strong> This is the expected value of a conditional PMF, denoted E[X|A] = Sum over x of x * P(X=x|A) = Sum over x of x * p<sub>X|A</sub>(x). It is no different than a regular expected value, you are simply using the conditional PMF for the random variable X.  Following the example above for conditional PMF, you may have a random variable X with a PMF of P(X=k)=1/4 for k in {1, 2, 3, 4} otherwise P is 0, so E[X]=2.5.  If event A is X>=2, then P(X=k|A)=1/3 for k in {2, 3, 4} otherwise P is 0, so E[X|A]=3.</li>
            <li><strong>Functions of a Random Variable</strong> You may have a random variable Y that is a function of another random variable X, say Y=g(X).  E.g. X maps the sample space to x, and Y maps x to y.  Thus, Y will also be a function of the sample space and is a random variable.  The expected value of Y is E[Y] = sum over y of y*P(Y=y).  This requires you to know the PMF of Y so you can plug in the probabilities.   You may not know or want to figure out the PMF of Y and instead know and want to rely on the PMF of X.  It can be derived that E[Y] = sum over x of g(x)*P(X=x).  To prove this intuitively, many values of x may map to the same value of y, and this formula simply adds up the probability of each x times the y it gets mapped to.  In the end the sum of all the probabilities of x that map to a given y is the same as the probability of y given by the PMF of Y, so it is equivalent to the first way of calculating E[Y].  In general E[g(x)] does not equal g(E[X]), although very importantly it is equal if g is a linear function of the random variable X, as shown below.</li>
            <li><strong>Linear Functions of a Random Variable</strong> If you consider a constant to be a random variable where every outcome of the sample space is always mapped to the same constant number, then the PMF for that random variable would be 0 everywhere except at the constant where it would be 1.  The expected value of this random variable would be the constant.  So, E[a] = a if a is a constant.  (E[X] = sum over x of x*P(X=x), P(X=x)=0 everywhere except at a where P(X=a)=1, so the sum has only one nonzero term which is a*P(X=a) = a*1 = a).  Now, consider E[aX], where you can consider a new random variable Y is a function of X, so Y=g(X)=aX.  Well E[Y] = sum over x of g(x)*P(X=x) = sum over x of a*x*P(X=x) = a * sum over x of x*P(X=x) = a * E[X].  So we have E[aX] = aE[X] where a is a constant.  e.g. X could be people's heights in inches, and Y is people's height in cm, so a = 2.54.  Y simply scales X by 2.54, and the expected value of X is simply scaled by 2.54 as well.  Finally, E[aX + b] with a and b constants, where Y = g(X) = aX+b, then E[Y] = sum over x of g(x)*P(X=x) = sum over x of (ax+b)*P(X=x) = sum over x of ax*P(X=x) + sum over x of b*P(X=x) = a * sum over x of x *P(X=x) + b*sum over x of P(X=x) = aE[X] + b. So E[aX+b] = aE[X] + b.</li>
            <li><strong>Total Expectation Theorem</strong> Consider a sample space divided into n disjoint events A1, A2, ... An.  Consider a random variable of that sample space, X.  You can ask what is P(X=x) for the PMF of X, or you can ask P(X=x|An) for the conditional PMF of X given the event An occurs.  Well for any given x, P(X=x) should equal P(A1)*P(X=x|A1) + P(A2)*P(X=x|A2) + ... + P(An)*P(X=x|An).  Thus, multiplying both sides by x and taking the sum over all x, gives sum over x of x*P(X=x) = sum over x of (x*P(A1)*P(X=x|A1) + x*P(A2)*P(X=x|A2) + ... + x*P(An)*P(X=x|An)), which means E[X] = P(A1)E[X|A1] + P(A2)E[X|A2] + .... + P(An)E[X|An].</li>
            <li><strong>Joint PMFs</strong> If you have a function of random variables that involves two or more random variables, g(X, Y), and want to know its expected value.  g(X,Y) is itself a random variable, say Z, so as usual, E[Z] = sum over z of z*p<sub>Z</sub>(z).  As with other functions of a random variable, you can show that E[Z] = E[g(X,Y)] = sum over x, sum over y of g(x,y)*p<sub>X,Y</sub>(x, y).  Essentially you are taking the joint PMF of X and Y and for each x and y, multiplying z by that joint PMF, so when multiple x,y pairs gets mapped to the same z value, the joint PMF probabilities will add up to the total probability for that value of z.  So, you don't need to know explicitly the PMF of Z to get E[Z].  In general, E[g(X,Y)] does not equal g(E[X,Y]).  The important exception is in linear functions of multiple random variables. We have E[X+Y+Z] = E[X] + E[Y] + E[Z].  (Consider average test scores on three tests sum over x, sum over y, sum over z of (x+y+z)*p(X=x, Y=y, Z=z) =).  </li>
        </ul>
        <li><strong>Variance</strong> Variance denoted var(X) is the expected value of a particular function of a random variable - the deviation squared.  It is used to measure how spread out a random variable is, and you might first think to define a random variable called the deviation which is X-E[X] and find its expected value, E[X-E[X]] but this will be 0 because the differences are signed and cancel each other out.  Next you might think to take the absolute value of that difference but it is more useful to take the square of that difference which is what variance is.  Consider a random variable X from a sample space to values x.  You can calculate the expected value E[X], which equals a constant. Now consider a function of X, Y = g(x) = (X - E[X])^2.  This subtracts a constant from the random variable X and then squares it, and Y is a legitimate function of the sample space and a random variable itself.  Y represents the distance squared between each value of X and the expected value of X.  Being squared, it gives greater weight to greater distances.  The expected value of Y is the variance, so by definition var(X) = E[Y] = sum over x of g(x)*P(X=x) = sum over x of (x - E[X])^2 * P(X=x).  This can be shown to equal sum over x of (x^2 - 2xE[X] + E[X]^2)*P(X=x) = sum over x of x^2*P(X=x) - sum over x of 2xE[X]*P(X=x) + sum over x of E[X]^2*P(X=x) = E[g(x)=x^2] - E[g(x)=2xE[X]] + E[g(x) = E[X]^2] = E[X^2] - 2E[X]E[x] + E[E[X]^2] = E[X^2] - 2E[X]^2 + E[X]^2 = E[X^2] - (E[X])^2. This result is somewhat simpler way to calculate it.  For a constant a, Var(X+a) = Var(X), Var(aX) = a^2 * Var(X), and Var(X+Y) = Var(X) + Var(Y) if X and Y are independent random variables. </li>
        <ul>
            <li><strong>Linear Functions of a Random Variable</strong> The Variance is always positive since it is a sum of squared values (non negative values).  Adding a constant to a random variable doesn't change it's variance, adding a constant simply translates the random variable and doesn't change the spread of the random variable.  If you multiply a random variable by a constant, then the variance is multiplied by the square of that constant, which can be seen by var(aX) = sum over x of (ax - E[aX])^2 * P(X=x) = sum over x of (a^2*x^2 - 2axE[aX] + E[aX]^2) * P(X=x) = a^2 * sum over x of x^2 * P(X=x) - 2aE[aX] * sum over x of x * P(X=x) + E[aX]^2 * sum over x of P(X=x) = a^2 * E[X^2] - 2a^2E[X]E[X] + a^2 * E[X]^2 = a^2*E[X^2]-2a^2*E[X]^2+a^2*E[X]^2 = a^2*E[X^2] - a^2*E[X]^2 = a^2 * (E[X^2] - E[X]^2) = a^2 * var(X).  Altogether these properties can be shortened to var(aX + b) = a^2 * var(X) where X is a random variable and a and b are constants.</li>
            <li><strong>Standard Deviation</strong> The standard deviation is the square root of the variance of a random variable X, often denoted as σ<sub>X</sub> = sqrt(var(X)).  This is often more convenient than the variance because the variance will be in different units than the value of your random variable, namely it will be in those units squared.  Standard deviation will be in the same units as your random variable.</li>
        </ul>
    </ul>

    <h2>Sampling Distributions</h2>
    <ul>
        <li>We often cannot calculate statistics for an entire population, and thus calculate a statistic from a randomly selected sample of the population, and want to know how certain we can be that we know the true statistic for the entire population.</li>
        <li><strong>Sampling Distribution</strong> If you take a statistic like the mean, median, the min or max value, variance, etc. for a sample of the population, across many samples, you will create a probability distbution for that value which is called a sampling distribution.</li>
        <li><strong>Unbiased and Biased Estimators</strong> If the mean of a sampling distribution for some statistic is equal (approximately) to the actual value of that statistic for the whole population, then the sample statistic is an unbiased estimator.  This is true for the mean of a sample/population.  If the mean of a sampling distribution for some statistic is not centered around the actual value fo that statistic for the whole population, then it is a biased estimator.  This is true for the max and min value of a sample/population.</li>
        <li><strong>Central Limit Theorem</strong> This states that the sampling distribution of the mean for a population will be normally distributed as long as the population is not too skewed, or the sample size is large enough (at least 30 is a good rule of thumb).  You can have a smaller sample size if the population is normally distributed and the central limit theorem will still hold.  CLT says 1) The sampling distribution of the mean will have a mean x that will be close to the true population mean μ. And 2) The sampling distribution of the mean will have standard deviation equal to the population standard deviation over the square root of the sample size σ / sqrt(n). The standard deviation of a sampling distribution is called the standard error of the estimate of the mean. We may not know the population's standard deviation so we often use the standard deviation of a sample in its place.</li>
        <li><strong>Confidence Intervals</strong> Usually we only have one sample, not many that we can make a sampling distribution with.  We want to say how sure we are that the statistic for that sample that we're interested in is close to the actual population statistic.  We can use the standard deviation of the sample as an estimate for the standard deviation of that population.  Then per CLT, we can calculate the standard error of our sample, using the estimate of the standard deviation divided by the square root of the sample size.  Also using CLT, we know that the sampling distribution is normally distributed and centered on the actual population mean. Since a normal distribution has 95% of its values fall with 1.96 standard deviations of the mean, we know that 95% of sample statistics will thus be less than 1.96 * standard error away from the actual mean.  Thus using our standard error estimate and the mean of our one sample, we can say that the mean of our sample is within 1.96 * standard error of the actual population mean.  This interval is called a 95% confidence interval.</li>
        <li>Python's numpy library has a convenient function for creating a random sample from a population.  It is random.choice(population array, sample size, replace), where pop array is an array to choose from, sample size is an integer, and replace should be FALSE because once an item is added to the sample it should not be able to be picked again.  It returns a list of the sample.</li>
    </ul>
</body>
</html>