<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Probability</h1>

    <h2>Set Theory</h2>
    <ul>
        <li><strong>Set</strong> Sets are a collection of elements where each element is unique, and order doesn't matter. Represented by curly braces and often denoted by a capital letter e.g. A = {Book, Folder, Pen, Paper, Hat}</li>
        <li><strong>Subset</strong> One set is a subset of another if each element of the set is contained in the other. e.g. A is a subset of B if A = {1, 3, 2} and B = {4, 2, 1, 5, 3}</li>
        <li><strong>Union</strong> The union of two sets is the set of all elements that appear in at least one of the two sets, written as (A or B)</li>
        <li><strong>Intersection</strong> The intersection of two sets is the set of all elements that appear in both of the two sets, written as (A and B)</li>
        <li><strong>Complement</strong> The complement of a set is all the elements not in the set, but in some superset, written A<sup>C</sup>. In the context of probability, your superset would be the sample space, and the set would be an event, and its complement would be all sample points not in the event.  So, the event and its complement cover the entire sample space.</li>
    </ul>

    <h2>Probability</h2>
    <ul>
        <li><strong>Experiment/Trial</strong> An Experiment is any procedure that can be repeated infinitely and has a well-defined set of possible outcomes. e.g. flipping a coin once is an experiment.  Often, an experiment is repeated many times to be subjected to statistical analysis.  In this case, there is still just a single experiment but it can be thought of as an experiment composed of other experiments which we call trials.  Mathematically an experiment consists of 1) a Sample Space Ω or S of all possible outcomes, 2) a set of events F where each event is a set containing 0 or more outcomes, and 3) A probability measure function P that maps events to probabilities.</li>
        <li><strong>Sample Point or Outcome and Sample Space</strong> A Sample Point or Outcome of an Experiment is the result of a single execution of the experiment.  The Sample Space of an Experiment Ω is the set of all possible outcomes of the experiment.  The outcomes of a sample space should be mutually exclusive (if one happens then another does not) and collectively exhaustive (no matter what happens, the outcome of the experiment is in the sample space) </li>
        <li><strong>Event</strong> An event is a set of outcomes and is a subset of the sample space.  It is often more convenient to consider events than every possible outcome.  An event can consistent of a single outcome which is called an elementary or atomic event.</li>
        <ul>
            <li><strong>Dependence/Independence</strong> Independent events means the outcome of one event does not effect the probability of the other event.  Event B is not dependent on event A means P(B|A) = P(B), because A occuring has no effect on the probability of B occuring.  Since P(B|A) = P(B and A)/P(A) (P(A) not zero) by definition of conditional probability, we have P(B and A) = P(A)P(B) when event B is not dependent on event A.  If P(B) is not zero, then P(B and A)/P(B) = P(A and B)/P(B) = P(A|B) = P(A), so A is not dependent on B.  So we say A and B are independent events and the definition of independence is taken to be P(A and B) = P(A)P(B).  If either P(A) or P(B) is 0, then they are considered independent, and in fact any event with probability 0 is independent of every other event.  More generally, n events are independent only if the probability of the intersection of every combination of those events is equal to the product of those events' probabilities occuring on their own.  This means not only every pair of events, but every combination of three events, four events, up to n events.  e.g. flipping a coin twice, the outcome of the first flip does not effect the outcome of the second flip.  Dependent events means one event does effect the probability of the other, e.g. pulling marbles out of a bag that are either blue or red, the first time you pull a marble out is going to change the probability of what you pull out after that.  For more than two events, consider flipping a coin twice and event A is the first flip is heads, event B is the second flip is heads, and event C is both flips are the same.  Events A and B are independent, Events A and C are independnet, and events B and C are independent because the first event occuring in each of those does not change the probability of the second event occuring, or said another way P(A and B)=P(A)P(B), P(A and C)=P(A)P(C), and P(B and C) = P(B)P(C).  However, A,B,C are not independent events because P(A and B and C) does not equal P(A)P(B)P(C).  This is because P(C|A and B) = 1, i.e. if you know both A and B occured, then you know for sure that C occured.  So C is not independent of (A and B).  Likewise if you know C and A occurred then you know for sure B occurred or if you know C and B occured then you know for sure A occured.</li>
            <li><strong>Mutually Exclusive</strong> Two events are mutually exclusive if their intersection is empty.  e.g. flipping a coin the event of getting heads is mutually exclusive from the even of getting tails.  But rolling a die the event of getting an even number is not mutually exclusive frmo the event of getting a number greater than three. An event and its complement are always mutually exclusive.</li>
        </ul>
        <li><strong>Probability</strong> If we run an experiment an infinite number of times, the probability of a given event is the proportion of times it occurs.  This is known as the frequentist interpretation of probability.  This is denoted mathematically as P(Event) = (Number of times event occurred)/(Number of Trials).  A probabilistic model of something is a mathematical description of an uncertain situation that consists of a sample space Ω of all possible outcomes, and a probability law that assigns (ideally) every possible event A of the sample space a probability P(A).  The axioms for a probability rule are: P(A) >= 0 for all A, P(Ω)=1, and if A and B are disjoint events then P(A and B) = P(A) + P(B) (more generally this is true for the union of a countably infinite sequence of disjoint events)</li>
        <ul>
            <li><strong>Conditional Probability</strong> This is the probabiity of an event A occuring given than another event B occurs, and is written P(A|B). By definition P(A|B) = P(A and B)/P(B), given P(B) is not zero.  If the events are independent, then P(A|B) = P(A) and P(B|A) = P(B).  However this is not true if the events are dependent, e.g. if B is first picking a blue marble out of a bag of red and blue marbles, and A is second picking a red marble, then A is dependent on B because the ratio of red to blue marbles changes after the first pick.</li>
            <li><strong>Addition Rule</strong> The probability of event A happening or event B happening (or both) is P(A or B) = P(A) + P(B) - P(A and B).  If A and B are mutually exclusive, then P(A and B) = 0, but if they are not, then you have to subtract the probability of their intersection or else you'll be double counting.</li>
            <li><strong>Multiplication Rule</strong> The probability of event A happening and event event B happening is P(A and B) = P(A) * P(B|A).  If A and B are independent, the P(B|A) = P(B), but if they are not, then you have to take into account the probability of B occuring given A occuring.  This provides a definition of conditional probability as P(B|A) = P(A and B) / P(A).  </li>
            <li><strong>Bayes' Theorem</strong> This states that P(B|A) = P(A|B)*P(B) / P(A) = P(A and B)/P(A).  This is true because the set A and B = B and A.  Thus P(A and B) = P(B and A), thus P(A) * P(B|A) = P(B) * P(A|B), and thus P(B|A) = P(B) * P(A|B) / P(A)</li>
            <li><strong>Tree Diagrams</strong> This is a way of visually diagramming multiple experiments and their sample space, and you can see how the multiplaction rule works and Bayes' Theorem.</li>
            <ul>
                <li>Here is a diagram for independent events:</li><img src="./coin-tree-diagram.svg" alt="">
                <li>And here is a diagram for dependent events.  The first experiment is whether someone has strep throat, and P(ST) is the event that they do and has a 20% probability.  The second experiment is running a test for strep throat and the events are either a positive or negative test result.  The probabilities of the events are dependent on whether the person has strep throat or not.  If you wanted to know the probability of having strep throat given you had a positive test, then you could use Bayes' theorem: where P(ST|+) = P(+|ST) * P(ST) / P(+)</li>
                <img src="./Conditional_Probability_Application.svg" alt="" style="width: 800px">
            </ul>
        </ul>
        <li><strong>Probability Problem Solving </strong>The approach to any probabilistic problem generally breaks down to 1)Defining a sample space, 2)Defining a probability law on the sample space, 3)Defining an event of interest, and 4)Calculate probability of the event.</li>
        <li><strong>Law of Large Numbers</strong> You can't perform an infinite number of trials of an experiment, but the law of large numbers is that as you perform more and more trials, the probability of any given event will converge to its true probability.</li>
        <li>For example, say the experiment is flipping a coin twice and the observation is which side of the coin lands up on each flip.
        <ul>
            <li>There are four possible sample points for this experiment depending on the side of the first flip and the side of the second flip.  The full sample space could be represented as S = {HH, HT, TH, TT}</li>
            <li>Possible events you might be interested in are getting two heads, A = {HH}, getting two tails, B = {TT}, or getting a heads and a tails, C = {HT, TH}</li>
            <li>You might run 1000 trials of the experiment, and notice that {HH} occurred 252 times. Thus you could estimate the probability of getting two heads with two coin flips is P({HH}) = 252/1000 = .252 = 25.2%</li>
        </ul>
        </li>
    </ul>

    <h2>Combinatorics</h2>
    <ul>
        <li>Combinatorics is the mathematics of counting, you often need to be able to count outcomes in an event or in a sample space.  Counting is really needed in the case of uniform probability.</li>
        <li><strong>Discrete Uniform Probability</strong> When calculating the probability of events in a sample space where every outcome has the same probability, then you simply need to count the number of outcomes in the event and divide by the number of outcomes in the sample space to get the probability.  The hard part can be counting the outcomes in the event of interest and outcomes in the sample space, when you have a large events/sample space, which is where combinatorics comes in handy.  e.g. using the basic counting principles below you can calculate the probability that 6 rolls of a die will each roll a different number as 6! / 6^6.</li>
        <li><strong>Basic Counting principles</strong> Consider multiple stages each with multiple possible choices.  e.g. 1st stage has 2 choices, 2nd stage has 4 choices, 3rd stage has 3 choices.  Then the total number of outcomes across all 3 stages is 2 * 4 * 3.  Think of a tree diagram of this, and starting from the leaves of the tree, you have 3 outcomes repeated 4 times, and then you have that repeates 2 times.  An example of using this counting principle is picking letters/digits for a license plate, say you pick 6 and there are 26 letters and 10 digits for each choice, then you get 36*36*36*36*36*36 possible outcomes.  When you are picking items from a set of items and cannot reuse the items, then e.g. for the license plate you get 36*35*34*33*32*31.  If you go through every item in the set, then each outcome is called a permutation of the n items, and permutations refer to all the possible orderings of n items, and is given by n!.  Finally, an important counting principle is going through a list of n items and choosing to pick each one or not to create a subset, for this you have n trials with 2 outcomes on each, so the number of subsets of n items is 2^n (includes possibility of picking none).</li>
        <li><strong>Combinations</strong> Represented as (n k) except n is stacked on top of k, and said as n choose k.  It is for determining the number of possible subsets of k elements from a set of n elements.  Since they are sets, the order of the k elements does not matter.  If it did matter, then there would be k!/(n-k)! possibilities (example of basic counting principles above).  But we know also from basic counting principles that each subset of k elements has k! different orderings.  So (n k) = k! / ((n-k)!k!).  Note 0! is defined to equal 1 because it makes this formula work in the case that k=n or k=0.  n and k are the binomial coefficients. </li>
        <li><strong>Binomial Probabilities</strong> Consider the experiment of n coin tosses, and the event that you get k heads.  If the probability of getting heads is p, then for any given outcome, the probability for that outcome is p^k * (1-p)^(n-k).  The problem is now counting how many outcomes have k heads and multiplying that by the probability of each outcome above.  Counting how many outcomes have k heads is equivalent to counting how many ways you can pick out k items out the n tosses to be heads, i.e. the number of possible subsets of k elements out of n.  This is simply n choose k, so the probability of k heads in n coin tosses is (n k) * p^k * (1-p)^(n-k).  This applies to any binomial distribution with n independent trials and probability p of success on each trial.  Consider summing up the probability of k heads in n tosses from k=0 to k=n.  This would be equal to 1 since it covers the whole sample space. Additionally, consider the question out of 10 coin tosses, you know 3 heads occurred, and you want to know what is the probability that the first two tosses were heads, so given event A (3 heads) what is the chance of event B (first two are heads)?  Within event A the probability of all outcomes is the same since every outcome has the same number of heads and it is p^3 * (1-p)^(n-3).  So, it is uniform probability inside event A and all you need to do is count the ways B can occur and divide by the number of ways A can occur.  A can occur (n 3) * p^3 * (1-p)^(n-3) ways, and given A, B can occur 8 ways since the only uncertainty in B is where the third head goes, which given the first two are heads, leaves 8 possible spots the third head could occur.  So the answer is 8 divided by the probability of A above.</li>
        <li><strong>Partitioning</strong> A more general combination is partitioning a set into a given number of subsets of given cardinalities.  A combination can be though of as how many ways can I split a set of n elements into 2 subsets, one a subset of k elements and one a subset of n-k elements.  A partion asks how many ways can a split a set of n elements into m subsets of cardinality k<sub>1</sub>, k<sub>2</sub>, ..., k<sub>m</sub>.  This is (n k<sub>1</sub>) * (n-k <sub>1</sub> k<sub>2</sub>) * ... * (n-k<sub>1</sub>-...-k<sub>m-1</sub> k<sub>m</sub>). The last term is always equal to 1, and algebraically this works out to n!/(k<sub>1</sub>! * k<sub>2</sub>! * ... * k<sub>m</sub>!).  e.g. if we want to know how many ways there are to deal 52 cards into 4, 13-card bridge hands, then it would be (52 13)*(39 13)*(26 13)*(13 13) = 52! / (13! * 13! * 13! * 13!).   Now consider the question what is the probability of dealing one ace to each person?  Well, You have the demoninator you need of how many 13-card hands there are total.  The numerator comes from, how many ways are there to distribute 4 aces to 4 hands? There is 4! since you have 4 hands to choose from for the first ace, 3 hands for the second ace, and so on.  Now how many ways are there to distribute the remaining 48 cards to 4 hands? It comes from partioning 48 cards into 4, 12-card partitions.  So, the answer is (4! * (48 12) * (36 12) * (24 12) * (12 12)) / (52 13)*(39 13)*(26 13)*(13 13)</li>
    </ul>

    <h2>Random Variables and Probability Distributions</h2>
    <ul>
        <li><strong>Random Variable</strong> A random variable is a function from a sample space to a set of numbers, generally the reals or integers.  It is generally represented by a capital letter like X, and its values are represented by a lower case letter like x.  A single experiment that generates a sample space could have multiple random variables, e.g. say your experiment is picking a student out of a class, your sample space would consist of one outcome for each student in the class.  You could have a random variable H from each outcome (a student) to the height of that student in inches (a real number or integer) and a second random variable W from each outcome to the weight of that student.  Additionally, you can have random variables defined on other random variables, say Hbar = 2.54*H and gives the height of a student in centimeters, Hbar is still a random variable because each outcome in the sample space maps to a single value of Hbar, and thus Hbar is a function from outcomes in a sample space to numbers.  It divides the sample space up into a series of mutually exclusive events that cover the entire sample space, assigning each event a numeric value.</li>
        <ul>
            <li><strong>Discrete Random Variable</strong> A discrete random variable has a countable range often the integers.  E.g. you could flip a coin four times, creating a sample space of {HHHH, HHHT, HHTH, HHTT, HTHH, HTHT, HTTH, HTTT, THHH, THHT, THTH, THTT, TTHH, TTHT, TTTH, TTTT}.  You could create a random variable that maps this sample space onto the number of heads in each outcome, which will have a range of {0, 1, 2, 3, 4}</li>
            <li><strong>Continous Random Variable</strong> e.g. This is a function from a sample space to the continuum, any real number.  e.g. The temperature measured at a given time is a continuous random variable, or the time it takes a person to run a mile.</li>
            <li><strong>Mixed Random Variable</strong> This is a random variable with both discrete and continuous properties.  E.g. consider a game where you flip a coin, with probability 1/2 of getting heads and 1/2 of getting tails.  If you get heads, you win half a dollar, but if you get tails, you spin a wheel that can land in a continuous range between 0 and 1 that determine how much money you win.  The outcome of this experiment is discrete because there is a definite probability of getting half a dollar (continuous random variables give 0 probability to any one value).  But it is continuous also due to the wheel spin, where you have uniform probability of getting between 0 and 1 dollars.  The probability distribution for this might look like, if you think in terms of having a pound of probability to spread out, you'd put 1/2 pound directly on 0.5 (for flipping heads) and then spread the other 1/2 pound evenly between 0 and 1 (for spinning the wheel).  This mass/density function is weird, but the CDF is well defined - it would be 0 for x&lt0, increase linearly from 0 to 1/4 for x in [0, 1/2), it would jump instantly to 3/4 at x=1/2, then increase linearly from 3/4 to 1 for x in [1/2, 1], then it would be 1 for x>1.</li>
            <li>Python's numpy library has a convenient method for simulating a random event a given number of times random.choice(list to choose from, number of times to choose, replace after choosing), where the first arg could be a list 1-6 for a die, the second arg is 5 for rolling it 5 times, and the third arg is TRUE because you can roll the same value again.  It outputs a list of the results of its random choices.</li>
        </ul>

        <li><strong>Probability Distribution</strong> A general discription of a probability distribution is a function P: A->R, where A is related to the sample space S and the output R (reals) assigns a probability.  If S is the sample space, then A is the set of all subsets of S whose probability can be measured, i.e. all possible events.  Often, we use a random variable X to transform a sample space to a number (e.g Reals or Integers), and speak of the probability distribution of the random variable, which replaces the sample space S in the description above, and the arguments to the probability function are subsets of X.  Once a random variable is defined, you can ask what the probabiltiy of a given output value of the random variable.  Mapping all possible values of the random variable to their probability is the probability distribution of the random variable.</li>
        <ul>
            <li><strong>Probability Mass Function (PMF)</strong> This is how probabilities are assigned for discrete random variables.  Consider a random variable X, with values x mapped from each outcome in a sample space.  You are interested in the probability of each value of x, based on the probability of outcomes in the sample space.  This probability is a function from x to a probability, often denoted in a few different ways, p<sub>X</sub>(x) = P(X=x) = P({ω∈Ω s.t. X(ω)=x}), where p<sub>X</sub>(x)≥0 and the sum over all values of x of p<sub>X</sub>(x) equals 1.  The last representation makes explicit you want the total probability of all outcomes in the sample space that map to x under the random variable X.  In general, a random variable X can map many outcomes to the same value, and your task is to add up the probabilities of all those outcomes for each x to determine the probability of x.  e.g. you roll a die twice and could have a random variable F that maps to the first value of the die roll, a random variable S that maps to the second value of the die roll, and a random variable M = min(F, S).  If you want to know what is P(M=2) for example, you know the outcomes (2,2), (2,3), (2,4), (2,5), (2,6), (3,2), (4,2), (5,2), (6,2) are what map to 2 under M.  Since the outcomes have uniform probability of 1/(6*6), you count them and get that P(M=2) = 9/36. You can also look at the PMF over a range of X values, as in P(1&lt=X&lt=3), which equals P(X=1) + P(X=2) + P(X=3). Sometimes it is easier when looking at a range to calculate the probabilities of the X values not in that range and subtract that from 1.  e.g. for the flip a coin four times example, you would map the output range {0, 1, 2, 3, 4} to their probabilities, which are 1/16, 4/16, 6/16, 4/16, 1/16 respectively, thus P(X=1) = 4/16, and P(1&lt=X&lt=3) = P(X=1)+P(X=2)+P(X=3) = 4/16+6/16+4/16 = 14/16</li>
            <ul>
                <li><strong>Conditional PMF</strong> A conditional PMF is the PMF of a random variable X given that some event A has occurred.  It is written as p<sub>X|A</sub>(x) = P(X=x|A).  For instance you may have a random variable X that can take values of 1, 2, 3, or 4 with equal probability of 1/4 for each value, so for k in {1, 2, 3, 4}, P(k) = 1/4. You might know that event A = (X >= 2) has occurred, so you know X doesn't equal 1.  The conditional PMF would have P(X=1|A)=0, whereas P(X=2|A) = 1/3, P(X=3|A) = 1/3, and P(X=4|A) = 1/3.</li>
                <li><strong>Joint PMFs</strong> Given two random variables X and Y of the same experiment, we might be interested in the probability of events where X has some value and Y has some value, simultaneously.  So we want to know the probabilities of all pairs of (x, y) that arise from each outcome in the sample space.  This is the joint probability of X and Y, written p<sub>X,Y</sub>(x, y) = P(X=x and Y=y).  Think of X taking on value along the x axis, and Y taking on values along the y axis, and every outcome in the sample space gets mapped to some (x, y) under X and Y.  We could plot the probability of getting (x,y) as a value on the z axis.  Joint PMFs must have sum over all x, sum over all y, of P(X=x and Y=y) = 1, so it is a normalized probability distribution.  This extends to 3 or more random variables, e.g. with three random variables X, Y, Z we write the join probability p<sub>X, Y, Z</sub>(x, y, z) = P(X=x and Y=y and Z=z).</li>
                <ul>
                    <li><strong>Marginal PMF</strong> This is when you look at only one of the random variables at a time, p<sub>X</sub>(x) = sum over all y of p<sub>X, Y</sub>(x,y).  Which gives you the probability of getting the value x, regardless of the value of y.  This extends to three or more random variables, written as p<sub>X</sub>(x) = sum over all y and sum over all z of p<sub>X, Y, Z</sub>(x, y, z), which you can think of as fix x, and sum up the PMF over all y and z for that fixed x.  Do that for each x to get the marginal PMF of x.</li>
                    <li><strong>Conditional PMF</strong> This is when you give a condition to one of the random variables, and you want to know the probability of getting values of the other random variable given that condition.  Think of fixing Y to be some value y, and you want to know the probability of getting each value of x given that Y=y, then you have p<sub>X, Y</sub>(x|y) = P(X=x|Y=y) = p<sub>X, Y</sub>(x, y) / p<sub>Y</sub>(y).  You must have sum over all x of p<sub>X,Y</sub>(x|y) = 1 so the conditional PMF is still a normalized probability distribution.</li>
                    <li><strong>Multiplication Rule</strong> This applies the probability multiplication rule of P(A and B) = P(A)*P(B|A) to joint PMFs, as p<sub>X, Y</sub>(x, y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x).  This extends to three or more random variables, as p<sub>X, Y, Z</sub>(x, y, z) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) * p<sub>Z|X,Y</sub>(z|x,y)</li>
                    <li><strong>Independence</strong> Multiple PMFs are independent if their joint PMF equals the product of their marginal PMFs for each value of each random variable.  e.g. for random variables X, Y, and Z, they are independent if p<sub>X, Y, Z</sub>(x,y,z) = p<sub>X</sub>(x) * p<sub>Y</sub>(y) * p<sub>Z</sub>(z) for all x, y, z.  Intuitively, independence means that e.g. for two random variables, the outcome of Y does not change your beliefs about the outcome of X, p<sub>X|Y</sub>(x|y) = p<sub>X</sub>(x) (note this equation only works for p<sub>Y</sub>(y) nonzero so the prior definition of independence is more general).  Or for three random variables, p(X=x|Y=y, Z=z) = p(X=x). </li>
                </ul>
                <li><strong>Geometric PMF</strong> This PMF describes a discrete random variable with probabilities for successive values that decrease geometrically.  Consider the experiment of repeating a coin toss until you get heads, where you have probability p of getting heads on each toss.  Consider the random variable X from the outcomes in the sample space (e.g. H, TH, TTH, TTTH, etc.) to the number of tosses made.  The probability of k tosses will be (1-p)^(k-1) * p.  So p<sub>X</sub>(k) = P(X=k) = p * (1-p)^(k-1), where k=1,2,3,...  Thus P(X=1) = p, P(X=2) = p(1-p), P(X=3) = p(1-p)^2, and so on, where you are adding another factor of (1-p) for each successive value of X.  The PMF is highest at 1 and decreases with each value of X by a factor of (1-p), so this random variable has a geometric PMF and is called a geometric random variable.</li>
                <ul>
                    <li><strong>Memorylessness</strong> The geometric PMF is memoryless, meaning P(X>m+n | X>=m) = P(X>n) for all m and n in {0,1,2,...}.  The only memoryless discrete probability distribution or PMF is the geometric PMF.  If you consider the coin tossing example above, it means that if you for example toss two tails to start with, you want to know what is the probability of getting heads on subsequent tosses given you had two tails to start.  If X represents the number of tosses to get heads, then you are given the condition that X>2, and you are interested in P(X|X>2).  Well, since the past coin flips don't influence future ones, you know that P(X=3|X>2)=P(X=1), and P(X=4|X>2)=P(X=2).  Think of a tree of all possible outcomes for X, and you know the first two branches from the root are both tails, you are asking what is the probability model for the tree after that point, and it is a copy of the whole tree starting at that point.  You might define a random variable Y=X-2, which gives you the number of tosses it takes to get heads after the first two tosses.  So you can say P(Y=k|X>2) = P(X-2=k|X>2) = P(X=k).</li>
                    <li><strong>Expected Value</strong> The expected value of a geometric random variable X with probability of success p is E[X] and can be calculated using a few tricks.  Using the total expectation theorem, we can create two disjoint events, X=1 and X>1.  So, E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1].  First of all we know P(X=1)=p and P(X=1)+P(X>1)=1,so P(X>1)=1-p.  Next we know E[X|X=1] is the sum over x of x*P(X=x|x=1) = 1*1 = 1, because P(X=x|x=1) will be 0 for all terms except when x=1 for which it will be 1.  Finally, we need to know E[X|X>1].  Consider E[X-1], which is the expected value of a linear function of the random variable X, so we know E[X-1]=E[X]-1.  This is also true for the conditional expectation, E[X-1|X>1] = E[X|X>1]-1 (shown using the definition of expectation and using conditional probabilities). From this we get E[X|X>1] = E[X-1|X>1]+1.  Well, E[X-1|X>1] is the expected value of the number of trials after the first trial to get success, given the first trial was a failure and due to memoryless this should be equal to E[X] itself.  Thus E[X|X>1] = E[X]+1, which is basically saying that the expected value of the number of trials until success given the first trial was a failure, is the same as the expected value of the number of trails until success if you were starting over, plus one.  Putting everythign together we get E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1] = p + (1-p)*(E[X]+1)= p + E[X] + 1 - pE[X] - p = E[X] + 1 - pE[X], so 0 = 1 - pE[X], so E[X] = 1/p.</li>
                </ul>
                <li><strong>Binomial Distribution</strong> This PMF describes a discrete random variable that is the number of successes out of an n-trial experiment where each trial has a boolean outcome with probability p of success and 1-p of failure.  For example, tossing a coin n times, with probability p of heads and 1-p of tails, you get p<sub>X</sub>(k) = P(X = k) = (n k)*p^k*(1-p)^(n-k), the probability of k successes, where k is between 0 and n. e.g. the flip a coin four times example has a binomial distribution, the n=4 and p=0.5 (say succes is getting heads), and E(X)=2 adn Var(X)=1.  Another example is randomly guessing 20 questions on a multiple choice test with four options for each question where each option is equally likely to be right, then you would have a binomial distribution for your score with n=20 and p=0.25, and E(X)=5. 
                <ul>
                    <li><strong>Expected Value</strong> The expected value E(X), or average if you conducted many experiments is E(X) = n*p.  Algebraically you could show this by doing sum over k from 0 to n of k * p(X=k), with p(X=k) defined above.  An easier way of finding the expected value is to create n random variables X<sub>i</sub> for i=1 to i=n, where X<sub>i</sub> is 1 if the ith trial is a success or 0 otherwise.  X<sub>i</sub> is known as an indicator variable.  We have that X = sum over i of X<sub>i</sub>, since X is a count of the number of successes and can be between 0 and n.  We also have that for any i, E[X<sub>i</sub>]=sum over x<sub>i</sub> of x<sub>i</sub>*P(X<sub>i</sub>=x<sub>i</sub>) = 0*(1-p)+1*p = p.  Since the sum over i of X<sub>i</sub> is a linear combination of random variables, we have E[X] = E[sum over i of X<sub>i</sub>] = sum over i of E[X<sub>i</sub>] = sum over i of p = n*p.</li>
                    <li><strong>Variance</strong> Using the same indicator variables from the expected value, we know the variance of a single X<sub>i</sub> is E[(x<sub>i</sub> - E[X<sub>i</sub>])^2] = sum over x<sub>i</sub> of (x<sub>i</sub> - p)^2 * P(X<sub>i</sub>=x<sub>i</sub>) = (0-p)^2*(1-p) + (1-p)^2*p = p^2*(1-p) + (1 - 2p + p^2)*p = p^2 - p^3 + p - 2*p^2 + p^3 = p - p^2 = p(1-p).  Since each X<sub>i</sub> is independent, we have var(X) = var(sum over i of X<sub>i</sub>) = sum over i of var(X<sub>i</sub>) = sum over i of p(1-p) = n*p*(1-p).  So, var(X) = np(1-p).
                </ul>
                </li>
                <li><strong>Poisson Distribution</strong> This is a PMF for discrete random variables, common for counting the number of occurences of something in a given interval of time, like the number of cars through an intersection during a given hour of the day. It has a parameter λ that is the expected value or average value of the distribution.  This is the result of making the observation many times, e.g. 10 cars one day, 7 cars the next, 15 the next day, etc. - the average value of those counts as the number of observations approaces infinity (law of large numbers) is the expected value.  The variance or spread of the distribution is always equal to λ, thus bigger expected values always have a bigger variance.</li>
                <li>Python's scipy.stats library has a method to calculate the probability mass function value for a given random variable value of a binomial distribution and poisson distribution.  For the binomial distribution it is binom.pmf(value to calculate, number of trials, probability of success on each trial).  E.g. with the flip a coin four times example, you could get the probability of flipping 2 heads with binom.pmf(2, 4, .5).  For the poisson distribution, it is poisson.pmf(value to calculate, expected value). E.g. if the expected value of rainy days in the next month is 10, and you wanted to know the probability of having 6 you could use poisson.pmf(6, 10) </li>
                <li>The scipy.stats library also has a method for generating a list of random values that follow the poisson distribution (probably one for binomial too).  You can use poisson.rvs(expected value, size = num of values), e.g. poisson.rvs(10, size=1000) would generate a list of 1000 values following a poisson distribution with expected value 10.  You could then plot this or find the mean, or min and max values, etc.</li>
            </ul>
            <li><strong>Probability Density Function</strong> These define the probability distribution of a continuous random variable.  In principle, you would be interested in the probability that a continuous random variable falls in an interval between a and b, and you would go back to the sample space and add up the probability of all the outcomes that map to a value between a and b under the random variable.   As with discrete random variables, we don't want to go back to the sample space all the time to compute probabilities, but instead of pmf which places a certain amount of weight at each discrete value of the random value which defines the probability and adds up to 1, we define the density of weight along the continuum of a continuous random variable, with a probability density function f<sub>X</sub>(x).  We define that a random variable is continuous if we can describe the probability of getting a value between a and b as P(a≤X≤b) = integral from a to b of f<sub>X</sub>(x)dx.  This is analagous to a discrete random variable, if you want to know the total probability of X in [a, b], you would sum over x in [a, b] of P(X=x).  Any individual value a of a continuous random variable has probability 0 because the integral from a to a will be zero regardless of the value of the density function.  We require any density function to have f<sub>X</sub>(x) ≥ 0 for all x, and for integral from -∞ to ∞ of f<sub>X</sub>(x)dx = 1.  (Notice we don't require f<sub>X</sub>(x)≤1, unlike with PMFs where every value of the PMF must be less than 1). The probability density function returns values of probabilty per unit length, not probability itself like a pmf.  Over a small interval [x, x+δ], the probability that the random variable will take value x is approximately δ*f<sub>X</sub>(x).  We may have a set B that might be the union of multiple intervals of the random variable, and we can integrate over B to get the probability that X is in B.</li>
            <ul>
                <li><strong>Joint PDFs</strong> Like Joint PMFs, we might have two continuous random variables X and Y on some sample space, and we want to know the probabilities of pairs (x, y) that outcomes in the sample space map too.  So we need to define a distribution for pairs (x,y).  We say that X and Y are said to be jointly continuous if we can calculate the probability of (x,y) being in a set S (an event) using a double integral over S of f<sub>X,Y</sub>(x, y)dxdy, where f is the joint PDF.  We require f<sub>X,Y</sub>(x,y) ≥ 0 and the double integral over the entire xy plane is 1.  A joint PDF gives you probability per unit area, so you integrate that over an area to get probability.  Intuitively, for a small interval [x, x+δ] and [y, y+δ], the probability that (x,y) will be in that set is approximately δ<sup>2</sup>*f<sub>X,Y</sub>(x,y).</li>
                <ul>
                    <li><strong>Marginal PDF</strong> When you have a joint PDF with random variables X and Y, you may be interested in probabilities of getting values of X regardless of the value of Y.  Over a small interval, we want the probability P(x≤X≤x+δ) to be approximately δ*f<sub>X</sub>(x), and we want to give a meaning to f<sub>X</sub>(x) as the marginal PDF of X.  In terms of the joint PDF, this probability could be found by integrating over the area X in [x, x+δ] and Y from -∞ to ∞.  So, it is the double integral of y from -∞ to ∞ and x from x to x+δ of f<sub>X,Y</sub>(x,y) dx dy.  Since δ is small, the integral over x evaluates to approximately δ * f<sub>X,Y</sub>(x,y) at each value of y.  Then we integrate that over y from -∞ to ∞.  So we have that δ*f<sub>X</sub>(x)≈P(x≤X≤x+δ)=integral from -∞ to ∞ of δ * f<sub>X,Y</sub>(x,y) dy. Dividing out δ, this gives f<sub>X</sub>(x) = integral from -∞ to ∞ of f<sub>X,Y</sub>(x,y) dy</li>
                    <li><strong>Conditional PDF</strong> This is when you want to "fix" a value of one variable Y in a joint PDF, and get a PDF for the other variable X given that Y=y.  However, you can't "fix" values of Y since the probability of a single value is always 0, so really you want to know the PDF of X when Y is in an infintesimally small interval y to y+δ.  Also, recall the PDF doesn't tell you the probability of getting a particular value of X, rather it gives you the approximate probability that x will be in a small interval by multiplying the PDF at a point x by the length of that small interval.  Given all that, we want to define a conditional PDF f<sub>X|Y</sub>(x|y) so that P(x≤X≤x+δ|Y≈y) ≈ f<sub>X|Y</sub>(x|y)*δ.  We are using the Y≈y to say not that Y=y but Y is in an infintesimally small interval around y.  This leads us to define the conditional PDF as f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y).  Notice this is analagous to the discrete case.  It essentially gives you a normalized slice of the PDF at Y=y.  </li>
                    <li><strong>Multiplication Rule</strong> This is simply derived from the definition of a conditional PDF. f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x).  This is useful for finding the joint probability function when you know the probability of X, and the conditional probability of Y, and X and Y are not independent.  Consider breaking a stick of length l at some point X, and then breaking the remaining stick of length X again at a point Y.  The PDF of X is uniform assuming we are equally likely to break it at any point from 0 to l. Y is dependent on X because the value of Y you can get depends on what X was, it can be anywhere between 0 and X, and we asssume it is uniform on that interval.  (Imagine a tree diagram where you first have all the values of X between 0 and l all equally likely.  Then each of those branch out to possible Y values between 0 and X.  The Y branches will be different for each X, there will be more Y branches for larger values of X and less for smaller values of X.)  To get the probability of each outcome of the tree diagram (the experiment), you multiply the branches together, i.e. the probability of getting the X in that outcome, then the probability of getting the Y given that X.  So the probability density of any given outcome x, y is given by f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x) = 1/l * 1/x, where 0&ltx&ltl and 0&lty&ltx.</li>
                    <li><strong>Independence</strong> We say X and Y are independent if f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y</sub>(y).  This is interpreted the same as in teh discrete case, where we are saying there is no conditional probability of y if you know x or vice-versa.  i.e. X doesn't change your beliefs about Y, and vice versa. It can be shown for independent random variables, P(X in A, Y in B) = P(X in A) * P(Y in B) where A and B are some events. If X and Y are independent, then f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x) * f<sub>Y</sub>(y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x), so the conditional probability of X is the same as the marginal probability of X, i.e. the value of Y doesn't have any impact on the PDF of X.</li>
                </ul>
                <li><strong>Uniform Distribution</strong> This would be a PDF that is 0 everywhere except for some interval [a, b].  That is,  f<sub>X</sub>(x) = 1/(b-a) for [a,b] and 0 elsewhere.  It mean that the random variable has the same probability of being in any two intervals of the same length inside of [a,b], and no probability of being outside the interval [a, b].  Notice that the total area under the curve is 1/(b-a) * (b-a) = 1, as required to be a PDF.  It's expected value is E[X] = integral from a to b of x* 1/(b-1)dx = (a+b)/2.  Intuitively this makes since as the center of mass of uniform density mass should be at the midpoint of the mass.  The variance is var(X) = σ<sub>X</sub><sup>2</sup> = integral from a to b of (x - (a+b)/2)<sup>2</sup> * 1/(b-a) dx = (b-a)<sup>2</sup>/12.  Thus, the standard deviation is σ<sub>X</sub> = (b-a)/sqrt(12), which intuitively makes sense that it is proprotional to the width of the interval [a,b].</li>
                <li><strong>Gaussian</strong> The Gaussian (normal) distribution is the most important probability distribution in probability theory, it shows up all over the place. Basically, if you have a random variable that can be decomposed into the sum of many independent random variables, then it will have an approximately normal distribution.  The standard normal N(0, 1): f<sub>X</sub>(x) = 1/sqrt(2pi) * e^(-x^2 /2).  To think about the shape, consider 1/(2^x), then 1/(2^abs(x)), then 1/(2^(x^2)).  The first illustrates the decreasing exponential function, then the second just reflects it across the y axis, then the third scales it horizontally in a dynamic ways, stretching the function horizontally between -1 and 1, with the most stretching happening at 0, and gradually stretching less out to -1 and 1, and then outside of -1 and 1 the function is squished horizonatlly more and more out to infinity.  The 1/sqrt(2pi) ensures the integratal from -∞ to ∞ is 1 as required to be a PDF.  The expected value E[X]=0 due to symmetry, and the variance var(X)=1 (can be shown with some calculus).</li>
                <ul>
                    <li><strong>General Normal</strong> The standard normal can be manipulated to translate it to have a different expected value and variance.  The general normal is N(μ, σ<sup>2</sup>): f<sub>X</sub>(x) = 1/(σ * sqrt(2pi)) * e^(-(x-μ)^2 / 2σ^2).  Consider that you are translating horizontally by μ by replacing x with x-μ, and going back to considering 1/(2^abs(x)), if you consider 1/(2^(abs(x)/c)), bigger values of c mean a bigger exponent which means the function decreases more rapidly.  This is essentially the underlying action of the σ^2 term in the exponent of the general normal, where the smaller sigma, the quicker the function decreases, and the larger sigma means the function decreases less quickly.  This corresponds to more or less variance.  the constant term at the beginning  of the normal function is scaled apporpriately to ensure the area under it is still 1.  With this formula, it turns out if you do the math you get E[X] = μ and var(X) = σ<sup>2</sup></li>
                    <li><strong>Function of Normal Random Variables</strong> As with any random variable X, a linear combination Y = aX + b has E[Y] = aE[X] + b, and var(Y) = a^2var(X).  So, for a normal random variable X has E[aX+b] = aμ+b and var(aX+b) = a^2*σ^2.  However, a nontrivial fact about normal random variables (not random variables in general) is that linear combinations of a normal random variable are themselves normal.  So Y=aX+b is also normal, and is the general normal N(aμ+b, a^2*σ^2).  Intuitively, consider that aX+b = a(X+b/a), so you are just translating x horizontally by b/a, then scaling that horizontally by a, which will still result in a normal shaped curve.</li>
                    <li><strong>Calculating Probabilities</strong> The CDF of the general normal distribution is the integral from -∞ to x of the normal, and this integral does not have a closed form.  So, calculating probabilities of a normal random variable having some value lying within some interval is normally done with a table of precalculated values.   You can use a table of CDF values for the standard normal N(0, 1) for any general normal.  The standard normal tables normally give you values of the CDF from 0 to about 2 in increments of 0.01.  So, 0 will have probability 0.5 since you have a 0.5 probability of X being less than 0.5, and will range up to probability 0.9817 for values less than or equal to 2.09.  You can "standardize" a general normal X with N(μ, σ^2) by applying a linear transformation of Y=(X-μ)/σ = (1/σ)X + (-μ/σ), and Y will be a normal PDF with E[Y]=(1/σ)E[X]+(-μ/σ)=(1/σ)μ+(-μ/σ)=0 and var(Y)=(1/σ)^2 * var(X) = (1/σ)^2 * σ^2 = 1.  Thus Y is N(0, 1), the standard normal.  This can be used to calculate CDF values of X using the standard normal table: P(X≤x) = P(Y≤(x-μ)/σ).  Intuitively, consider  that Y=(X-μ)/σ translates X horizontally so that the mean is moved to 0 for Y, then divides by X's standard deviation, so that the Y values are expressed as multiples of the standard deviation of X.</li>
                    <li><strong>Joint PDF of two Independent Normal Distributions</strong> The joint PDF of two Independent, normally distributed random variables X and Y, is f<sub>X,Y</sub>(x,y)=f<sub>X</sub>(x)*f<sub>Y</sub>(y) and called a bivariate normal.  This works out to 1/(σ<sub>x</sub> * σ<sub>y</sub> * sqrt(2pi)) * e^(-(x-μ<sub>x</sub>)^2 / 2σ<sub>x</sub>^2 - (y-μ<sub>y</sub>)^2 / 2σ<sub>y</sub>^2).  As usual, think of X and Y values being on the 2D plane, with the joint PDF plotted along the z axis.  The PDF will have a constant value along concentric ellipses in the XY plane, because the exponent will evaluate to a constant along the ellipses defined by (x-μ<sub>x</sub>)^2 / 2σ<sub>x</sub>^2 + (y-μ<sub>y</sub>)^2 / 2σ<sub>y</sub>^2.  This exponent term is the equation of an ellipse centered at (μ<sub>x</sub>, μ<sub>y</sub>).  The radius of the ellipse in the X direction increases as σ<sub>x</sub> increases, and the radius in the Y direction increase as σ<sub>y</sub> increases.  The joint PDF looks like a 3 dimensional bell, that is highest at (μ<sub>x</sub>, μ<sub>y</sub>) and decreases along concentric ellipses, whose radii are smaller/larger along the X or Y axis if σ for that axis is smaller/larger.  When σ<sub>x</sub>=σ<sub>y</sub>, you get it decreasing along concentric circles.</li>
                    <li><strong>Joint PDF of two Dependent Normal Distributions</strong>  This similarly forms a 3 dimensional bell shape PDF like with two independnent normal distributions.  However for independent normals, the axes of the ellipse are always parallel to the X-Y axes.  Thus when you take a slice of the bell at a particular value of X, you simply get the normal distribution of Y scaled (multiplied) by the probability of that value of X.  If you divide by the probability of X then you renormalize that slice of Y and so the normalized probability distribution of Y is the same at every value of X.  The most likely Y is always in the same place, etc.  The same argument holds for X on fixed values of Y.  For dependent normal distributions, the axes of the ellipse will not be parallel to the axes of X and Y.  Thus, imagine fixing X and taking the slice at X=x.  For different values of x, the distribution of Y will fundamentally change, e.g. if x is at the mean of x, then the distribution of Y is greatest at the mean of y, but if x is greater than the mean of x, you will slice through the bell in a way that makes the distribution of Y greatest at some point other than the mean of y.  So you might have for values of X greater than the mean of X, you expect to get values of Y greater than the mean of Y, or perhpas less than the mean of Y depending on the orientation of the joint PDF.</li>
                    <li><strong>The sum of Two Independent Normal Distributions</strong> If X and Y are independent, normally distributed random variables, then their sum W = X+Y is also normal.  This is because, as discussed below for derived distributions, f<sub>W</sub>(w) = integral over all x of f<sub>X</sub>(x)*f<sub>Y</sub>(w-x) dx.  If you plug in the PDF equations for the general normal of X and Y and integrate, you get an answer of the form ce^(-γw^2), which is a normal distribution with mean equal to μ<sub>x</sub> + μ<sub>y</sub>, and variance equal to σ<sub>x</sub>^2 + σ<sub>y</sub>^2.  The mean and variance of W shouldn't be a surprise because for any linear functions of random variables, E[X+Y]=E[X]+E[Y], and for independent random variables var(X+Y)=var(X)+var(Y).  The surprising thing is that W is normal.</li>
                </ul>
            </ul>
            <li><strong>Cumulative Distribution Function (CDF)</strong> This is a concept that usefully applies to both discrete and continuous random variables, both PMFs and PDFs, as well as mixed random variables, allowing a unifying concept for thinking about and proving things for all types of random variables.  It is defined as the probability F<sub>X</sub>(x) that a random variable will take a value less than or equal to x, i.e. F<sub>X</sub>(x) = P(X≤x) = sum over k≤x of p<sub>X</sub>(k) = integral from -∞ to x of f<sub>X</sub>(t)dt.  Where the first formula using a sum is for discrete random variables and the second with an integral is for continuous random variables.  For the continuous case, the derivative of the CDF eqauls the PDF, i.e. d/dx F<sub>X</sub>(x) = f<sub>X</sub>(x).  Consider the uniform continuous PDF, the CDF for this will be 0 for x&lta, then increase linearly from 0 to 1 between a and b, then be 1 for x>b.  For a discrete random variable, the CDF will be a staircase function that increases with each value of the PMF that has a nonzero probability by the probability of getting that value, and again top out at 1.  In general, the CDF is always increasing, and goes from a lowest value of 0 to a highest value of 1. The CDF is handy for calculating ranges of a discrete random variable, e.g. if you want to know P(3&lt=X&lt=6) = CDF(X=6) - CDF(X=2).  Or if you want to know P(X>6) = 1 - CDF(X=6).  This applies to continuous random variables also, where the CDF is helpful for getting the area under the curve.  You can get 1)the probability that the random variable is less than a certain amount by getting the value of the CDF at that point, 2)the probability of a range of the random variable by subtracting two values of the CDF at the endpoints of the range, or 3) the probability that the random variable is greater than a certain amount by subtracting the value of the CDF at that point from 1.</li>
            <ul>
                <li>Python's scipy.stats library has a method to calculate the cdf for a given random variable value of a binomial distribution or a poisson distribution. For binomial, it is binom.cdf(value to calculate, number of trials, probability of success on each trial).  For poisson, it is poisson.cdf(value to calculate, expected value).</li>
                <li>The scipy.stats library also has method to calculate the cdf for a normal distribution.  norm.cdf(value to calculate, mean of distribution, standard deviation)</li>
            </ul>
        </ul>
        <li><strong>Expected Value</strong> Represented as E[X], the expected value of a discrete random variable X is E[X] = Sum over x of x * p<sub>X</sub>(x).  Loosely speaking, if you interpret the probability for each value x of a random variable X given by a PMF as the frequency of getting x if you run the experiment infinite times, then the expected value would give you the average value of x you would get.  E[X] could also be interpreted as the center of gravity of the PMF of X, which can make it easy to visualize the value of E[X].  If you interpret the probability of each value of x as a weight at point x, the center of gravity is the point along the line x where it would balance perfectly.  So a uniform discrete probability from 1 to n, would have a center of mass right in the middle at n/2 which is the expected value.  For a continuous random variable the expected value is defined analagouly, E[X] = integral from -∞ to ∞ of xf<sub>X</sub>(x)dx, which is the same as finding the center of gravity of a mass with varying density given by f<sub>X</sub>(x).  As with discrete random variables, you can intuitively interpret it as the average value you expect to get after infinite experiments.</li>
        <ul>
            <li><strong>Conditional Expectation</strong> This is the expected value of a conditional PMF, denoted E[X|A] = Sum over x of x * P(X=x|A) = Sum over x of x * p<sub>X|A</sub>(x).  Or for continuous random variables, say of a joint PDF of X and Y, you would have E[Y|X] = integral from -∞ to ∞ of y*f<sub>Y|X</sub>(y|x)dy.  It is no different than a regular expected value, you are simply using the conditional PMF for the random variable X.  Following the example above for conditional PMF, you may have a random variable X with a PMF of P(X=k)=1/4 for k in {1, 2, 3, 4} otherwise P is 0, so E[X]=2.5.  If event A is X>=2, then P(X=k|A)=1/3 for k in {2, 3, 4} otherwise P is 0, so E[X|A]=3.</li>
            <li><strong>Functions of a Random Variable</strong> You may have a random variable Y that is a function of another random variable X, say Y=g(X).  E.g. X maps the sample space to x, and Y maps x to y.  Thus, Y will also be a function of the sample space and is a random variable.  The expected value of Y is E[Y] = sum over y of y*P(Y=y).  This requires you to know the PMF of Y so you can plug in the probabilities.   You may not know or want to figure out the PMF of Y and instead know and want to rely on the PMF of X.  It can be derived that E[Y] = sum over x of g(x)*P(X=x), which is called the law of the unconcious statistician.  To prove this intuitively, many values of x may map to the same value of y, and this formula simply adds up the probability of each x times the y it gets mapped to.  In the end the sum of all the probabilities of x that map to a given y is the same as the probability of y given by the PMF of Y, so it is equivalent to the first way of calculating E[Y].  For the continuous case, it is analagous, with E[g(x)] = integral from -∞ to ∞ of g(x) * f<sub>X</sub>(x)dx.  In general E[g(x)] does not equal g(E[X]), although very importantly it is equal if g is a linear function of the random variable X, as shown below.</li>
            <li><strong>Linear Functions of a Random Variable</strong> If you consider a constant to be a random variable where every outcome of the sample space is always mapped to the same constant number, then the PMF for that random variable would be 0 everywhere except at the constant where it would be 1.  The expected value of this random variable would be the constant.  So, E[a] = a if a is a constant.  (E[X] = sum over x of x*P(X=x), P(X=x)=0 everywhere except at a where P(X=a)=1, so the sum has only one nonzero term which is a*P(X=a) = a*1 = a).  Now, consider E[aX], where you can consider a new random variable Y is a function of X, so Y=g(X)=aX.  Well E[Y] = sum over x of g(x)*P(X=x) = sum over x of a*x*P(X=x) = a * sum over x of x*P(X=x) = a * E[X].  So we have E[aX] = aE[X] where a is a constant.  e.g. X could be people's heights in inches, and Y is people's height in cm, so a = 2.54.  Y simply scales X by 2.54, and the expected value of X is simply scaled by 2.54 as well.  Finally, E[aX + b] with a and b constants, where Y = g(X) = aX+b, then E[Y] = sum over x of g(x)*P(X=x) = sum over x of (ax+b)*P(X=x) = sum over x of ax*P(X=x) + sum over x of b*P(X=x) = a * sum over x of x *P(X=x) + b*sum over x of P(X=x) = aE[X] + b. So E[aX+b] = aE[X] + b.</li>
            <li><strong>Total Expectation Theorem</strong> Consider a sample space divided into n disjoint events A1, A2, ... An.  Consider a random variable of that sample space, X.  You can ask what is P(X=x) for the PMF of X, or you can ask P(X=x|An) for the conditional PMF of X given the event An occurs.  Well for any given x, P(X=x) should equal P(A1)*P(X=x|A1) + P(A2)*P(X=x|A2) + ... + P(An)*P(X=x|An).  Thus, multiplying both sides by x and taking the sum over all x, gives sum over x of x*P(X=x) = sum over x of (x*P(A1)*P(X=x|A1) + x*P(A2)*P(X=x|A2) + ... + x*P(An)*P(X=x|An)), which means E[X] = P(A1)E[X|A1] + P(A2)E[X|A2] + .... + P(An)E[X|An].</li>
            <li><strong>Law of Iterated Expectations</strong> Given two random variables X and Y, you might want E[X|Y].  For example, consider breaking a stick at a point y, and then breaking the remaining stick at a point x.  The point y is chosen with uniform probability across the length of the stick, and then the point x is chosen with uniform probability across the remaining length y.  So E[Y]=l/2 where l is the length of the stick (since Y's probability  is uniformly distributed, l/2 is the center of mass), and E[X|Y=y] = y/2 (this is the center of mass for the uniform probability between 0 and y).  Once you know what y is, then you know the conditional probability distribution of X, and thus you can calculate an expected value for X.  But if you don't know what y is yet, then E[X|Y] is itself a random variable, and is a function g(Y) of Y equal to g(Y)=Y/2, (consider the event Y=y getting mapped to the number E[X|Y=y], for every value of Y).  We can ask what is the expected value of g(Y) = E[X|Y]? (Consider a tree diagram that first branches into possible values of Y, and then for each Y, has the possible values of X, each Y branch will have an expected value of X, E[X|Y=y], and we want over all values of Y, what is the expected value of those expected values?).  So, per the definition of expected value in the discrete case, E[E[X|Y]] = Sum over all y of E[X|Y=y]*p<sub>Y</sub>(y).  By the total expectation theorem, we know that this formula is equal to E[X], you are breaking up the sample space into disjoint events where Y=y, then summing up the conditional probability of X within each event times the probability of that event.  So, the cool result is that E[E[X|Y]] = E[X] and this is just another way of stating the total expectation theorem, but is called the law of iterated expectations.  For the stick example, it allows us to calculate that E[X]=E[E[X|Y]]=E[Y/2]=(1/2)E[Y]=1/2*l/2=l/4.</li>
            <li><strong>Joint PMFs</strong> If you have a function of random variables that involves two or more random variables, g(X, Y), and want to know its expected value.  g(X,Y) is itself a random variable, say Z, so as usual, E[Z] = sum over z of z*p<sub>Z</sub>(z).  As with other functions of a random variable, you can show that E[Z] = E[g(X,Y)] = sum over x, sum over y of g(x,y)*p<sub>X,Y</sub>(x, y).  Essentially you are taking the joint PMF of X and Y and for each x and y, multiplying z by that joint PMF, so when multiple x,y pairs gets mapped to the same z value, the joint PMF probabilities will add up to the total probability for that value of z.  So, you don't need to know explicitly the PMF of Z to get E[Z].  This holds in the continuous case as well, except E[g(X,Y)] = double integral from -∞ to ∞ of g(x, y)*f<sub>X,Y</sub>(x, y)dxdy, where f is the join PDF of X and Y.  In general, E[g(X,Y)] does not equal g(E[X,Y]).  However, there are important exceptions.  First, for linear functions of multiple random variables you have E[X+Y] = E[X] + E[Y], e.g. for E[X+Y]=sum over x of (sum over y of (x+y)*P(X=x, Y=y)) = sum over x of (sum over y of x * P(X=x, Y=y)) + sum over x of (sum over y of y * P(X=x, Y=y)) = sum over x of (x * sum over y of P(X=x, Y=y)) + sum over y of (sum over x of y *P(X=x, Y=y)) = sum over x of (x * P(X=x)) + sum over y of (y * sum over x of P(X=x, Y=y)) = E[X] + sum over y of (y*P(Y=y)) = E[X]+E[Y].  Second, if X and Y are independent, then you also have E[XY] = E[X]E[Y] because E[XY] = sum over x of (sum over y of x*y*p(X=x, Y=y)) = sum over x of (sum over y of x*y*p(X=x)*p(Y=y)) = sum over x of x*P(X=x) * sum over y of y*P(Y=y) = sum over x of x*P(X=x) * E[Y] = E[Y] * sum over x of x*P(X=x) = E[Y]*E[X] = E[X]E[Y].  We can also show that E[g(X)h(Y)]=E[g(X)]E[h(Y)] if X and Y are independent.  This is a result of if X and Y are independent that g(X) and h(Y) are independent, for any functions g and h of X and Y respectively. </li>
        </ul>
        <li><strong>Variance</strong> Variance denoted var(X) = E[(X-E[X])^2] is the expected value of a particular function of a random variable - the deviation squared.  It is used to measure how spread out a random variable is, and you might first think to define a random variable called the deviation which is X-E[X] and find its expected value, E[X-E[X]] but this will be 0 because the differences are signed and cancel each other out.  Next you might think to take the absolute value of that difference but it is more useful to take the square of that difference which is what variance is.  Consider a random variable X from a sample space to values x.  You can calculate the expected value E[X], which equals a constant. Now consider a function of X, Y = g(x) = (X - E[X])^2.  This subtracts a constant from the random variable X and then squares it, and Y is a legitimate function of the sample space and a random variable itself.  Y represents the distance squared between each value of X and the expected value of X.  Being squared, it gives greater weight to greater distances.  The expected value of Y is the variance, so by definition var(X) = E[Y] = sum over x of g(x)*P(X=x) = sum over x of (x - E[X])^2 * P(X=x).  This can be shown to equal sum over x of (x^2 - 2xE[X] + E[X]^2)*P(X=x) = sum over x of x^2*P(X=x) - sum over x of 2xE[X]*P(X=x) + sum over x of E[X]^2*P(X=x) = E[g(x)=x^2] - E[g(x)=2xE[X]] + E[g(x) = E[X]^2] = E[X^2] - 2E[X]E[x] + E[E[X]^2] = E[X^2] - 2E[X]^2 + E[X]^2 = E[X^2] - (E[X])^2. This result is somewhat simpler way to calculate it and is also true for continuous random variables.  For the continuous case, the variance is defined analagously, as var(X) = σ<sub>X</sub>^2 = E[(x-E[X])^2] = integral from -∞ to ∞ of (x-E[X])^2 * f<sub>X</sub>(x)dx</li>
        <ul>
            <li><strong>Linear Functions of a Random Variable</strong> The Variance is always positive since it is a sum of squared values (non negative values).  Adding a constant to a random variable doesn't change it's variance, adding a constant simply translates the random variable and doesn't change the spread of the random variable.  If you multiply a random variable by a constant, then the variance is multiplied by the square of that constant, which can be seen by var(aX) = sum over x of (ax - E[aX])^2 * P(X=x) = sum over x of (a^2*x^2 - 2axE[aX] + E[aX]^2) * P(X =x) = a^2 * sum over x of x^2 * P(X=x) - 2aE[aX] * sum over x of x * P(X=x) + E[aX]^2 * sum over x of P(X=x) = a^2 * E[X^2] - 2a^2E[X]E[X] + a^2 * E[X]^2 = a^2*E[X^2]-2a^2*E[X]^2+a^2*E[X]^2 = a^2*E[X^2] - a^2*E[X]^2 = a^2 * (E[X^2] - E[X]^2) = a^2 * var(X).  Altogether these properties can be shortened to var(aX + b) = a^2 * var(X) where X is a random variable and a and b are constants.</li>
            <li><strong>Standard Deviation</strong> The standard deviation is the square root of the variance of a random variable X, often denoted as σ<sub>X</sub> = sqrt(var(X)).  This is often more convenient than the variance because the variance will be in different units than the value of your random variable, namely it will be in those units squared.  Standard deviation will be in the same units as your random variable.</li>
            <li><strong>Joint PMFs</strong>If X and Y are independent, then var(X+Y) = var(X) + var(Y), because var(X+Y) = E[(X+Y)^2]-E[X+Y]^2 = E[X^2 + 2XY + Y^2]-(E[X]+E[Y])^2 = E[X^2]+2E[XY]+E[Y^2]-E[X]^2-2E[X]E[Y]-E[Y]^2 = E[X^2]+E[Y^2]-E[X]^2-E[Y]^2 = E[X^2]-E[X]^2 + E[Y^2]-E[Y]^2 = var(X) + var(Y). </li>
            <li><strong>Sums of Random Variables</strong> If you have a random variable that is the sum of random variables Y = Sum over i of X<sub>i</sub>.  For simplicity assume the expected value of each X<sub>i</sub> is 0.  Then, you get var(Y) = E[(Y-E[Y])^2] = E[(X<sub>1</sub>+....+X<sub>n</sub>-E[X<sub>1</sub>+....+X<sub>n</sub>])^2]=E[(X<sub>1</sub>+....+X<sub>n</sub>-E[X<sub>1</sub>]+....+E[X<sub>n</sub>])^2]=E[(X<sub>1</sub>+....+X<sub>n</sub>)^2]=E[Sum over i of X<sub>i</sub>^2 + Sum over i, j where i&lt>j of X<sub>i</sub>X<sub>j</sub>]=E[Sum over i of X<sub>i</sub>^2] + E[Sum over i, j where i&lt>j of X<sub>i</sub>X<sub>j</sub>]=Sum over i of var(X<sub>i</sub>) + Sum over i, j where i&lt>j of cov(X<sub>i</sub>, X<sub>j</sub>). </li>
        </ul>
    </ul>

    <h2>Derived Distributions for Functions of Random Variables</h2>
    <ul>
        <li><strong>Derived Distribution</strong> This is the PDF or PMF of a function of one or more random variables, e.g. g(X,Y).</li>
        <ul>
            <li><strong>Expected Value</strong> You don't need to find the derived distribution if all you need to do is find the expected value of the function.  This is per the law of the unconcious statistician.</li>
        </ul>
        <li><strong>Discrete Derived Distribution</strong> Take a discrete random variable X, and a function of it Y=g(X).  The PMF for a given y is the sum of the PMF for X for each x that maps to that y.  That is, p<sub>Y</sub>(y) = P(g(X) = y) = sum over x where g(x) = y of p<sub>X</sub>(x).</li>
        <li><strong>Continuous Derived Distribution</strong> For a continuous random variable X and function Y = g(X), you can think of mapping probabilities from one or more x's to a y, because the probability of any given point is 0.  You have to think the probability over a small interval of Y should be the same as the probability over the interval of X that maps to it.  It is generally easiest to find the CDF of Y F<sub>Y</sub>(y) = P(Y≤y), and then taking its derivative to get the PDF f<sub>Y</sub>(y) = d/dy F<sub>Y</sub>(y)</li>
        <ul>
            <li>For example, consider X with a uniform distribution between [0, 2], so f<sub>X</sub>(x) = 1/2 between 0 and 2.  Thus F<sub>X</sub>(x) = 1/2 x between 0 and 2, and is 0 before this interval and 1 after this interval.  Consider now Y = X^3.  First we find the CDF of Y, which is P(Y≤y).  If Y≤y then this means X^3≤y, so P(Y≤y) = P(X^3≤y).  If X^3 is less than some number a, then X must be less than a^(1/3).  So, P(X^3≤y)  = P(X ≤ y^(1/3)).  The probability that X is less than some number is 1/2 x for x between 0 and 2, 0 before this and 1 after this, so P(X ≤ y^(1/3)) = 1/2 y^(1/3), where y^(1/3) is between 0 and 2, 0 before this and 1 after, so y is between 0 and 8, 0 before, and 1 after.  So, F<sub>Y</sub>(y) = 1/2 y^(1/3).  The derivative is 1 / (6* Y ^(2/3)) between y=0 and y=8, which is the PDF.</li>
            <li>Another example, consider V is the constant speed of a car between 30 and 60mph with uniform probability.  You want to know the PDF of the time it takes to travel 200miles, T(V)=200/V.  The PDF of V is 1/30 for v between 30 and 60 and 0 elsewhere.  The CDF of V is P(V≤v) = v/30 - 1 for v between 30 and 60, and is 0 before that interval and 1 after.  Well, the CDF of T(V) is P(T≤t) = P(200/V≤t) = P(V ≥ 200/t).  The probability that V is greater than something is 1 minus the CDF, or 1-v/30+1 = -v/30+2 again for v between 30 and 60, with 1 before that interval and 0 after.  So P(V ≥ 200/t) = -200/30t + 2, for 200/t greater than 30 and less than 60 with 1 for 200/t less than 30 and 0 for 200/t greater than 60, or for t less than 200/30 and greater than 200/60 with 1 for t greater than 200/30 and 0 for t less than 200/60.  Taking the derivative, you get the PDF of T, which is 200/(30t^2) for t between 200/60 and 200/30, and 0 elsewhere.</li>
        </ul>
        <li><strong>Linear Derived Distribution</strong> For a Derived distribution of the form Y = aX + b, the b term simply shifts the distribution right if b positive or left if b is negative.  The a term stretches the X distribution horizontally, essentially the PDF for a given interval of X is mapped to an interval of Y streched out by a.  (e.g. for a=2, the PDF from X = 2 to 4 gets mapped to the PDF of Y from 4 to 8, thus stretching out the PDF.)  With stretching, this increases the area under the curve so the PDF of Y has to be multiplied by 1/a to keep the area at 1.  Formally, if Y=aX+b, then F<sub>Y</sub>(y) = P(Y≤y) = P(aX+b≤y).  If a is positive, this equals P(X≤(y-b)/a)=F<sub>X</sub>((y-b)/a), and taking the derivative using the chain rule gives you 1/a * f<sub>X</sub>((y-b)/a).  If a is negative, then this equals P(X≥(y-b)/a)=1-F<sub>X</sub>((y-b)/a), and taking the derivative you get -1/a * f<sub>X</sub>((y-b)/a).  So, if Y=aX+b, then f<sub>Y</sub>(y) = 1/abs(a) * f<sub>X</sub>((y-b)/a).  This corresponds to shifting f<sub>X</sub> to the right by b if b is positive or the left by b if b is negative, and then stretching f<sub>X</sub> horizontally by a if a is >1 or shrinking it horizontaly if a&lt1, while also streching/shrinking vertically by 1/a.  This can be used to show that a linear combination of a normal distribution is still a normal distribution, given N(μ, σ<sup>2</sup>): f<sub>X</sub>(x) = 1/(σ * sqrt(2pi)) * e^(-(x-μ)^2 / 2σ^2), you get for Y=aX+b, f<sub>Y</sub>(y) = 1/abs(a) * f<sub>X</sub>((y-b)/a) = 1/(abs(a) * σ * sqrt(2pi)) * e^(-((y-b)/a-μ)^2 / 2σ^2) =  1/(abs(a) * σ * sqrt(2pi)) * e^(-(y-(aμ+b))^2 / 2(aσ)^2) = N(aμ+b, (aσ)^2)</li>
        <li><strong>Derived Distribution of multiple random variables</strong> For a random variable of the form Z = g(X,Y) = Y/X, the method to finding the PDF is generally the same, where you find the CDF first and then take its derivative.  Consider X, and Y are independent random variables both with uniform probability between 0 and 1, and 0 probability elsewhere.  So they have a joint PDF equal to 1 in the unit square and 0 elsewhere.  We first want P(Z≤z)=P(Y/X≤z)=P(Y≤zX).  First if z is negative, then P(Y≤zX) = 0, because X is always positive so this would require Y to be negative, and Y is always postive.  So given z is positive, then if z≤1, then P(Y≤zX) is the volume of the joint PDF of X,Y over the triangular area under Y=zX, between X=0 and X=1, so P(Y≤zX)=1 * (1/2 * 1 * z) = z/2 for z≤1.  If z>1, then P(Y≤zX) is the volume of the joint PDF of X,Y over the unit square, less the triangular area above Y=zX between y=0 and y=1.  So, P(Y≤zX) = 1 * (1 - (1/2 * 1 * 1/z)) = 1 - 1/(2z) for z>1.  To get the PDF of Y/X, we now take the derivative, so for z&lt0, it is 0, for z≤1 you get 1/2, and for z>1 you get 1/(2z^2).</li>
        <li><strong>Monotonic Function of a Random Variable</strong> If X is a random variable and Y=g(X) is a function of it that it is strictly monotonic, i.e. always increasing or always decreasing, but not necessarily the same slope throughout.  In this case, consider a small interval of X x≤X≤x+δ, this is a subset of X, and it should have the same probability as g(x)≤Y≤g(x+δ).   Now, we know the length of the interval in X is δ, but how big is the interval in Y?  Well, the slope of g(X) at x gives you the change in y over change in x at x.  Since g(X) is monotonic, you can approximate for small δ, that the change in x is delta, so the change in y is slope of g(X) at x times δ.  Or the size of the interval in Y is dg(x)/dx*δ, actually it is the absolute value of that quantity to give a length.  We know the probability of X at the point x for small δ is δ*f<sub>X</sub>(x) and the probability of Y at the point y for a small interval is the same except with y's.  Well, the small interval δ in X corresponds to the small interval in Y of dg(x)/dx*δ, so δ*f<sub>X</sub>(x) = abs(dg(x)/dx*δ)*f<sub>Y</sub>(y), thus f<sub>X</sub>(x) = abs(dg(x)/dx)*f<sub>Y</sub>(y), where y = g(x).   Now you want to eliminate x and solve for f<sub>Y</sub>(y).  For example consider Y=X^3.  Then f<sub>X</sub>(x) = abs(dg(x)/dx)*f<sub>Y</sub>(y), so f<sub>X</sub>(x) = 3x^2*f<sub>Y</sub>(y), so f<sub>Y</sub>(y) = f<sub>X</sub>(x)/(3x^2).  Well since we know y=x^3, we know x=y^(1/3), so f<sub>Y</sub>(y) = f<sub>X</sub>(y^(1/3))/(3y^(2/3)). This you could solve given the PDF of X.  Note in general, when the slope of g(x) is shallow, the PDF of Y is high because that means the probability for many x's are getting mapped to a small number of y's.  Likewise if the slope of g(x) is steep, then the PDF of Y is low because just a small number of x's are getting mapped to a large number of y's, thus spreading out the probability over many y's and making the probability distributed more widely and thus smaller. </li>
        <li><strong>The sum of two random variables</strong> Consider two independent random variables X and Y, and their sum W = X+Y.  The, P(W=w) = P(X+Y=w) = Sum over x of P(X=x)*P(Y=w-x).  This is true since X and Y are independent, and it picks out all the points where the sum of x and y is w and adds up their probability for any given w.  For the discrete case, this is p<sub>W</sub>(w) = Sum over x of p<sub>X</sub>(x)*p<sub>Y</sub>(w-x).  This is called the convolution formula. One way to get the pmf of W is to take a graph of the pmf of X and a graph of the pmf of Y, and then flip the graph of Y across the vertical axis.  Then shift the graph of y to the right w units (for positive w).  This aligns the values of Y and X that add up to W.  Then you multiply the PMFs together and add this up across all X's to get the PMF of W at w.  For the continuous case, it is analagous, you have f<sub>W</sub>(w) = integral over all x of f<sub>X</sub>(x)*f<sub>Y</sub>(w-x) dx.</li>
    </ul>

    <h2>Covariance and Correlations</h2>
    <ul>
        <li><strong>Covariance</strong> Consider two discrete random variables X and Y, which are not independent.  If you plotted the joint pmf p<sub>X,Y</sub>(x,y), by simply putting a dot in the X-Y plane everywhere that (x,y) has a nonzero probability, you would see a pattern such as the points tend to fall along a line with positive slope so that bigger values of x seem to get paired with bigger values of y, or similarly fall along a line with negative slope where bigger values of x seem to get paired with smaller (more negative) values of y.  Thus the probability of x very much depends on what y you look at, and the probability of y very much depends on what x you look at.  You can calculate E[X] and E[Y] based on the marginal PMFs of X and Y.  Now you can ask for any given (x,y), what is x-E[X], and what is y-E[y]?  If you multiply these together you can define a random variable for all (x,y) of (x-E[X])(y-E[Y]), and this will be positive if x and y are both above or both below their respective means, and negative if x is above its mean and y is below its mean or vice-versa.  If you take the expected value of this, you get the covariance, cov(X,Y) = E[(X-E[X])(Y-E[Y])], this will give you a positive number if (x,y) pairs tend to fall on a line with positive slope, and a negative number if (x,y) pairs tend to fall on a line with negative slope. For the former, it means as x increases, so does y.  For the latter, it means as x increases, y decreases.</li>
        <ul>
            <li><strong>Variance</strong> Notice that the covariance of a random variable with itself is E[(x-E[X])(x-E[X])] = E[(x-E[X])^2], which is equal to var(X).  If you plot X along the horizontal axis and X along the vertical axis, then all the possible pairs (x,x) fall along the line where X=X, i.e. a line with slope 1 passing through the origin.  So thinking in terms of covariance the deviations of both X's are always the exact same sign, so you know you will get a positive covariance as you should.  If the distribution of x is spread out so that high values of x have decent probability, then the covariance will be larger, but if higher values of x have lower probability and there isn't as much spread, then you know the covariance will be a smaller value overall.  All of this behavior is exactly how variance should behave.</li>
            <li><strong>Shortcut Calculation</strong> If you do the algebra cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY - E[Y]X - E[X]Y + E[X]E[Y]] = E[XY]-E[E[Y]X]-E[E[X]Y]+E[E[X]E[Y]] = E[XY]-E[Y]E[X]-E[X]E[Y] + E[X]E[Y]E[1] = E[XY] = E[XY]-2E[X]E[Y]+E[X]E[Y] = E[XY]-E[X]E[Y]. This form is often easier to work with.</li>
            <li><strong>Linear Function of a random variable</strong> We have cov(aX+b, Y) = E[(aX+b-E[aX+b])(Y-E[Y])] = E[(aX+b-E[aX]-E[b])(Y-E[Y])] = E[(aX+b-aE[X]-b)(Y-E[Y])] = E[a(X-E[X])(Y-E[Y])] = a*cov(X,Y).  This is saying that if you shift X by b, then it doesn't change the covariance, because the mean of X gets shifted along with the values of X, so the distance between each value of X and the mean of X doesn't change, so the covariance is unaffected.  However, the multiplier a actually stretches the distribution of X if greater than 1, or squishes it if less than 1, so now the distances between X and the mean of X are a times what they used to be, and the covariance is a times what it used to be.</li>
            <li><strong>Independent Random Variables</strong> If X and Y are independent, then cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[X-E[X]]E[Y-E[Y]] = 0 * 0 = 0.  Consider for independent random variables, if you fix Y at y, then that slice of probability at Y=y, equals the marginal probability of X, scaled by the probability of Y at y.  So, you would expect that just like E[X-E[X]] would give you 0 for the marginal probability of X, so will E[X-E[X]] for that slice along Y=y.  Along that slice at Y=y, E[Y-E[Y]] is constant, equal to whatever E[Y-E[Y]] for the marginal probability of Y at Y=y.  It doesn't much matter, because along every slice of Y, E[X-E[X]] will be 0, so the covariance will evaluate to 0.  E[X-E[X]] is nonzero at the slice Y=y when the distribution of X at Y=y is different than the marginal distribution of X, so that the X-E[X] terms don't zero each other out at Y=y, and of course this means X and Y must be dependent. Covariance only becomes nonzero if there is a pattern where over many slices of Y, E[X-E[X]] tends to be nonzero and the same sign as E[Y-E[Y]], or nonzero and the opposite sign as E[Y-E[Y]].  Over the many slices of Y, if the signs are sometimes the same and sometimes different, then they will cancel each other out and the covariance will be close to 0.  The preceding discussion sort of assumes a uniform distribution of X and Y, so keep in mind the deviances (X-E[X] and Y-E[Y]) are getting weighted by probabilities as you get their expected values and covariance, but fundamentally the concept is the same, it just could be affected e.g. if some things look correlated but have very low probability then they won't count much to the covariance, or if x,y pairs that are very uncorrelated have high probability they will count more to the covariance being close to 0.  NOTE it is not true that if X and Y have 0 covariance then they must be independent.  You could have dependent X and Y that e.g. both take on values between -1 and 1, and both have expected values of 0, so cov(X,Y)=E[XY].  You could have that if one value is nonzero, then the other must be 0, so the clearly effect each other (dependent), but cov(X,Y)=E[XY]=0.  i.e. they can be dependent without forming a covariant relationship.</li>
        </ul>
        <li><strong>Correlation Coefficient</strong> The correlation coefficient, rho, is ρ = E[((X-E[X])/σ<sub>X</sub>)((Y-E[Y])/σ<sub>Y</sub>)] = cov(X,Y)/(σ<sub>X</sub>σ<sub>Y</sub>).  This standardizes the deviations of X and Y, and compares the standardized deviations.  Whereas the covariance has units of X*Y, the correlation coefficient is unitless. Note it is not defined if one of the random variables has 0 standard deviation (it would be a constant random variable) since this would require division by 0.  It turns out -1≤ρ≤1, giving us an absolute scale telling us how associated X and Y are.  If ρ=0 then we say X and Y are uncorrelated.</li>
        <ul>
            <li><strong>Independent X, Y</strong> Similar for covariance independent X,Y have ρ=0.  This is easily seen by the definition of ρ, but the same logic applies where, for every slice of probability at Y=y, the standardized deviations of X should all cancel each other out and have expected value 0, just as they do for the marginal distribution of X.  So, over all slices of Y, you get 0 and thus ρ=0, meaning that X doesn't vary from the mean in a way that is dependent on Y.  (As it shouldn't since they are supposed to be independent)</li>
            <li><strong>X and Y linearly related</strong> For Y=X, ρ=1 and for Y=-X ρ=-1.  For Y=X, the numerator of the correlation coefficient will be var(X) and so will the denominator.  For Y=-X, the cov(X,-X)=E[(X-E[X])(-X-E[-X])]=-E[(X-E[X])(X-E[X])]=-var(X), so the correlation coefficient will be -1.  In general if X-E[X]=c(Y-E[Y]), then ρ=1 if c is positive, and ρ=-1 if c is negative.  This can be seen by substituting in the equation for ρ, so that it is all in terms of X and c, and the c can be pulled outside of expectations, and will result in abs(c)/c * ρ(X,X). It can also be shown that if ρ=1 or ρ=-1, then it must be the case that X and Y are linearly related, and X-E[X] = c(Y-E[Y])</li>
            <li><strong>Linear functions of random variables</strong> Since the standard deviation of aX = sqrt(E[aX-E[aX])^2]) = sqrt(E[(a(X-E[X]))^2]) = sqrt(a^2 * E[(X-E[X])^2]) = abs(a) * σ<sub>X</sub>. We have ρ(aX+b, Y) = a*cov(X,Y) / abs(a) σ<sub>X</sub>σ<sub>Y</sub> = sign of a * ρ(X,Y).  This means, in the example that Y=X, then all linear functions of X, will have ρ=1 or -1, depending on the sign of a.  Consider that a linear function of X will have the same probability distribution as X, except the mean will be shifted by b, and then the distribution will be scaled by a, getting streched out if a>1, and squeezed in if a&lt1.  Since ρ looks at the standardized deviations from the mean, the shifting of the mean by b makes no difference since your are looking only at distances from the mean, and the scaling of the distribution by a also makes no difference since the distances get standardized, and the standard will strech or shrink according to a, just as the distances do.  Thus all that matters is the sign of a.  This also means if X and Y are different, and we replace X with a linear function of X (e.g. changing units of X from degrees F to C), then this doesn't change the correlation coefficient with Y (perhaps day of year), unless a is negative.</li>

        </ul>
    </ul>

    <h2>Inferences</h2>
    <ul>
        <li><strong>Inference</strong> An inference problem is finding the probability distribution of an unknown random variable given the value of a known random variable.  Making a measurement of something is an inference problem.  The experiment is that something measurable occurs in the real world and you take a measurement of it using some measuring device, which has some error or noise associated with it.  You define a random variable X that represents the actual state of the world, and a random variable Y that represents your measurement result.  You don't know the value of X but you know or have some beliefs about the probability distribution of X, either a discrete PMF p<sub>X</sub>(x) or a continuous PDF f<sub>X</sub>(x) (called the prior distribution).  You do know the value of Y, and you know the conditional probability distribution of Y given X, which amounts to a model of your measuring device, either a discrete conditional PMF p<sub>Y|X</sub>(y|x) or a continuous PDF f<sub>Y|X</sub>(y|x).  The goal of the inference problem is to figure out the conditional probability of X given the value of Y given by your measurement device, called the posterior distribution and is either a discrete conditional PMF p<sub>X|Y</sub>(x|y) or a continuous PDF f<sub>X|Y</sub>(x|y).  To do this, you use some variation of Bayes' Rule, depending on whether the random variables are discrete or continuous.</li>
        <ul>
            <li><strong>X and Y Discrete</strong> To solve the inference problem for the discrete case, we know p<sub>X,Y</sub>(x,y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) = p<sub>Y</sub>(y) * p<sub>X|Y</sub>(x|y).  Think about this in terms of event subsets of the sample space to convince yourself its true.  Thus we can derive that  p<sub>X|Y</sub>(x|y) = p<sub>X,Y</sub>(x,y) / p<sub>Y</sub>(y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) / p<sub>Y</sub>(y).  This could be used in the example of a plane being overhead or not and a radar device that measures if a plane is overhead.  You could say there is a 10% chance of there actually being a plane overhead P(X), and the radar device is wrong 10% of the time P(Y|X), and use bayes rule to find the probability there is a plane overhead given a positive measurement P(X|Y) is (.1 * .9)/(.1*.9 + .9*.1) = 0.5.  To get the denominator, it is useful to know that p<sub>Y</sub>(y) = sum over all x of p<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x).</li>
            <li><strong>X and Y Continuous</strong> This is analagous to the discrete case, where we just replace PMFs with PDFs and sums with integrals.  So we have f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x) / f<sub>Y</sub>(y).  And we may not be given the denominator but we can get it by f<sub>Y</sub>(y) = integral over all x of f<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x). As an example, you might have a current through a resistor represented by X, and a measurement of that current using a device that has some gaussian noise represented by Y.  You get a measurement y, and you want a probability distribution for what X is, the actual current through the resistor.</li>
            <li><strong>X Discrete, Y Continuous</strong> For example, X may be a bit that is sent, either a 0 or 1.  Y is your measurement of that bit and may equal X plus some random gaussian noise W, Y = X+W.  You have a belief about the PMF of X, say 0 or 1 is equally likely, and you have a model for your measurement device, i.e. the conditional PDF of Y given X.  For instance the conditional PDF of Y given X might be the normal distribution centered around X.  To get the conditional PMF of X given Y (the inference), we note that P(X=x, y≤Y≤y+δ) = P(X=x)P(y≤Y≤y+δ|X=x) = P(y≤Y≤y+δ)P(X=x|y≤Y≤y+δ).  (Here we're just thinking in terms of events in the sample space, specifically the event i.e. all the outcomes where X=x, and the event i.e. all the outcomes where y≤Y≤y+δ).  The probability that y≤Y≤y+δ is given by f<sub>Y</sub>(y)*δ, in the limit that δ approaches 0, and we can condition this on X being some value. From this we can get p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x)*δ = f<sub>Y</sub>(y)*δ * p<sub>X|Y</sub>(x,y).  So p<sub>X|Y</sub>(x,y) = (p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x)) / f<sub>Y</sub>(y).  Here again you may not know the denominator, but it can be found with f<sub>Y</sub>(y) = Sum over all x of p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x). The intuition is the same as the previous cases, just keep in mind you are dealing with probability densities with f, where you get a measurement Y=y, then for every value of X, you need to know the value of the PDF at y for that x.  If you add all that up, you get the denominator, and your numerator can be the PDF value of your y for a particular x of interest.  Its weird because the PDF isn't giving you actual probabilities, it's giving you densities, but all you need is the ratio of the densities to get the probabilities of x that you need (the density units cancel out in the ratio).</li>
            <li><strong>X Continuous, Y Discrete</strong> For example, X may be the magnitude of the current (continuous) through a device that emits photons and Y is a count of the number of photons emitted per second (discrete).  The inference equation you need is gotten to by the same logic as the previous case, just with the roles reversed.  So, f<sub>X|Y</sub>(x,y) = (f<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x)) / p<sub>Y</sub>(y). Where p<sub>Y</sub>(y) = integral over all x of f<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x)dx. </li>
        </ul>

    </ul>

    <h2>Sampling Distributions</h2>
    <ul>
        <li>We often cannot calculate statistics for an entire population, and thus calculate a statistic from a randomly selected sample of the population, and want to know how certain we can be that we know the true statistic for the entire population.</li>
        <li><strong>Sampling Distribution</strong> If you take a statistic like the mean, median, the min or max value, variance, etc. for a sample of the population, across many samples, you will create a probability distbution for that value which is called a sampling distribution.</li>
        <li><strong>Unbiased and Biased Estimators</strong> If the mean of a sampling distribution for some statistic is equal (approximately) to the actual value of that statistic for the whole population, then the sample statistic is an unbiased estimator.  This is true for the mean of a sample/population.  If the mean of a sampling distribution for some statistic is not centered around the actual value fo that statistic for the whole population, then it is a biased estimator.  This is true for the max and min value of a sample/population.</li>
        <li><strong>Central Limit Theorem</strong> This states that the sampling distribution of the mean for a population will be normally distributed as long as the population is not too skewed, or the sample size is large enough (at least 30 is a good rule of thumb).  You can have a smaller sample size if the population is normally distributed and the central limit theorem will still hold.  CLT says 1) The sampling distribution of the mean will have a mean x that will be close to the true population mean μ. And 2) The sampling distribution of the mean will have standard deviation equal to the population standard deviation over the square root of the sample size σ / sqrt(n). The standard deviation of a sampling distribution is called the standard error of the estimate of the mean. We may not know the population's standard deviation so we often use the standard deviation of a sample in its place.</li>
        <li><strong>Confidence Intervals</strong> Usually we only have one sample, not many that we can make a sampling distribution with.  We want to say how sure we are that the statistic for that sample that we're interested in is close to the actual population statistic.  We can use the standard deviation of the sample as an estimate for the standard deviation of that population.  Then per CLT, we can calculate the standard error of our sample, using the estimate of the standard deviation divided by the square root of the sample size.  Also using CLT, we know that the sampling distribution is normally distributed and centered on the actual population mean. Since a normal distribution has 95% of its values fall with 1.96 standard deviations of the mean, we know that 95% of sample statistics will thus be less than 1.96 * standard error away from the actual mean.  Thus using our standard error estimate and the mean of our one sample, we can say that the mean of our sample is within 1.96 * standard error of the actual population mean.  This interval is called a 95% confidence interval.</li>
        <li>Python's numpy library has a convenient function for creating a random sample from a population.  It is random.choice(population array, sample size, replace), where pop array is an array to choose from, sample size is an integer, and replace should be FALSE because once an item is added to the sample it should not be able to be picked again.  It returns a list of the sample.</li>
    </ul>
</body>
</html>