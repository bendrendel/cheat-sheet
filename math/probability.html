<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability</title>
</head>
<body>
    <a href="../index.html">Home</a>
    <h1>Probability</h1>

    <h2>Set Theory</h2>
    <ul>
        <li><strong>Set</strong> Sets are a collection of elements where each element is unique, and order doesn't matter. Represented by curly braces and often denoted by a capital letter e.g. A = {Book, Folder, Pen, Paper, Hat}</li>
        <li><strong>Subset</strong> One set is a subset of another if each element of the set is contained in the other. e.g. A is a subset of B if A = {1, 3, 2} and B = {4, 2, 1, 5, 3}</li>
        <li><strong>Union</strong> The union of two sets is the set of all elements that appear in at least one of the two sets, written as (A or B)</li>
        <li><strong>Intersection</strong> The intersection of two sets is the set of all elements that appear in both of the two sets, written as (A and B)</li>
        <li><strong>Complement</strong> The complement of a set is all the elements not in the set, but in some superset, written A<sup>C</sup>. In the context of probability, your superset would be the sample space, and the set would be an event, and its complement would be all sample points not in the event.  So, the event and its complement cover the entire sample space.</li>
    </ul>

    <h2>Probability</h2>
    <ul>
        <li><strong>Experiment/Trial</strong> An Experiment is any procedure that can be repeated infinitely and has a well-defined set of possible outcomes. e.g. flipping a coin once is an experiment.  Often, an experiment is repeated many times to be subjected to statistical analysis.  In this case, there is still just a single experiment but it can be thought of as an experiment composed of other experiments which we call trials.  Mathematically an experiment consists of 1) a Sample Space Ω or S of all possible outcomes, 2) a set of events F where each event is a set containing 0 or more outcomes, and 3) A probability measure function P that maps events to probabilities.</li>
        <li><strong>Sample Point or Outcome and Sample Space</strong> A Sample Point or Outcome of an Experiment is the result of a single execution of the experiment.  The Sample Space of an Experiment Ω is the set of all possible outcomes of the experiment.  The outcomes of a sample space should be mutually exclusive (if one happens then another does not) and collectively exhaustive (no matter what happens, the outcome of the experiment is in the sample space) </li>
        <li><strong>Event</strong> An event is a set of outcomes and is a subset of the sample space.  It is often more convenient to consider events than every possible outcome.  An event can consistent of a single outcome which is called an elementary or atomic event.</li>
        <ul>
            <li><strong>Dependence/Independence</strong> Independent events means the outcome of one event does not effect the probability of the other event.  Event B is not dependent on event A means P(B|A) = P(B), because A occuring has no effect on the probability of B occuring.  Since P(B|A) = P(B and A)/P(A) (P(A) not zero) by definition of conditional probability, we have P(B and A) = P(A)P(B) when event B is not dependent on event A.  If P(B) is not zero, then P(B and A)/P(B) = P(A and B)/P(B) = P(A|B) = P(A), so A is not dependent on B.  So we say A and B are independent events and the definition of independence is taken to be P(A and B) = P(A)P(B).  If either P(A) or P(B) is 0, then they are considered independent, and in fact any event with probability 0 is independent of every other event.  More generally, n events are independent only if the probability of the intersection of every combination of those events is equal to the product of those events' probabilities occuring on their own.  This means not only every pair of events, but every combination of three events, four events, up to n events.  e.g. flipping a coin twice, the outcome of the first flip does not effect the outcome of the second flip.  Dependent events means one event does effect the probability of the other, e.g. pulling marbles out of a bag that are either blue or red, the first time you pull a marble out is going to change the probability of what you pull out after that.  For more than two events, consider flipping a coin twice and event A is the first flip is heads, event B is the second flip is heads, and event C is both flips are the same.  Events A and B are independent, Events A and C are independnet, and events B and C are independent because the first event occuring in each of those does not change the probability of the second event occuring, or said another way P(A and B)=P(A)P(B), P(A and C)=P(A)P(C), and P(B and C) = P(B)P(C).  However, A,B,C are not independent events because P(A and B and C) does not equal P(A)P(B)P(C).  This is because P(C|A and B) = 1, i.e. if you know both A and B occured, then you know for sure that C occured.  So C is not independent of (A and B).  Likewise if you know C and A occurred then you know for sure B occurred or if you know C and B occured then you know for sure A occured.</li>
            <li><strong>Mutually Exclusive</strong> Two events are mutually exclusive if their intersection is empty.  e.g. flipping a coin the event of getting heads is mutually exclusive from the even of getting tails.  But rolling a die the event of getting an even number is not mutually exclusive frmo the event of getting a number greater than three. An event and its complement are always mutually exclusive.</li>
        </ul>
        <li><strong>Probability</strong> If we run an experiment an infinite number of times, the probability of a given event is the proportion of times it occurs.  This is known as the frequentist interpretation of probability.  This is denoted mathematically as P(Event) = (Number of times event occurred)/(Number of Trials).  A probabilistic model of something is a mathematical description of an uncertain situation that consists of a sample space Ω of all possible outcomes, and a probability law that assigns (ideally) every possible event A of the sample space a probability P(A).  The axioms for a probability rule are: P(A) >= 0 for all A, P(Ω)=1, and if A and B are disjoint events then P(A and B) = P(A) + P(B) (more generally this is true for the union of a countably infinite sequence of disjoint events)</li>
        <ul>
            <li><strong>Conditional Probability</strong> This is the probabiity of an event A occuring given than another event B occurs, and is written P(A|B). By definition P(A|B) = P(A and B)/P(B), given P(B) is not zero.  If the events are independent, then P(A|B) = P(A) and P(B|A) = P(B).  However this is not true if the events are dependent, e.g. if B is first picking a blue marble out of a bag of red and blue marbles, and A is second picking a red marble, then A is dependent on B because the ratio of red to blue marbles changes after the first pick.</li>
            <li><strong>Addition Rule</strong> The probability of event A happening or event B happening (or both) is P(A or B) = P(A) + P(B) - P(A and B).  If A and B are mutually exclusive, then P(A and B) = 0, but if they are not, then you have to subtract the probability of their intersection or else you'll be double counting.</li>
            <li><strong>Multiplication Rule</strong> The probability of event A happening and event event B happening is P(A and B) = P(A) * P(B|A).  If A and B are independent, the P(B|A) = P(B), but if they are not, then you have to take into account the probability of B occuring given A occuring.  This provides a definition of conditional probability as P(B|A) = P(A and B) / P(A).  </li>
            <li><strong>Bayes' Theorem</strong> This states that P(B|A) = P(A|B)*P(B) / P(A) = P(A and B)/P(A).  This is true because the set A and B = B and A.  Thus P(A and B) = P(B and A), thus P(A) * P(B|A) = P(B) * P(A|B), and thus P(B|A) = P(B) * P(A|B) / P(A)</li>
            <li><strong>Tree Diagrams</strong> This is a way of visually diagramming multiple experiments and their sample space, and you can see how the multiplaction rule works and Bayes' Theorem.</li>
            <ul>
                <li>Here is a diagram for independent events:</li><img src="./coin-tree-diagram.svg" alt="">
                <li>And here is a diagram for dependent events.  The first experiment is whether someone has strep throat, and P(ST) is the event that they do and has a 20% probability.  The second experiment is running a test for strep throat and the events are either a positive or negative test result.  The probabilities of the events are dependent on whether the person has strep throat or not.  If you wanted to know the probability of having strep throat given you had a positive test, then you could use Bayes' theorem: where P(ST|+) = P(+|ST) * P(ST) / P(+)</li>
                <img src="./Conditional_Probability_Application.svg" alt="" style="width: 800px">
            </ul>
        </ul>
        <li><strong>Probability Problem Solving </strong>The approach to any probabilistic problem generally breaks down to 1)Defining a sample space, 2)Defining a probability law on the sample space, 3)Defining an event of interest, and 4)Calculate probability of the event.</li>
        <li><strong>Law of Large Numbers</strong> You can't perform an infinite number of trials of an experiment, but the law of large numbers is that as you perform more and more trials, the probability of any given event will converge to its true probability.</li>
        <li>For example, say the experiment is flipping a coin twice and the observation is which side of the coin lands up on each flip.
        <ul>
            <li>There are four possible sample points for this experiment depending on the side of the first flip and the side of the second flip.  The full sample space could be represented as S = {HH, HT, TH, TT}</li>
            <li>Possible events you might be interested in are getting two heads, A = {HH}, getting two tails, B = {TT}, or getting a heads and a tails, C = {HT, TH}</li>
            <li>You might run 1000 trials of the experiment, and notice that {HH} occurred 252 times. Thus you could estimate the probability of getting two heads with two coin flips is P({HH}) = 252/1000 = .252 = 25.2%</li>
        </ul>
        </li>
    </ul>

    <h2>Combinatorics</h2>
    <ul>
        <li>Combinatorics is the mathematics of counting, you often need to be able to count outcomes in an event or in a sample space.  Counting is really needed in the case of uniform probability.</li>
        <li><strong>Discrete Uniform Probability</strong> When calculating the probability of events in a sample space where every outcome has the same probability, then you simply need to count the number of outcomes in the event and divide by the number of outcomes in the sample space to get the probability.  The hard part can be counting the outcomes in the event of interest and outcomes in the sample space, when you have a large events/sample space, which is where combinatorics comes in handy.  e.g. using the basic counting principles below you can calculate the probability that 6 rolls of a die will each roll a different number as 6! / 6^6.</li>
        <li><strong>Basic Counting principles</strong> Consider multiple stages each with multiple possible choices.  e.g. 1st stage has 2 choices, 2nd stage has 4 choices, 3rd stage has 3 choices.  Then the total number of outcomes across all 3 stages is 2 * 4 * 3.  Think of a tree diagram of this, and starting from the leaves of the tree, you have 3 outcomes repeated 4 times, and then you have that repeates 2 times.  An example of using this counting principle is picking letters/digits for a license plate, say you pick 6 and there are 26 letters and 10 digits for each choice, then you get 36*36*36*36*36*36 possible outcomes.  When you are picking items from a set of items and cannot reuse the items, then e.g. for the license plate you get 36*35*34*33*32*31.  If you go through every item in the set, then each outcome is called a permutation of the n items, and permutations refer to all the possible orderings of n items, and is given by n!.  Finally, an important counting principle is going through a list of n items and choosing to pick each one or not to create a subset, for this you have n trials with 2 outcomes on each, so the number of subsets of n items is 2^n (includes possibility of picking none).</li>
        <li><strong>Combinations</strong> Represented as (n k) except n is stacked on top of k, and said as n choose k.  It is for determining the number of possible subsets of k elements from a set of n elements.  Since they are sets, the order of the k elements does not matter.  If it did matter, then there would be n!/(n-k)! possibilities (example of basic counting principles above).  But we know also from basic counting principles that each subset of k elements has k! different orderings.  So (n k) = n! / ((n-k)!k!).  Note 0! is defined to equal 1 and it makes this formula work in the case that k=n or k=0 or n=0 - (n k)=1 in all these cases. Note k must be less than or equal to n. n and k are the binomial coefficients. </li>
        <li><strong>Binomial Probabilities</strong> Consider the experiment of n coin tosses, and the event that you get k heads.  If the probability of getting heads is p, then for any given outcome, the probability for that outcome is p^k * (1-p)^(n-k).  The problem is now counting how many outcomes have k heads and multiplying that by the probability of each outcome above.  Counting how many outcomes have k heads is equivalent to counting how many ways you can pick out k items out the n tosses to be heads, i.e. the number of possible subsets of k elements out of n.  This is simply n choose k, so the probability of k heads in n coin tosses is (n k) * p^k * (1-p)^(n-k).  This applies to any binomial distribution with n independent trials and probability p of success on each trial.  Consider summing up the probability of k heads in n tosses from k=0 to k=n.  This would be equal to 1 since it covers the whole sample space. Additionally, consider the question out of 10 coin tosses, you know 3 heads occurred, and you want to know what is the probability that the first two tosses were heads, so given event A (3 heads) what is the chance of event B (first two are heads)?  Within event A the probability of all outcomes is the same since every outcome has the same number of heads and it is p^3 * (1-p)^(n-3).  So, it is uniform probability inside event A and all you need to do is count the ways B can occur and divide by the number of ways A can occur.  A can occur (n 3) * p^3 * (1-p)^(n-3) ways, and given A, B can occur 8 ways since the only uncertainty in B is where the third head goes, which given the first two are heads, leaves 8 possible spots the third head could occur.  So the answer is 8 divided by the probability of A above.</li>
        <li><strong>Partitioning</strong> A more general combination is partitioning a set into a given number of subsets of given cardinalities.  A combination can be though of as how many ways can I split a set of n elements into 2 subsets, one a subset of k elements and one a subset of n-k elements.  A partion asks how many ways can a split a set of n elements into m subsets of cardinality k<sub>1</sub>, k<sub>2</sub>, ..., k<sub>m</sub>.  This is (n k<sub>1</sub>) * (n-k <sub>1</sub> k<sub>2</sub>) * ... * (n-k<sub>1</sub>-...-k<sub>m-1</sub> k<sub>m</sub>). The last term is always equal to 1, and algebraically this works out to n!/(k<sub>1</sub>! * k<sub>2</sub>! * ... * k<sub>m</sub>!).  e.g. if we want to know how many ways there are to deal 52 cards into 4, 13-card bridge hands, then it would be (52 13)*(39 13)*(26 13)*(13 13) = 52! / (13! * 13! * 13! * 13!).   Now consider the question what is the probability of dealing one ace to each person?  Well, You have the demoninator you need of how many 13-card hands there are total.  The numerator comes from, how many ways are there to distribute 4 aces to 4 hands? There is 4! since you have 4 hands to choose from for the first ace, 3 hands for the second ace, and so on.  Now how many ways are there to distribute the remaining 48 cards to 4 hands? It comes from partioning 48 cards into 4, 12-card partitions.  So, the answer is (4! * (48 12) * (36 12) * (24 12) * (12 12)) / (52 13)*(39 13)*(26 13)*(13 13)</li>
    </ul>

    <h2>Random Variables and Probability Distributions</h2>
    <ul>
        <li><strong>Random Variable</strong> A random variable is a function from a sample space to a set of numbers, generally the reals or integers.  It is generally represented by a capital letter like X, and its values are represented by a lower case letter like x.  A single experiment that generates a sample space could have multiple random variables, e.g. say your experiment is picking a student out of a class, your sample space would consist of one outcome for each student in the class.  You could have a random variable H from each outcome (a student) to the height of that student in inches (a real number or integer) and a second random variable W from each outcome to the weight of that student.  Additionally, you can have random variables defined on other random variables, say Hbar = 2.54*H and gives the height of a student in centimeters, Hbar is still a random variable because each outcome in the sample space maps to a single value of Hbar, and thus Hbar is a function from outcomes in a sample space to numbers.  It divides the sample space up into a series of mutually exclusive events that cover the entire sample space, assigning each event a numeric value.</li>
        <ul>
            <li><strong>Discrete Random Variable</strong> A discrete random variable has a countable range often the integers.  E.g. you could flip a coin four times, creating a sample space of {HHHH, HHHT, HHTH, HHTT, HTHH, HTHT, HTTH, HTTT, THHH, THHT, THTH, THTT, TTHH, TTHT, TTTH, TTTT}.  You could create a random variable that maps this sample space onto the number of heads in each outcome, which will have a range of {0, 1, 2, 3, 4}</li>
            <li><strong>Continous Random Variable</strong> e.g. This is a function from a sample space to the continuum, any real number.  e.g. The temperature measured at a given time is a continuous random variable, or the time it takes a person to run a mile.</li>
            <li><strong>Mixed Random Variable</strong> This is a random variable with both discrete and continuous properties.  E.g. consider a game where you flip a coin, with probability 1/2 of getting heads and 1/2 of getting tails.  If you get heads, you win half a dollar, but if you get tails, you spin a wheel that can land in a continuous range between 0 and 1 that determine how much money you win.  The outcome of this experiment is discrete because there is a definite probability of getting half a dollar (continuous random variables give 0 probability to any one value).  But it is continuous also due to the wheel spin, where you have uniform probability of getting between 0 and 1 dollars.  The probability distribution for this might look like, if you think in terms of having a pound of probability to spread out, you'd put 1/2 pound directly on 0.5 (for flipping heads) and then spread the other 1/2 pound evenly between 0 and 1 (for spinning the wheel).  This mass/density function is weird, but the CDF is well defined - it would be 0 for x&lt0, increase linearly from 0 to 1/4 for x in [0, 1/2), it would jump instantly to 3/4 at x=1/2, then increase linearly from 3/4 to 1 for x in [1/2, 1], then it would be 1 for x>1.</li>
            <li>Python's numpy library has a convenient method for simulating a random event a given number of times random.choice(list to choose from, number of times to choose, replace after choosing), where the first arg could be a list 1-6 for a die, the second arg is 5 for rolling it 5 times, and the third arg is TRUE because you can roll the same value again.  It outputs a list of the results of its random choices.</li>
        </ul>

        <li><strong>Probability Distribution</strong> A general discription of a probability distribution is a function P: A->R, where A is related to the sample space S and the output R (reals) assigns a probability.  If S is the sample space, then A is the set of all subsets of S whose probability can be measured, i.e. all possible events.  Often, we use a random variable X to transform a sample space to a number (e.g Reals or Integers), and speak of the probability distribution of the random variable, which replaces the sample space S in the description above, and the arguments to the probability function are subsets of X.  Once a random variable is defined, you can ask what the probabiltiy of a given output value of the random variable.  Mapping all possible values of the random variable to their probability is the probability distribution of the random variable.</li>
        <ul>
            <li><strong>Probability Mass Function (PMF)</strong> This is how probabilities are assigned for discrete random variables.  Consider a random variable X, with values x mapped from each outcome in a sample space.  You are interested in the probability of each value of x, based on the probability of outcomes in the sample space.  This probability is a function from x to a probability, often denoted in a few different ways, p<sub>X</sub>(x) = P(X=x) = P({ω∈Ω s.t. X(ω)=x}), where p<sub>X</sub>(x)≥0 and the sum over all values of x of p<sub>X</sub>(x) equals 1.  The last representation makes explicit you want the total probability of all outcomes in the sample space that map to x under the random variable X.  In general, a random variable X can map many outcomes to the same value, and your task is to add up the probabilities of all those outcomes for each x to determine the probability of x.  e.g. you roll a die twice and could have a random variable F that maps to the first value of the die roll, a random variable S that maps to the second value of the die roll, and a random variable M = min(F, S).  If you want to know what is P(M=2) for example, you know the outcomes (2,2), (2,3), (2,4), (2,5), (2,6), (3,2), (4,2), (5,2), (6,2) are what map to 2 under M.  Since the outcomes have uniform probability of 1/(6*6), you count them and get that P(M=2) = 9/36. You can also look at the PMF over a range of X values, as in P(1&lt=X&lt=3), which equals P(X=1) + P(X=2) + P(X=3). Sometimes it is easier when looking at a range to calculate the probabilities of the X values not in that range and subtract that from 1.  e.g. for the flip a coin four times example, you would map the output range {0, 1, 2, 3, 4} to their probabilities, which are 1/16, 4/16, 6/16, 4/16, 1/16 respectively, thus P(X=1) = 4/16, and P(1&lt=X&lt=3) = P(X=1)+P(X=2)+P(X=3) = 4/16+6/16+4/16 = 14/16</li>
            <ul>
                <li><strong>Conditional PMF</strong> A conditional PMF is the PMF of a random variable X given that some event A has occurred.  It is written as p<sub>X|A</sub>(x) = P(X=x|A).  For instance you may have a random variable X that can take values of 1, 2, 3, or 4 with equal probability of 1/4 for each value, so for k in {1, 2, 3, 4}, P(k) = 1/4. You might know that event A = (X >= 2) has occurred, so you know X doesn't equal 1.  The conditional PMF would have P(X=1|A)=0, whereas P(X=2|A) = 1/3, P(X=3|A) = 1/3, and P(X=4|A) = 1/3.</li>
                <li><strong>Joint PMFs</strong> Given two random variables X and Y of the same experiment, we might be interested in the probability of events where X has some value and Y has some value, simultaneously.  So we want to know the probabilities of all pairs of (x, y) that arise from each outcome in the sample space.  This is the joint probability of X and Y, written p<sub>X,Y</sub>(x, y) = P(X=x and Y=y).  Think of X taking on value along the x axis, and Y taking on values along the y axis, and every outcome in the sample space gets mapped to some (x, y) under X and Y.  We could plot the probability of getting (x,y) as a value on the z axis.  Joint PMFs must have sum over all x, sum over all y, of P(X=x and Y=y) = 1, so it is a normalized probability distribution.  This extends to 3 or more random variables, e.g. with three random variables X, Y, Z we write the join probability p<sub>X, Y, Z</sub>(x, y, z) = P(X=x and Y=y and Z=z).</li>
                <ul>
                    <li><strong>Marginal PMF</strong> This is when you look at only one of the random variables at a time, p<sub>X</sub>(x) = sum over all y of p<sub>X, Y</sub>(x,y).  Which gives you the probability of getting the value x, regardless of the value of y.  This extends to three or more random variables, written as p<sub>X</sub>(x) = sum over all y and sum over all z of p<sub>X, Y, Z</sub>(x, y, z), which you can think of as fix x, and sum up the PMF over all y and z for that fixed x.  Do that for each x to get the marginal PMF of x.</li>
                    <li><strong>Conditional PMF</strong> This is when you give a condition to one of the random variables, and you want to know the probability of getting values of the other random variable given that condition.  Think of fixing Y to be some value y, and you want to know the probability of getting each value of x given that Y=y, then you have p<sub>X, Y</sub>(x|y) = P(X=x|Y=y) = p<sub>X, Y</sub>(x, y) / p<sub>Y</sub>(y).  You must have sum over all x of p<sub>X,Y</sub>(x|y) = 1 so the conditional PMF is still a normalized probability distribution.</li>
                    <li><strong>Multiplication Rule</strong> This applies the probability multiplication rule of P(A and B) = P(A)*P(B|A) to joint PMFs, as p<sub>X, Y</sub>(x, y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x).  This extends to three or more random variables, as p<sub>X, Y, Z</sub>(x, y, z) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) * p<sub>Z|X,Y</sub>(z|x,y)</li>
                    <li><strong>Independence</strong> Multiple PMFs are independent if their joint PMF equals the product of their marginal PMFs for each value of each random variable.  e.g. for random variables X, Y, and Z, they are independent if p<sub>X, Y, Z</sub>(x,y,z) = p<sub>X</sub>(x) * p<sub>Y</sub>(y) * p<sub>Z</sub>(z) for all x, y, z.  Intuitively, independence means that e.g. for two random variables, the outcome of Y does not change your beliefs about the outcome of X, p<sub>X|Y</sub>(x|y) = p<sub>X</sub>(x) (note this equation only works for p<sub>Y</sub>(y) nonzero so the prior definition of independence is more general).  Or for three random variables, p(X=x|Y=y, Z=z) = p(X=x). </li>
                </ul>
                <li><strong>Geometric PMF</strong> This PMF describes a discrete random variable with probabilities for successive values that decrease geometrically.  Consider the experiment of repeating a coin toss until you get heads, where you have probability p of getting heads on each toss.  Consider the random variable X from the outcomes in the sample space (e.g. H, TH, TTH, TTTH, etc.) to the number of tosses made.  The probability of k tosses will be (1-p)^(k-1) * p.  So p<sub>X</sub>(k) = P(X=k) = p * (1-p)^(k-1), where k=1,2,3,...  Thus P(X=1) = p, P(X=2) = p(1-p), P(X=3) = p(1-p)^2, and so on, where you are adding another factor of (1-p) for each successive value of X.  The PMF is highest at 1 and decreases with each value of X by a factor of (1-p), so this random variable has a geometric PMF and is called a geometric random variable.</li>
                <ul>
                    <li><strong>Memorylessness</strong> The geometric PMF is memoryless, meaning P(X>m+n | X>=m) = P(X>n) for all m and n in {0,1,2,...}.  The only memoryless discrete probability distribution or PMF is the geometric PMF.  If you consider the coin tossing example above, it means that if you for example toss two tails to start with, you want to know what is the probability of getting heads on subsequent tosses given you had two tails to start.  If X represents the number of tosses to get heads, then you are given the condition that X>2, and you are interested in P(X|X>2).  Well, since the past coin flips don't influence future ones, you know that P(X=3|X>2)=P(X=1), and P(X=4|X>2)=P(X=2).  Think of a tree of all possible outcomes for X, and you know the first two branches from the root are both tails, you are asking what is the probability model for the tree after that point, and it is a copy of the whole tree starting at that point.  You might define a random variable Y=X-2, which gives you the number of tosses it takes to get heads after the first two tosses.  So you can say P(Y=k|X>2) = P(X-2=k|X>2) = P(X=k).</li>
                    <li><strong>Expected Value</strong> The expected value of a geometric random variable X with probability of success p is E[X] and can be calculated using a few tricks.  Using the total expectation theorem, we can create two disjoint events, X=1 and X>1.  So, E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1].  First of all we know P(X=1)=p and P(X=1)+P(X>1)=1,so P(X>1)=1-p.  Next we know E[X|X=1] is the sum over x of x*P(X=x|x=1) = 1*1 = 1, because P(X=x|x=1) will be 0 for all terms except when x=1 for which it will be 1.  Finally, we need to know E[X|X>1].  Consider E[X-1], which is the expected value of a linear function of the random variable X, so we know E[X-1]=E[X]-1.  This is also true for the conditional expectation, E[X-1|X>1] = E[X|X>1]-1 (shown using the definition of expectation and using conditional probabilities). From this we get E[X|X>1] = E[X-1|X>1]+1.  Well, E[X-1|X>1] is the expected value of the number of trials after the first trial to get success, given the first trial was a failure and due to memoryless this should be equal to E[X] itself.  Thus E[X|X>1] = E[X]+1, which is basically saying that the expected value of the number of trials until success given the first trial was a failure, is the same as the expected value of the number of trails until success if you were starting over, plus one.  Putting everythign together we get E[X] = P(X=1)E[X|X=1] + P(X>1)E[X|X>1] = p + (1-p)*(E[X]+1)= p + E[X] + 1 - pE[X] - p = E[X] + 1 - pE[X], so 0 = 1 - pE[X], so E[X] = 1/p.</li>
                    <li><strong>Variance</strong> It can be derived that the variance of a geometric distribution is (1-p)/p<sup>2</sup></li>
                </ul>
                <li><strong>Binomial Distribution</strong> This PMF describes a discrete random variable that is the number of successes out of an n-trial experiment where each trial has a boolean outcome with probability p of success and 1-p of failure.  For example, tossing a coin n times, with probability p of heads and 1-p of tails, you get p<sub>X</sub>(k) = P(X = k) = (n k)*p^k*(1-p)^(n-k), the probability of k successes, where k is between 0 and n. e.g. the flip a coin four times example has a binomial distribution, the n=4 and p=0.5 (say succes is getting heads), and E(X)=2 adn Var(X)=1.  Another example is randomly guessing 20 questions on a multiple choice test with four options for each question where each option is equally likely to be right, then you would have a binomial distribution for your score with n=20 and p=0.25, and E(X)=5. 
                <ul>
                    <li><strong>Expected Value</strong> The expected value E(X), or average if you conducted many experiments is E(X) = n*p.  Algebraically you could show this by doing sum over k from 0 to n of k * p(X=k), with p(X=k) defined above.  An easier way of finding the expected value is to create n random variables X<sub>i</sub> for i=1 to i=n, where X<sub>i</sub> is 1 if the ith trial is a success or 0 otherwise.  X<sub>i</sub> is known as an indicator variable.  We have that X = sum over i of X<sub>i</sub>, since X is a count of the number of successes and can be between 0 and n.  We also have that for any i, E[X<sub>i</sub>]=sum over x<sub>i</sub> of x<sub>i</sub>*P(X<sub>i</sub>=x<sub>i</sub>) = 0*(1-p)+1*p = p.  Since the sum over i of X<sub>i</sub> is a linear combination of random variables, we have E[X] = E[sum over i of X<sub>i</sub>] = sum over i of E[X<sub>i</sub>] = sum over i of p = n*p.</li>
                    <li><strong>Variance</strong> Using the same indicator variables from the expected value, we know the variance of a single X<sub>i</sub> is E[(x<sub>i</sub> - E[X<sub>i</sub>])^2] = sum over x<sub>i</sub> of (x<sub>i</sub> - p)^2 * P(X<sub>i</sub>=x<sub>i</sub>) = (0-p)^2*(1-p) + (1-p)^2*p = p^2*(1-p) + (1 - 2p + p^2)*p = p^2 - p^3 + p - 2*p^2 + p^3 = p - p^2 = p(1-p).  Since each X<sub>i</sub> is independent, we have var(X) = var(sum over i of X<sub>i</sub>) = sum over i of var(X<sub>i</sub>) = sum over i of p(1-p) = n*p*(1-p).  So, var(X) = np(1-p).
                </ul>
                </li>
                <li><strong>Poisson Distribution</strong> This is a PMF for discrete random variables, common for counting the number of occurences of something in a given interval of time, like the number of cars through an intersection during a given hour of the day. It has a parameter λ that is the expected value or average value of the distribution.  This is the result of making the observation many times, e.g. 10 cars one day, 7 cars the next, 15 the next day, etc. - the average value of those counts as the number of observations approaces infinity (law of large numbers) is the expected value.  The variance or spread of the distribution is always equal to λ, thus bigger expected values always have a bigger variance.</li>
                <li>Python's scipy.stats library has a method to calculate the probability mass function value for a given random variable value of a binomial distribution and poisson distribution.  For the binomial distribution it is binom.pmf(value to calculate, number of trials, probability of success on each trial).  E.g. with the flip a coin four times example, you could get the probability of flipping 2 heads with binom.pmf(2, 4, .5).  For the poisson distribution, it is poisson.pmf(value to calculate, expected value). E.g. if the expected value of rainy days in the next month is 10, and you wanted to know the probability of having 6 you could use poisson.pmf(6, 10) </li>
                <li>The scipy.stats library also has a method for generating a list of random values that follow the poisson distribution (probably one for binomial too).  You can use poisson.rvs(expected value, size = num of values), e.g. poisson.rvs(10, size=1000) would generate a list of 1000 values following a poisson distribution with expected value 10.  You could then plot this or find the mean, or min and max values, etc.</li>
            </ul>
            <li><strong>Probability Density Function</strong> These define the probability distribution of a continuous random variable.  In principle, you would be interested in the probability that a continuous random variable falls in an interval between a and b, and you would go back to the sample space and add up the probability of all the outcomes that map to a value between a and b under the random variable.   As with discrete random variables, we don't want to go back to the sample space all the time to compute probabilities, but instead of pmf which places a certain amount of weight at each discrete value of the random value which defines the probability and adds up to 1, we define the density of weight along the continuum of a continuous random variable, with a probability density function f<sub>X</sub>(x).  We define that a random variable is continuous if we can describe the probability of getting a value between a and b as P(a≤X≤b) = integral from a to b of f<sub>X</sub>(x)dx.  This is analagous to a discrete random variable, if you want to know the total probability of X in [a, b], you would sum over x in [a, b] of P(X=x).  Any individual value a of a continuous random variable has probability 0 because the integral from a to a will be zero regardless of the value of the density function.  We require any density function to have f<sub>X</sub>(x) ≥ 0 for all x, and for integral from -∞ to ∞ of f<sub>X</sub>(x)dx = 1.  (Notice we don't require f<sub>X</sub>(x)≤1, unlike with PMFs where every value of the PMF must be less than 1). The probability density function returns values of probabilty per unit length, not probability itself like a pmf.  Over a small interval [x, x+δ], the probability that the random variable will take value x is approximately δ*f<sub>X</sub>(x).  We may have a set B that might be the union of multiple intervals of the random variable, and we can integrate over B to get the probability that X is in B.</li>
            <ul>
                <li><strong>Joint PDFs</strong> Like Joint PMFs, we might have two continuous random variables X and Y on some sample space, and we want to know the probabilities of pairs (x, y) that outcomes in the sample space map too.  So we need to define a distribution for pairs (x,y).  We say that X and Y are said to be jointly continuous if we can calculate the probability of (x,y) being in a set S (an event) using a double integral over S of f<sub>X,Y</sub>(x, y)dxdy, where f is the joint PDF.  We require f<sub>X,Y</sub>(x,y) ≥ 0 and the double integral over the entire xy plane is 1.  A joint PDF gives you probability per unit area, so you integrate that over an area to get probability.  Intuitively, for a small interval [x, x+δ] and [y, y+δ], the probability that (x,y) will be in that set is approximately δ<sup>2</sup>*f<sub>X,Y</sub>(x,y).</li>
                <ul>
                    <li><strong>Marginal PDF</strong> When you have a joint PDF with random variables X and Y, you may be interested in probabilities of getting values of X regardless of the value of Y.  Over a small interval, we want the probability P(x≤X≤x+δ) to be approximately δ*f<sub>X</sub>(x), and we want to give a meaning to f<sub>X</sub>(x) as the marginal PDF of X.  In terms of the joint PDF, this probability could be found by integrating over the area X in [x, x+δ] and Y from -∞ to ∞.  So, it is the double integral of y from -∞ to ∞ and x from x to x+δ of f<sub>X,Y</sub>(x,y) dx dy.  Since δ is small, the integral over x evaluates to approximately δ * f<sub>X,Y</sub>(x,y) at each value of y.  Then we integrate that over y from -∞ to ∞.  So we have that δ*f<sub>X</sub>(x)≈P(x≤X≤x+δ)=integral from -∞ to ∞ of δ * f<sub>X,Y</sub>(x,y) dy. Dividing out δ, this gives f<sub>X</sub>(x) = integral from -∞ to ∞ of f<sub>X,Y</sub>(x,y) dy</li>
                    <li><strong>Conditional PDF</strong> This is when you want to "fix" a value of one variable Y in a joint PDF, and get a PDF for the other variable X given that Y=y.  However, you can't "fix" values of Y since the probability of a single value is always 0, so really you want to know the PDF of X when Y is in an infintesimally small interval y to y+δ.  Also, recall the PDF doesn't tell you the probability of getting a particular value of X, rather it gives you the approximate probability that x will be in a small interval by multiplying the PDF at a point x by the length of that small interval.  Given all that, we want to define a conditional PDF f<sub>X|Y</sub>(x|y) so that P(x≤X≤x+δ|Y≈y) ≈ f<sub>X|Y</sub>(x|y)*δ.  We are using the Y≈y to say not that Y=y but Y is in an infintesimally small interval around y.  This leads us to define the conditional PDF as f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y).  Notice this is analagous to the discrete case.  It essentially gives you a normalized slice of the PDF at Y=y.  </li>
                    <li><strong>Multiplication Rule</strong> This is simply derived from the definition of a conditional PDF. f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x).  This is useful for finding the joint probability function when you know the probability of X, and the conditional probability of Y, and X and Y are not independent.  Consider breaking a stick of length l at some point X, and then breaking the remaining stick of length X again at a point Y.  The PDF of X is uniform assuming we are equally likely to break it at any point from 0 to l. Y is dependent on X because the value of Y you can get depends on what X was, it can be anywhere between 0 and X, and we asssume it is uniform on that interval.  (Imagine a tree diagram where you first have all the values of X between 0 and l all equally likely.  Then each of those branch out to possible Y values between 0 and X.  The Y branches will be different for each X, there will be more Y branches for larger values of X and less for smaller values of X.)  To get the probability of each outcome of the tree diagram (the experiment), you multiply the branches together, i.e. the probability of getting the X in that outcome, then the probability of getting the Y given that X.  So the probability density of any given outcome x, y is given by f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x) = 1/l * 1/x, where 0&ltx&ltl and 0&lty&ltx.</li>
                    <li><strong>Independence</strong> We say X and Y are independent if f<sub>X,Y</sub>(x,y) = f<sub>X</sub>(x) * f<sub>Y</sub>(y).  This is interpreted the same as in teh discrete case, where we are saying there is no conditional probability of y if you know x or vice-versa.  i.e. X doesn't change your beliefs about Y, and vice versa. It can be shown for independent random variables, P(X in A, Y in B) = P(X in A) * P(Y in B) where A and B are some events. If X and Y are independent, then f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x) * f<sub>Y</sub>(y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x), so the conditional probability of X is the same as the marginal probability of X, i.e. the value of Y doesn't have any impact on the PDF of X.</li>
                </ul>
                <li><strong>Uniform Distribution</strong> This would be a PDF that is 0 everywhere except for some interval [a, b].  That is,  f<sub>X</sub>(x) = 1/(b-a) for [a,b] and 0 elsewhere.  It mean that the random variable has the same probability of being in any two intervals of the same length inside of [a,b], and no probability of being outside the interval [a, b].  Notice that the total area under the curve is 1/(b-a) * (b-a) = 1, as required to be a PDF.  It's expected value is E[X] = integral from a to b of x* 1/(b-1)dx = (a+b)/2.  Intuitively this makes since as the center of mass of uniform density mass should be at the midpoint of the mass.  The variance is var(X) = σ<sub>X</sub><sup>2</sup> = integral from a to b of (x - (a+b)/2)<sup>2</sup> * 1/(b-a) dx = (b-a)<sup>2</sup>/12.  Thus, the standard deviation is σ<sub>X</sub> = (b-a)/sqrt(12), which intuitively makes sense that it is proprotional to the width of the interval [a,b].</li>
                <li><strong>Gaussian</strong> The Gaussian (normal) distribution is the most important probability distribution in probability theory, it shows up all over the place. Basically, if you have a random variable that can be decomposed into the sum of many independent random variables, then it will have an approximately normal distribution.  The standard normal N(0, 1): f<sub>X</sub>(x) = 1/sqrt(2pi) * e^(-x^2 /2).  To think about the shape, consider 1/(2^x), then 1/(2^abs(x)), then 1/(2^(x^2)).  The first illustrates the decreasing exponential function, then the second just reflects it across the y axis, then the third scales it horizontally in a dynamic ways, stretching the function horizontally between -1 and 1, with the most stretching happening at 0, and gradually stretching less out to -1 and 1, and then outside of -1 and 1 the function is squished horizonatlly more and more out to infinity.  The 1/sqrt(2pi) ensures the integratal from -∞ to ∞ is 1 as required to be a PDF.  The expected value E[X]=0 due to symmetry, and the variance var(X)=1 (can be shown with some calculus).</li>
                <ul>
                    <li><strong>General Normal</strong> The standard normal can be manipulated to translate it to have a different expected value and variance.  The general normal is N(μ, σ<sup>2</sup>): f<sub>X</sub>(x) = 1/(σ * sqrt(2pi)) * e^(-(x-μ)^2 / 2σ^2).  Consider that you are translating horizontally by μ by replacing x with x-μ, and going back to considering 1/(2^abs(x)), if you consider 1/(2^(abs(x)/c)), bigger values of c mean a bigger exponent which means the function decreases more rapidly.  This is essentially the underlying action of the σ^2 term in the exponent of the general normal, where the smaller sigma, the quicker the function decreases, and the larger sigma means the function decreases less quickly.  This corresponds to more or less variance.  the constant term at the beginning  of the normal function is scaled apporpriately to ensure the area under it is still 1.  With this formula, it turns out if you do the math you get E[X] = μ and var(X) = σ<sup>2</sup></li>
                    <li><strong>Function of Normal Random Variables</strong> As with any random variable X, a linear combination Y = aX + b has E[Y] = aE[X] + b, and var(Y) = a^2var(X).  So, for a normal random variable X has E[aX+b] = aμ+b and var(aX+b) = a^2*σ^2.  However, a nontrivial fact about normal random variables (not random variables in general) is that linear combinations of a normal random variable are themselves normal.  So Y=aX+b is also normal, and is the general normal N(aμ+b, a^2*σ^2).  Intuitively, consider that aX+b = a(X+b/a), so you are just translating x horizontally by b/a, then scaling that horizontally by a, which will still result in a normal shaped curve.</li>
                    <li><strong>Calculating Probabilities</strong> The CDF of the general normal distribution is the integral from -∞ to x of the normal, and this integral does not have a closed form.  So, calculating probabilities of a normal random variable having some value lying within some interval is normally done with a table of precalculated values.   You can use a table of CDF values for the standard normal N(0, 1) for any general normal.  The standard normal tables normally give you values of the CDF from 0 to about 2 in increments of 0.01.  So, 0 will have probability 0.5 since you have a 0.5 probability of X being less than 0.5, and will range up to probability 0.9817 for values less than or equal to 2.09.  You can "standardize" a general normal X with N(μ, σ^2) by applying a linear transformation of Y=(X-μ)/σ = (1/σ)X + (-μ/σ), and Y will be a normal PDF with E[Y]=(1/σ)E[X]+(-μ/σ)=(1/σ)μ+(-μ/σ)=0 and var(Y)=(1/σ)^2 * var(X) = (1/σ)^2 * σ^2 = 1.  Thus Y is N(0, 1), the standard normal.  This can be used to calculate CDF values of X using the standard normal table: P(X≤x) = P(Y≤(x-μ)/σ).  Intuitively, consider  that Y=(X-μ)/σ translates X horizontally so that the mean is moved to 0 for Y, then divides by X's standard deviation, so that the Y values are expressed as multiples of the standard deviation of X.</li>
                    <li><strong>Joint PDF of two Independent Normal Distributions</strong> The joint PDF of two Independent, normally distributed random variables X and Y, is f<sub>X,Y</sub>(x,y)=f<sub>X</sub>(x)*f<sub>Y</sub>(y) and called a bivariate normal.  This works out to 1/(σ<sub>x</sub> * σ<sub>y</sub> * sqrt(2pi)) * e^(-(x-μ<sub>x</sub>)^2 / 2σ<sub>x</sub>^2 - (y-μ<sub>y</sub>)^2 / 2σ<sub>y</sub>^2).  As usual, think of X and Y values being on the 2D plane, with the joint PDF plotted along the z axis.  The PDF will have a constant value along concentric ellipses in the XY plane, because the exponent will evaluate to a constant along the ellipses defined by (x-μ<sub>x</sub>)^2 / 2σ<sub>x</sub>^2 + (y-μ<sub>y</sub>)^2 / 2σ<sub>y</sub>^2.  This exponent term is the equation of an ellipse centered at (μ<sub>x</sub>, μ<sub>y</sub>).  The radius of the ellipse in the X direction increases as σ<sub>x</sub> increases, and the radius in the Y direction increase as σ<sub>y</sub> increases.  The joint PDF looks like a 3 dimensional bell, that is highest at (μ<sub>x</sub>, μ<sub>y</sub>) and decreases along concentric ellipses, whose radii are smaller/larger along the X or Y axis if σ for that axis is smaller/larger.  When σ<sub>x</sub>=σ<sub>y</sub>, you get it decreasing along concentric circles.</li>
                    <li><strong>Joint PDF of two Dependent Normal Distributions</strong>  This similarly forms a 3 dimensional bell shape PDF like with two independnent normal distributions.  However for independent normals, the axes of the ellipse are always parallel to the X-Y axes.  Thus when you take a slice of the bell at a particular value of X, you simply get the normal distribution of Y scaled (multiplied) by the probability of that value of X.  If you divide by the probability of X then you renormalize that slice of Y and so the normalized probability distribution of Y is the same at every value of X.  The most likely Y is always in the same place, etc.  The same argument holds for X on fixed values of Y.  For dependent normal distributions, the axes of the ellipse will not be parallel to the axes of X and Y.  Thus, imagine fixing X and taking the slice at X=x.  For different values of x, the distribution of Y will fundamentally change, e.g. if x is at the mean of x, then the distribution of Y is greatest at the mean of y, but if x is greater than the mean of x, you will slice through the bell in a way that makes the distribution of Y greatest at some point other than the mean of y.  So you might have for values of X greater than the mean of X, you expect to get values of Y greater than the mean of Y, or perhpas less than the mean of Y depending on the orientation of the joint PDF.</li>
                    <li><strong>The sum of Two Independent Normal Distributions</strong> If X and Y are independent, normally distributed random variables, then their sum W = X+Y is also normal.  This is because, as discussed below for derived distributions, f<sub>W</sub>(w) = integral over all x of f<sub>X</sub>(x)*f<sub>Y</sub>(w-x) dx.  If you plug in the PDF equations for the general normal of X and Y and integrate, you get an answer of the form ce^(-γw^2), which is a normal distribution with mean equal to μ<sub>x</sub> + μ<sub>y</sub>, and variance equal to σ<sub>x</sub>^2 + σ<sub>y</sub>^2.  The mean and variance of W shouldn't be a surprise because for any linear functions of random variables, E[X+Y]=E[X]+E[Y], and for independent random variables var(X+Y)=var(X)+var(Y).  The surprising thing is that W is normal.</li>
                </ul>
            </ul>
            <li><strong>Cumulative Distribution Function (CDF)</strong> This is a concept that usefully applies to both discrete and continuous random variables, both PMFs and PDFs, as well as mixed random variables, allowing a unifying concept for thinking about and proving things for all types of random variables.  It is defined as the probability F<sub>X</sub>(x) that a random variable will take a value less than or equal to x, i.e. F<sub>X</sub>(x) = P(X≤x) = sum over k≤x of p<sub>X</sub>(k) = integral from -∞ to x of f<sub>X</sub>(t)dt.  Where the first formula using a sum is for discrete random variables and the second with an integral is for continuous random variables.  For the continuous case, the derivative of the CDF eqauls the PDF, i.e. d/dx F<sub>X</sub>(x) = f<sub>X</sub>(x).  Consider the uniform continuous PDF, the CDF for this will be 0 for x&lta, then increase linearly from 0 to 1 between a and b, then be 1 for x>b.  For a discrete random variable, the CDF will be a staircase function that increases with each value of the PMF that has a nonzero probability by the probability of getting that value, and again top out at 1.  In general, the CDF is always increasing, and goes from a lowest value of 0 to a highest value of 1. The CDF is handy for calculating ranges of a discrete random variable, e.g. if you want to know P(3&lt=X&lt=6) = CDF(X=6) - CDF(X=2).  Or if you want to know P(X>6) = 1 - CDF(X=6).  This applies to continuous random variables also, where the CDF is helpful for getting the area under the curve.  You can get 1)the probability that the random variable is less than a certain amount by getting the value of the CDF at that point, 2)the probability of a range of the random variable by subtracting two values of the CDF at the endpoints of the range, or 3) the probability that the random variable is greater than a certain amount by subtracting the value of the CDF at that point from 1.</li>
            <ul>
                <li>Python's scipy.stats library has a method to calculate the cdf for a given random variable value of a binomial distribution or a poisson distribution. For binomial, it is binom.cdf(value to calculate, number of trials, probability of success on each trial).  For poisson, it is poisson.cdf(value to calculate, expected value).</li>
                <li>The scipy.stats library also has method to calculate the cdf for a normal distribution.  norm.cdf(value to calculate, mean of distribution, standard deviation)</li>
            </ul>
        </ul>
        <li><strong>Expected Value</strong> Represented as E[X], the expected value of a discrete random variable X is E[X] = Sum over x of x * p<sub>X</sub>(x).  Loosely speaking, if you interpret the probability for each value x of a random variable X given by a PMF as the frequency of getting x if you run the experiment infinite times, then the expected value would give you the average value of x you would get.  E[X] could also be interpreted as the center of gravity of the PMF of X, which can make it easy to visualize the value of E[X].  If you interpret the probability of each value of x as a weight at point x, the center of gravity is the point along the line x where it would balance perfectly.  So a uniform discrete probability from 1 to n, would have a center of mass right in the middle at n/2 which is the expected value.  For a continuous random variable the expected value is defined analagouly, E[X] = integral from -∞ to ∞ of xf<sub>X</sub>(x)dx, which is the same as finding the center of gravity of a mass with varying density given by f<sub>X</sub>(x).  As with discrete random variables, you can intuitively interpret it as the average value you expect to get after infinite experiments.</li>
        <ul>
            <li><strong>Conditional Expectation</strong> This is the expected value of a conditional PMF, denoted E[X|A] = Sum over x of x * P(X=x|A) = Sum over x of x * p<sub>X|A</sub>(x).  Or for continuous random variables, say of a joint PDF of X and Y, you would have E[Y|X] = integral from -∞ to ∞ of y*f<sub>Y|X</sub>(y|x)dy.  It is no different than a regular expected value, you are simply using the conditional PMF for the random variable X.  Following the example above for conditional PMF, you may have a random variable X with a PMF of P(X=k)=1/4 for k in {1, 2, 3, 4} otherwise P is 0, so E[X]=2.5.  If event A is X>=2, then P(X=k|A)=1/3 for k in {2, 3, 4} otherwise P is 0, so E[X|A]=3.</li>
            <li><strong>Functions of a Random Variable</strong> You may have a random variable Y that is a function of another random variable X, say Y=g(X).  E.g. X maps the sample space to x, and Y maps x to y.  Thus, Y will also be a function of the sample space and is a random variable.  The expected value of Y is E[Y] = sum over y of y*P(Y=y).  This requires you to know the PMF of Y so you can plug in the probabilities.   You may not know or want to figure out the PMF of Y and instead know and want to rely on the PMF of X.  It can be derived that E[Y] = sum over x of g(x)*P(X=x), which is called the law of the unconcious statistician.  To prove this intuitively, many values of x may map to the same value of y, and this formula simply adds up the probability of each x times the y it gets mapped to.  In the end the sum of all the probabilities of x that map to a given y is the same as the probability of y given by the PMF of Y, so it is equivalent to the first way of calculating E[Y].  For the continuous case, it is analagous, with E[g(x)] = integral from -∞ to ∞ of g(x) * f<sub>X</sub>(x)dx.  In general E[g(x)] does not equal g(E[X]), although very importantly it is equal if g is a linear function of the random variable X, as shown below.</li>
            <li><strong>Linear Functions of a Random Variable</strong> If you consider a constant to be a random variable where every outcome of the sample space is always mapped to the same constant number, then the PMF for that random variable would be 0 everywhere except at the constant where it would be 1.  The expected value of this random variable would be the constant.  So, E[a] = a if a is a constant.  (E[X] = sum over x of x*P(X=x), P(X=x)=0 everywhere except at a where P(X=a)=1, so the sum has only one nonzero term which is a*P(X=a) = a*1 = a).  Now, consider E[aX], where you can consider a new random variable Y is a function of X, so Y=g(X)=aX.  Well E[Y] = sum over x of g(x)*P(X=x) = sum over x of a*x*P(X=x) = a * sum over x of x*P(X=x) = a * E[X].  So we have E[aX] = aE[X] where a is a constant.  e.g. X could be people's heights in inches, and Y is people's height in cm, so a = 2.54.  Y simply scales X by 2.54, and the expected value of X is simply scaled by 2.54 as well.  Finally, E[aX + b] with a and b constants, where Y = g(X) = aX+b, then E[Y] = sum over x of g(x)*P(X=x) = sum over x of (ax+b)*P(X=x) = sum over x of ax*P(X=x) + sum over x of b*P(X=x) = a * sum over x of x *P(X=x) + b*sum over x of P(X=x) = aE[X] + b. So E[aX+b] = aE[X] + b.</li>
            <li><strong>Total Expectation Theorem</strong> Consider a sample space divided into n disjoint events A1, A2, ... An.  Consider a random variable of that sample space, X.  You can ask what is P(X=x) for the PMF of X, or you can ask P(X=x|An) for the conditional PMF of X given the event An occurs.  Well for any given x, P(X=x) should equal P(A1)*P(X=x|A1) + P(A2)*P(X=x|A2) + ... + P(An)*P(X=x|An).  Thus, multiplying both sides by x and taking the sum over all x, gives sum over x of x*P(X=x) = sum over x of (x*P(A1)*P(X=x|A1) + x*P(A2)*P(X=x|A2) + ... + x*P(An)*P(X=x|An)), which means E[X] = P(A1)E[X|A1] + P(A2)E[X|A2] + .... + P(An)E[X|An].</li>
            <li><strong>Law of Iterated Expectations</strong> Given two random variables X and Y, you might want E[X|Y].  For example, consider breaking a stick at a point y, and then breaking the remaining stick at a point x.  The point y is chosen with uniform probability across the length of the stick, and then the point x is chosen with uniform probability across the remaining length y.  So E[Y]=l/2 where l is the length of the stick (since Y's probability  is uniformly distributed, l/2 is the center of mass), and E[X|Y=y] = y/2 (this is the center of mass for the uniform probability between 0 and y).  Once you know what y is, then you know the conditional probability distribution of X, and thus you can calculate an expected value for X.  But if you don't know what y is yet, then E[X|Y] is itself a random variable, and is a function g(Y) of Y equal to g(Y)=Y/2, (consider the event Y=y getting mapped to the number E[X|Y=y], for every value of Y).  We can ask what is the expected value of g(Y) = E[X|Y]? (Consider a tree diagram that first branches into possible values of Y, and then for each Y, has the possible values of X, each Y branch will have an expected value of X, E[X|Y=y], and we want over all values of Y, what is the expected value of those expected values?).  So, per the definition of expected value in the discrete case, E[E[X|Y]] = Sum over all y of E[X|Y=y]*p<sub>Y</sub>(y).  By the total expectation theorem, we know that this formula is equal to E[X], you are breaking up the sample space into disjoint events where Y=y, then summing up the conditional probability of X within each event times the probability of that event.  So, the cool result is that E[E[X|Y]] = E[X] and this is just another way of stating the total expectation theorem, but is called the law of iterated expectations.  For the stick example, it allows us to calculate that E[X]=E[E[X|Y]]=E[Y/2]=(1/2)E[Y]=1/2*l/2=l/4.</li>
            <li><strong>Joint PMFs</strong> If you have a function of random variables that involves two or more random variables, g(X, Y), and want to know its expected value.  g(X,Y) is itself a random variable, say Z, so as usual, E[Z] = sum over z of z*p<sub>Z</sub>(z).  As with other functions of a random variable, you can show that E[Z] = E[g(X,Y)] = sum over x, sum over y of g(x,y)*p<sub>X,Y</sub>(x, y).  Essentially you are taking the joint PMF of X and Y and for each x and y, multiplying z by that joint PMF, so if multiple x,y pairs gets mapped to the same z value, the joint PMF probabilities will add up to the total probability for that value of z.  So, you don't need to know explicitly the PMF of Z to get E[Z].  This holds in the continuous case as well, except E[g(X,Y)] = double integral from -∞ to ∞ of g(x, y)*f<sub>X,Y</sub>(x, y)dxdy, where f is the joint PDF of X and Y.  In general, E[g(X,Y)] does not equal g(E[X,Y]).  However, there are important exceptions.  First, for linear functions of multiple random variables you have E[X+Y] = E[X] + E[Y], e.g. for E[X+Y]=sum over x of (sum over y of (x+y)*P(X=x, Y=y)) = sum over x of (sum over y of x * P(X=x, Y=y)) + sum over x of (sum over y of y * P(X=x, Y=y)) = sum over x of (x * sum over y of P(X=x, Y=y)) + sum over y of (sum over x of y *P(X=x, Y=y)) = sum over x of (x * P(X=x)) + sum over y of (y * sum over x of P(X=x, Y=y)) = E[X] + sum over y of (y*P(Y=y)) = E[X]+E[Y].  Second, if X and Y are independent, then you also have E[XY] = E[X]E[Y] because E[XY] = sum over x of (sum over y of x*y*p(X=x, Y=y)) = sum over x of (sum over y of x*y*p(X=x)*p(Y=y)) = sum over x of x*P(X=x) * sum over y of y*P(Y=y) = sum over x of x*P(X=x) * E[Y] = E[Y] * sum over x of x*P(X=x) = E[Y]*E[X] = E[X]E[Y].  We can also show that E[g(X)h(Y)]=E[g(X)]E[h(Y)] if X and Y are independent.  This is a result of if X and Y are independent that g(X) and h(Y) are independent, for any functions g and h of X and Y respectively. </li>
        </ul>
        <li><strong>Variance</strong> Variance denoted var(X) = E[(X-E[X])^2] is the expected value of a particular function of a random variable - the deviation squared.  It is used to measure how spread out a random variable is, and you might first think to define a random variable called the deviation which is X-E[X] and find its expected value, E[X-E[X]] but this will be 0 because the differences are signed and cancel each other out.  Next you might think to take the absolute value of that difference but it is more useful to take the square of that difference which is what variance is.  Consider a random variable X from a sample space to values x.  You can calculate the expected value E[X], which equals a constant. Now consider a function of X, Y = g(x) = (X - E[X])^2.  This subtracts a constant from the random variable X and then squares it, and Y is a legitimate function of the sample space and a random variable itself.  Y represents the distance squared between each value of X and the expected value of X.  Being squared, it gives greater weight to greater distances.  The expected value of Y is the variance, so by definition var(X) = E[Y] = sum over x of g(x)*P(X=x) = sum over x of (x - E[X])^2 * P(X=x).  This can be shown to equal sum over x of (x^2 - 2xE[X] + E[X]^2)*P(X=x) = sum over x of x^2*P(X=x) - sum over x of 2xE[X]*P(X=x) + sum over x of E[X]^2*P(X=x) = E[g(x)=x^2] - E[g(x)=2xE[X]] + E[g(x) = E[X]^2] = E[X^2] - 2E[X]E[x] + E[E[X]^2] = E[X^2] - 2E[X]^2 + E[X]^2 = E[X^2] - (E[X])^2. This result is somewhat simpler way to calculate it and is also true for continuous random variables.  For the continuous case, the variance is defined analagously, as var(X) = σ<sub>X</sub>^2 = E[(x-E[X])^2] = integral from -∞ to ∞ of (x-E[X])^2 * f<sub>X</sub>(x)dx</li>
        <ul>
            <li><strong>Linear Functions of a Random Variable</strong> The Variance is always positive since it is a sum of squared values (non negative values).  Adding a constant to a random variable doesn't change it's variance, adding a constant simply translates the random variable and doesn't change the spread of the random variable.  If you multiply a random variable by a constant, then the variance is multiplied by the square of that constant, which can be seen by var(aX) = sum over x of (ax - E[aX])^2 * P(X=x) = sum over x of (a^2*x^2 - 2axE[aX] + E[aX]^2) * P(X =x) = a^2 * sum over x of x^2 * P(X=x) - 2aE[aX] * sum over x of x * P(X=x) + E[aX]^2 * sum over x of P(X=x) = a^2 * E[X^2] - 2a^2E[X]E[X] + a^2 * E[X]^2 = a^2*E[X^2]-2a^2*E[X]^2+a^2*E[X]^2 = a^2*E[X^2] - a^2*E[X]^2 = a^2 * (E[X^2] - E[X]^2) = a^2 * var(X).  Altogether these properties can be shortened to var(aX + b) = a^2 * var(X) where X is a random variable and a and b are constants.</li>
            <li><strong>Standard Deviation</strong> The standard deviation is the square root of the variance of a random variable X, often denoted as σ<sub>X</sub> = sqrt(var(X)).  This is often more convenient than the variance because the variance will be in different units than the value of your random variable, namely it will be in those units squared.  Standard deviation will be in the same units as your random variable.</li>
            <li><strong>Sum of Independent Random Variables</strong>If X and Y are independent, then var(X+Y) = var(X) + var(Y), because var(X+Y) = E[(X+Y)^2]-E[X+Y]^2 = E[X^2 + 2XY + Y^2]-(E[X]+E[Y])^2 = E[X^2]+2E[XY]+E[Y^2]-E[X]^2-2E[X]E[Y]-E[Y]^2 = E[X^2]+E[Y^2]-E[X]^2-E[Y]^2 = E[X^2]-E[X]^2 + E[Y^2]-E[Y]^2 = var(X) + var(Y). </li>
            <li><strong>Sums of Random Variables</strong> If you have a random variable that is the sum of random variables Y = Sum over i of X<sub>i</sub>.  For simplicity assume the expected value of each X<sub>i</sub> is 0.  Then, you get var(Y) = E[(Y-E[Y])^2] = E[(X<sub>1</sub>+....+X<sub>n</sub>-E[X<sub>1</sub>+....+X<sub>n</sub>])^2]=E[(X<sub>1</sub>+....+X<sub>n</sub>-E[X<sub>1</sub>]+....+E[X<sub>n</sub>])^2]=E[(X<sub>1</sub>+....+X<sub>n</sub>)^2]=E[Sum over i of X<sub>i</sub>^2 + Sum over i, j where i&lt>j of X<sub>i</sub>X<sub>j</sub>]=E[Sum over i of X<sub>i</sub>^2] + E[Sum over i, j where i&lt>j of X<sub>i</sub>X<sub>j</sub>]=Sum over i of var(X<sub>i</sub>) + Sum over i, j where i&lt>j of cov(X<sub>i</sub>, X<sub>j</sub>). </li>
            <li><strong>Conditional Variance</strong> Consider two random variables X and Y.  We can define a conditional variance of X given that Y=y, as var(X|Y=y).  (Imagine the probability distribution of X given that Y=y, it will have its own variance just as it has its own Expected value E[X|Y=y]).  Well, you can define the random variable (X-E[X|Y=y])^2, which for a given value of y, tells you the squared deviations of X from the expected value of X at that y.  Then you can ask, what is var(X|Y=y) = E[(X-E[X|Y=y])^2|Y=y]? More generally, this can be made into a function of Y, as var(X|Y) = E[(X-E[X|Y])^2|Y], which maps each event in your sample space where Y=y to the realized value var(X|Y=y).</li>
            <li><strong>Law of Total Variance</strong> This states that for two random variables X and Y, var(X) = E[var(X|Y)] + var(E[X|Y]).  Intuitively, first imagine that for each value of Y, X took on exactly one value with no variance, and that one value is thus also the E[X|Y] for each X (imagine branches of possible Y values, and each one has only one possible x value associated with it).  Then var(X|Y)=0 for all y, so using the law of total variance, var(X) = var(E[X|Y]). So the variance of X just equals the variance of the X's you get across all the Y's.  Secondly, imagine that the expected value of X is the same for every Y, although the variance of X may be different for different Y.  So E[X|Y]=c for all Y and thus the variance of E[X|Y]=0.  Thus the law of total variance says var(X) = E[var(X|Y)], so the variance of X is the average of the variances of X for each different Y.  So, you can see how the two terms in the law of total variance independently contribute to the total variance of X.  To prove this law algebraically, first recall, var(X)=E[X^2]-E[X]^2.  This would also apply to a conditional variance, as the conditional probability distribution of X is still a probability distribution with a variance.  So, var(X|Y)=E[X^2|Y]-E[X|Y]^2.  Since var(X|Y) is itself a random variable that is a function of Y, we can ask about its expected value, and we get E[var(X|Y)] = E[E[X^2|Y]-E[X|Y]^2] = E[E[X^2|Y]]-E[E[X|Y]^2].  Given the law of iterated expectations, we have E[E[X^2|Y]] = E[X^2].  So, we have E[var(X|Y)] = E[X^2]-E[E[X|Y]^2] (lets call this result 1).  Now, separately consider that E[X|Y] is a random variable that is a function of Y, and we could ask about its variance, we have var(E[X|Y])=E[E[X|Y]^2]-E[E[X|Y]]^2=E[E[X|Y]^2]-E[X]^2 (lets call this result 2), again simplifying the last term using the law of iterated expectations.  If we add together results 1 and 2, we get E[var(X|Y)] + var(E[X|Y]) = E[X^2]-E[E[X|Y]^2]+E[E[X|Y]^2]-E[X]^2 = E[X^2]-E[X]^2 = var(X).</li>
        </ul>
    </ul>

    <h2>Derived Distributions for Functions of Random Variables</h2>
    <ul>
        <li><strong>Derived Distribution</strong> This is the PDF or PMF of a function of one or more random variables, e.g. g(X,Y).</li>
        <ul>
            <li><strong>Expected Value</strong> You don't need to find the derived distribution if all you need to do is find the expected value of the function.  This is per the law of the unconcious statistician.</li>
        </ul>
        <li><strong>Discrete Derived Distribution</strong> Take a discrete random variable X, and a function of it Y=g(X).  The PMF for a given y is the sum of the PMF for X for each x that maps to that y.  That is, p<sub>Y</sub>(y) = P(g(X) = y) = sum over x where g(x) = y of p<sub>X</sub>(x).</li>
        <li><strong>Continuous Derived Distribution</strong> For a continuous random variable X and function Y = g(X), you can think of mapping probabilities from one or more x's to a y, because the probability of any given point is 0.  You have to think the probability over a small interval of Y should be the same as the probability over the interval of X that maps to it.  It is generally easiest to find the CDF of Y F<sub>Y</sub>(y) = P(Y≤y), and then taking its derivative to get the PDF f<sub>Y</sub>(y) = d/dy F<sub>Y</sub>(y)</li>
        <ul>
            <li>For example, consider X with a uniform distribution between [0, 2], so f<sub>X</sub>(x) = 1/2 between 0 and 2.  Thus F<sub>X</sub>(x) = 1/2 x between 0 and 2, and is 0 before this interval and 1 after this interval.  Consider now Y = X^3.  First we find the CDF of Y, which is P(Y≤y).  If Y≤y then this means X^3≤y, so P(Y≤y) = P(X^3≤y).  If X^3 is less than some number a, then X must be less than a^(1/3).  So, P(X^3≤y)  = P(X ≤ y^(1/3)).  The probability that X is less than some number is 1/2 x for x between 0 and 2, 0 before this and 1 after this, so P(X ≤ y^(1/3)) = 1/2 y^(1/3), where y^(1/3) is between 0 and 2, 0 before this and 1 after, so y is between 0 and 8, 0 before, and 1 after.  So, F<sub>Y</sub>(y) = 1/2 y^(1/3).  The derivative is 1 / (6* Y ^(2/3)) between y=0 and y=8, which is the PDF.</li>
            <li>Another example, consider V is the constant speed of a car between 30 and 60mph with uniform probability.  You want to know the PDF of the time it takes to travel 200miles, T(V)=200/V.  The PDF of V is 1/30 for v between 30 and 60 and 0 elsewhere.  The CDF of V is P(V≤v) = v/30 - 1 for v between 30 and 60, and is 0 before that interval and 1 after.  Well, the CDF of T(V) is P(T≤t) = P(200/V≤t) = P(V ≥ 200/t).  The probability that V is greater than something is 1 minus the CDF, or 1-v/30+1 = -v/30+2 again for v between 30 and 60, with 1 before that interval and 0 after.  So P(V ≥ 200/t) = -200/30t + 2, for 200/t greater than 30 and less than 60 with 1 for 200/t less than 30 and 0 for 200/t greater than 60, or for t less than 200/30 and greater than 200/60 with 1 for t greater than 200/30 and 0 for t less than 200/60.  Taking the derivative, you get the PDF of T, which is 200/(30t^2) for t between 200/60 and 200/30, and 0 elsewhere.</li>
        </ul>
        <li><strong>Linear Derived Distribution</strong> For a Derived distribution of the form Y = aX + b, the b term simply shifts the distribution right if b positive or left if b is negative.  The a term stretches the X distribution horizontally, essentially the PDF for a given interval of X is mapped to an interval of Y streched out by a.  (e.g. for a=2, the PDF from X = 2 to 4 gets mapped to the PDF of Y from 4 to 8, thus stretching out the PDF.)  With stretching, this increases the area under the curve so the PDF of Y has to be multiplied by 1/a to keep the area at 1.  Formally, if Y=aX+b, then F<sub>Y</sub>(y) = P(Y≤y) = P(aX+b≤y).  If a is positive, this equals P(X≤(y-b)/a)=F<sub>X</sub>((y-b)/a), and taking the derivative using the chain rule gives you 1/a * f<sub>X</sub>((y-b)/a).  If a is negative, then this equals P(X≥(y-b)/a)=1-F<sub>X</sub>((y-b)/a), and taking the derivative you get -1/a * f<sub>X</sub>((y-b)/a).  So, if Y=aX+b, then f<sub>Y</sub>(y) = 1/abs(a) * f<sub>X</sub>((y-b)/a).  This corresponds to shifting f<sub>X</sub> to the right by b if b is positive or the left by b if b is negative, and then stretching f<sub>X</sub> horizontally by a if a is >1 or shrinking it horizontaly if a&lt1, while also streching/shrinking vertically by 1/a.  This can be used to show that a linear combination of a normal distribution is still a normal distribution, given N(μ, σ<sup>2</sup>): f<sub>X</sub>(x) = 1/(σ * sqrt(2pi)) * e^(-(x-μ)^2 / 2σ^2), you get for Y=aX+b, f<sub>Y</sub>(y) = 1/abs(a) * f<sub>X</sub>((y-b)/a) = 1/(abs(a) * σ * sqrt(2pi)) * e^(-((y-b)/a-μ)^2 / 2σ^2) =  1/(abs(a) * σ * sqrt(2pi)) * e^(-(y-(aμ+b))^2 / 2(aσ)^2) = N(aμ+b, (aσ)^2)</li>
        <li><strong>Derived Distribution of multiple random variables</strong> For a random variable of the form Z = g(X,Y) = Y/X, the method to finding the PDF is generally the same, where you find the CDF first and then take its derivative.  Consider X, and Y are independent random variables both with uniform probability between 0 and 1, and 0 probability elsewhere.  So they have a joint PDF equal to 1 in the unit square and 0 elsewhere.  We first want P(Z≤z)=P(Y/X≤z)=P(Y≤zX).  First if z is negative, then P(Y≤zX) = 0, because X is always positive so this would require Y to be negative, and Y is always postive.  So given z is positive, then if z≤1, then P(Y≤zX) is the volume of the joint PDF of X,Y over the triangular area under Y=zX, between X=0 and X=1, so P(Y≤zX)=1 * (1/2 * 1 * z) = z/2 for z≤1.  If z>1, then P(Y≤zX) is the volume of the joint PDF of X,Y over the unit square, less the triangular area above Y=zX between y=0 and y=1.  So, P(Y≤zX) = 1 * (1 - (1/2 * 1 * 1/z)) = 1 - 1/(2z) for z>1.  To get the PDF of Y/X, we now take the derivative, so for z&lt0, it is 0, for z≤1 you get 1/2, and for z>1 you get 1/(2z^2).</li>
        <li><strong>Monotonic Function of a Random Variable</strong> If X is a random variable and Y=g(X) is a function of it that it is strictly monotonic, i.e. always increasing or always decreasing, but not necessarily the same slope throughout.  In this case, consider a small interval of X x≤X≤x+δ, this is a subset of X, and it should have the same probability as g(x)≤Y≤g(x+δ).   Now, we know the length of the interval in X is δ, but how big is the interval in Y?  Well, the slope of g(X) at x gives you the change in y over change in x at x.  Since g(X) is monotonic, you can approximate for small δ, that the change in x is delta, so the change in y is slope of g(X) at x times δ.  Or the size of the interval in Y is dg(x)/dx*δ, actually it is the absolute value of that quantity to give a length.  We know the probability of X at the point x for small δ is δ*f<sub>X</sub>(x) and the probability of Y at the point y for a small interval is the same except with y's.  Well, the small interval δ in X corresponds to the small interval in Y of dg(x)/dx*δ, so δ*f<sub>X</sub>(x) = abs(dg(x)/dx*δ)*f<sub>Y</sub>(y), thus f<sub>X</sub>(x) = abs(dg(x)/dx)*f<sub>Y</sub>(y), where y = g(x).   Now you want to eliminate x and solve for f<sub>Y</sub>(y).  For example consider Y=X^3.  Then f<sub>X</sub>(x) = abs(dg(x)/dx)*f<sub>Y</sub>(y), so f<sub>X</sub>(x) = 3x^2*f<sub>Y</sub>(y), so f<sub>Y</sub>(y) = f<sub>X</sub>(x)/(3x^2).  Well since we know y=x^3, we know x=y^(1/3), so f<sub>Y</sub>(y) = f<sub>X</sub>(y^(1/3))/(3y^(2/3)). This you could solve given the PDF of X.  Note in general, when the slope of g(x) is shallow, the PDF of Y is high because that means the probability for many x's are getting mapped to a small number of y's.  Likewise if the slope of g(x) is steep, then the PDF of Y is low because just a small number of x's are getting mapped to a large number of y's, thus spreading out the probability over many y's and making the probability distributed more widely and thus smaller. </li>
        <li><strong>The sum of two random variables</strong> Consider two independent random variables X and Y, and their sum W = X+Y.  The, P(W=w) = P(X+Y=w) = Sum over x of P(X=x)*P(Y=w-x).  This is true since X and Y are independent, and it picks out all the points where the sum of x and y is w and adds up their probability for any given w.  For the discrete case, this is p<sub>W</sub>(w) = Sum over x of p<sub>X</sub>(x)*p<sub>Y</sub>(w-x).  This is called the convolution formula. One way to get the pmf of W is to take a graph of the pmf of X and a graph of the pmf of Y, and then flip the graph of Y across the vertical axis.  Then shift the graph of y to the right w units (for positive w).  This aligns the values of Y and X that add up to W.  Then you multiply the PMFs together and add this up across all X's to get the PMF of W at w.  For the continuous case, it is analagous, you have f<sub>W</sub>(w) = integral over all x of f<sub>X</sub>(x)*f<sub>Y</sub>(w-x) dx.</li>
    </ul>

    <h2>Covariance and Correlations</h2>
    <ul>
        <li><strong>Covariance</strong> Consider two discrete random variables X and Y, which are not independent.  If you plotted the joint pmf p<sub>X,Y</sub>(x,y), by simply putting a dot in the X-Y plane everywhere that (x,y) has a nonzero probability, you would see a pattern such as the points tend to fall along a line with positive slope so that bigger values of x seem to get paired with bigger values of y, or similarly fall along a line with negative slope where bigger values of x seem to get paired with smaller (more negative) values of y.  Thus the probability of x very much depends on what y you look at, and the probability of y very much depends on what x you look at.  You can calculate E[X] and E[Y] based on the marginal PMFs of X and Y.  Now you can ask for any given (x,y), what is x-E[X], and what is y-E[y]?  If you multiply these together you can define a random variable for all (x,y) of (x-E[X])(y-E[Y]), and this will be positive if x and y are both above or both below their respective means, and negative if x is above its mean and y is below its mean or vice-versa.  If you take the expected value of this, you get the covariance, cov(X,Y) = E[(X-E[X])(Y-E[Y])], this will give you a positive number if (x,y) pairs tend to fall on a line with positive slope, and a negative number if (x,y) pairs tend to fall on a line with negative slope. For the former, it means as x increases, so does y.  For the latter, it means as x increases, y decreases.</li>
        <ul>
            <li><strong>Variance</strong> Notice that the covariance of a random variable with itself is E[(x-E[X])(x-E[X])] = E[(x-E[X])^2], which is equal to var(X).  If you plot X along the horizontal axis and X along the vertical axis, then all the possible pairs (x,x) fall along the line where X=X, i.e. a line with slope 1 passing through the origin.  So thinking in terms of covariance the deviations of both X's are always the exact same sign, so you know you will get a positive covariance as you should.  If the distribution of x is spread out so that high values of x have decent probability, then the covariance will be larger, but if higher values of x have lower probability and there isn't as much spread, then you know the covariance will be a smaller value overall.  All of this behavior is exactly how variance should behave.</li>
            <li><strong>Shortcut Calculation</strong> If you do the algebra cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY - E[Y]X - E[X]Y + E[X]E[Y]] = E[XY]-E[E[Y]X]-E[E[X]Y]+E[E[X]E[Y]] = E[XY]-E[Y]E[X]-E[X]E[Y] + E[X]E[Y]E[1] = E[XY] = E[XY]-2E[X]E[Y]+E[X]E[Y] = E[XY]-E[X]E[Y]. This form is often easier to work with.</li>
            <li><strong>Linear Function of a random variable</strong> We have cov(aX+b, Y) = E[(aX+b-E[aX+b])(Y-E[Y])] = E[(aX+b-E[aX]-E[b])(Y-E[Y])] = E[(aX+b-aE[X]-b)(Y-E[Y])] = E[a(X-E[X])(Y-E[Y])] = a*cov(X,Y).  This is saying that if you shift X by b, then it doesn't change the covariance, because the mean of X gets shifted along with the values of X, so the distance between each value of X and the mean of X doesn't change, so the covariance is unaffected.  However, the multiplier a actually stretches the distribution of X if greater than 1, or squishes it if less than 1, so now the distances between X and the mean of X are a times what they used to be, and the covariance is a times what it used to be.</li>
            <li><strong>Independent Random Variables</strong> If X and Y are independent, then cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[X-E[X]]E[Y-E[Y]] = 0 * 0 = 0.  Consider for independent random variables, if you fix Y at y, then that slice of probability at Y=y, equals the marginal probability of X, scaled by the probability of Y at y.  So, you would expect that just like E[X-E[X]] would give you 0 for the marginal probability of X, so will E[X-E[X]] for that slice along Y=y.  Along that slice at Y=y, E[Y-E[Y]] is constant, equal to whatever E[Y-E[Y]] for the marginal probability of Y at Y=y.  It doesn't much matter, because along every slice of Y, E[X-E[X]] will be 0, so the covariance will evaluate to 0.  E[X-E[X]] is nonzero at the slice Y=y when the distribution of X at Y=y is different than the marginal distribution of X, so that the X-E[X] terms don't zero each other out at Y=y, and of course this means X and Y must be dependent. Covariance only becomes nonzero if there is a pattern where over many slices of Y, E[X-E[X]] tends to be nonzero and the same sign as E[Y-E[Y]], or nonzero and the opposite sign as E[Y-E[Y]].  Over the many slices of Y, if the signs are sometimes the same and sometimes different, then they will cancel each other out and the covariance will be close to 0.  The preceding discussion sort of assumes a uniform distribution of X and Y, so keep in mind the deviances (X-E[X] and Y-E[Y]) are getting weighted by probabilities as you get their expected values and covariance, but fundamentally the concept is the same, it just could be affected e.g. if some things look correlated but have very low probability then they won't count much to the covariance, or if x,y pairs that are very uncorrelated have high probability they will count more to the covariance being close to 0.  NOTE it is not true that if X and Y have 0 covariance then they must be independent.  You could have dependent X and Y that e.g. both take on values between -1 and 1, and both have expected values of 0, so cov(X,Y)=E[XY].  You could have that if one value is nonzero, then the other must be 0, so the clearly effect each other (dependent), but cov(X,Y)=E[XY]=0.  i.e. they can be dependent without forming a covariant relationship.</li>
        </ul>
        <li><strong>Correlation Coefficient</strong> The correlation coefficient, rho, is ρ = E[((X-E[X])/σ<sub>X</sub>)((Y-E[Y])/σ<sub>Y</sub>)] = cov(X,Y)/(σ<sub>X</sub>σ<sub>Y</sub>).  This standardizes the deviations of X and Y, and compares the standardized deviations.  Whereas the covariance has units of X*Y, the correlation coefficient is unitless. Note it is not defined if one of the random variables has 0 standard deviation (it would be a constant random variable) since this would require division by 0.  It turns out -1≤ρ≤1, giving us an absolute scale telling us how associated X and Y are.  If ρ=0 then we say X and Y are uncorrelated.</li>
        <ul>
            <li><strong>Independent X, Y</strong> Similar for covariance independent X,Y have ρ=0.  This is easily seen by the definition of ρ, but the same logic applies where, for every slice of probability at Y=y, the standardized deviations of X should all cancel each other out and have expected value 0, just as they do for the marginal distribution of X.  So, over all slices of Y, you get 0 and thus ρ=0, meaning that X doesn't vary from the mean in a way that is dependent on Y.  (As it shouldn't since they are supposed to be independent)</li>
            <li><strong>X and Y linearly related</strong> For Y=X, ρ=1 and for Y=-X ρ=-1.  For Y=X, the numerator of the correlation coefficient will be var(X) and so will the denominator.  For Y=-X, the cov(X,-X)=E[(X-E[X])(-X-E[-X])]=-E[(X-E[X])(X-E[X])]=-var(X), so the correlation coefficient will be -1.  In general if X-E[X]=c(Y-E[Y]), then ρ=1 if c is positive, and ρ=-1 if c is negative.  This can be seen by substituting in the equation for ρ, so that it is all in terms of X and c, and the c can be pulled outside of expectations, and will result in abs(c)/c * ρ(X,X). It can also be shown that if ρ=1 or ρ=-1, then it must be the case that X and Y are linearly related, and X-E[X] = c(Y-E[Y])</li>
            <li><strong>Linear functions of random variables</strong> Since the standard deviation of aX = sqrt(E[aX-E[aX])^2]) = sqrt(E[(a(X-E[X]))^2]) = sqrt(a^2 * E[(X-E[X])^2]) = abs(a) * σ<sub>X</sub>. We have ρ(aX+b, Y) = a*cov(X,Y) / abs(a) σ<sub>X</sub>σ<sub>Y</sub> = sign of a * ρ(X,Y).  This means, in the example that Y=X, then all linear functions of X, will have ρ=1 or -1, depending on the sign of a.  Consider that a linear function of X will have the same probability distribution as X, except the mean will be shifted by b, and then the distribution will be scaled by a, getting streched out if a>1, and squeezed in if a&lt1.  Since ρ looks at the standardized deviations from the mean, the shifting of the mean by b makes no difference since your are looking only at distances from the mean, and the scaling of the distribution by a also makes no difference since the distances get standardized, and the standard will strech or shrink according to a, just as the distances do.  Thus all that matters is the sign of a.  This also means if X and Y are different, and we replace X with a linear function of X (e.g. changing units of X from degrees F to C), then this doesn't change the correlation coefficient with Y (perhaps day of year), unless a is negative.</li>

        </ul>
    </ul>

    <h2>Random Processes</h2>
    <ul>
        <li><strong>Random Processes</strong> AKA Stochastic Processes are models for random processes that evolve through time.  They capture the evolution of random phenomena over time.  We can use discrete time or continuous time.  You can think of a random process as a sequence of random variables X<sub>1</sub>, X<sub>2</sub>... for a complete description of the random process you'll need to know the joint probability distribution of every possible collection/subset of these random variables.  e.g. you need to know the distribution of each X<sub>i</sub>, and every combination of them like X<sub>2</sub>, X<sub>5</sub>, and X<sub>8</sub>.  Another way to think of a random process is to think of it as one single experiment, e.g. if you flip a head infinite times you could think of a sample space of outcomes that are all possible infinite sequences of 0's and 1's.  For this view, you need to have the probability distribution of this sample space.</li>
        <li><strong>Bernoulli Process</strong> The simplest, discrete time random process.  It is a sequence of independent bernoulli trials each with the same probability of success.  A bernoulli trial is something like a coin flip with 2 possible outcomes, usually called success and failure, with a probability p of success and 1-p of failure on each trial. Usually success is given a numerical value of 1, and failure is 0.  For any given trial, X<sub>t</sub>, the expected value is p, and the variance is p(1-p). (See discussion above for binomial distribution).</li>
        <ul>
            <li><strong>Examples</strong> Used to model getting a lottery ticket every week with same probability of winning each week. A financial market with random chance of going up or down each day or other time period.  Arrival of people at a business, where every second you either have a person arrive or not, or for arrivals of jobs to a server, where a job arrives every second or not.  In general for arrivals, you pick a time interval like seconds and succeed if there is an arrival in the interval or failure if not.  Be careful with arrivals, the bernoulli process assumes the same probability in each time interval, so if you are modelling a deli, it doesn't work over the whole day because people are more likely to arrivve at lunch time.</li>
            <li><strong>Infinite sequences</strong> For an infinite sequence of bernoulli trials, you have a sample space of all infinite sequences of 0's and 1's.  This sample space has an infinite number of members, and as it turns out the probability of any single sequence is 0, similar to a single value of a continuous random variable.  This is because the probability of each sequence is the product of the probability of the outcome of each trial, so it is an infitie product of p's and (1-p)'s.  i.e. p^k * (1-p)^j, and as k and j go to infinity, the limit of this product is 0.  So, infinite time trials can get interesting, although they are still discrete bernoulli processes.</li>
            <li><strong>Successes per time period</strong> One question about bernoulli processes is, how many successes/arrivals will occur over a certain number of trials/certain interval of time.  This is given by the binomial distribution, discussed above, where the random variable Y represents the number of successes/arrivals over n trials/time intervals, and the probability of k successes/arrivals is p<sub>Y</sub>(k) = (n k) p<sup>k</sup>(1-p)<sup>n-k</sup>, where k is between 0 and n.  E[Y]=np, and var(Y)=np(1-p).</li>
            <li><strong>Interarrival Time</strong> The other big question about bernoulli processes is, how much time/how many trials will it take to get a given number of successes/arrivals.  We call this the interarrival time.  The first interarrival time T<sub>1</sub> is the number of trials until the first success (AKA arrival), the second interarrival time T<sub>2</sub> is the number of trials until the second success AFTER the first success, so the second success occurs on the T<sub>1</sub> + T<sub>2</sub> trial.  Likewise, T<sub>3</sub> is the trials after the second arrival that third arrival occurs, and so on.</li>
                <ul>
                    <li><strong>First Interarrival Time</strong> You can define a random variable T<sub>1</sub> that is the number of trials t in the bernoulli process until the first success occurs.  So, P(T<sub>1</sub>=t) = (1-p)<sup>t-1</sup>*p.  (Since you have t-1 failures with probability 1-p and 1 success with probability p).  This forms a geometric distribution, as discussed above with expected value 1/p, and variance (1-p)/p<sup>2</sup></li>
                    <li><strong>Length of first string of failures</strong> A related problem is, in a bernoulli process, you will have a first failure, which could be the first trial or some later trial if the first and possibly subsequent trials are a success.  Once you hit that first failure, you want to know how many failures in a row (L trials) will you have until you have another success?  You might have only one failure and then the very next trial will be a success, or you could have many failures before you have another success.  Consider the tree diagram where you have had all successes and now you have hit your first failure.  You know that is a failure and the string of failures is at least L=1.  The next trial after that has a chance p of being a success and 1-p of being a failure.  If it is a success, then L=1, if it is a failure, then you know L is at least 2.  So the probability of L=1 is p.  Suppose however the next trial is a failure so L is now at least 2, if the trial after that is a success, then L=2, but if it is a failure, then L is at least 3.  So, the probability of L=2 is (1-p)*p, because after the first failure, you had another failure with probability (1-p) and then a success with probability p.  Continuing on like this, you can see that L=1 has probability p, L=2 has probability (1-p)*p, L=3 has probability (1-p)^2*p, and so on.  So the value of L is given by a plain old geometric distribution with probability of success p.</li>
                    <li><strong>Kth Arrival Time (The Pascal Distribution)</strong> Define the trial that the kth success/arrival occurs as Y<sub>k</sub> = T<sub>1</sub> + T<sub>2</sub> + ... + T<sub>k</sub>.  We are interested in the distribution of Y<sub>k</sub>. Well, each T<sub>i</sub> is an independent random variable with a geometric distribution with probability of success p.  That is, prior T's have no effect on future T's, and each time you get a success, you start the next T, which has a probability of p of being a success on the next trial, (1-p)*p of being a success on the trial after that, and so on geometrically.  So, we want the PMF of Y<sub>k</sub>, i.e. what is the probability that the kth arrival will happen on the tth trial.  If the kth arrival happens on the tth trial, that means that k-1 arrivals must have happened in the first t-1 trials, and that trial t is an arrival (the kth arrival).  Well, what is the probability of getting k-1 arrivals in t-1 trials?  This is a basic binomial distribution equal to (t-1 k-1) p<sup>k-1</sup>(1-p)<sup>(t-k)</sup>, where the exponent of (1-p) is a simplification of (t-1)-(k-1).  Also, for the binomial distribution and really the whole question to make sense, we must have 0≤k-1≤t-1, meaning 1≤k≤t.  Now, the probability of the tth trial being an arrival given the first t-1 trials had k-1 arrivals is p.  So, P(Y<sub>k</sub>=t) = P(k-1 arrivals in t-1 trials)*P(arrival on trial t|k-1 arrivals in t-1 trials) = (t-1 k-1) p<sup>k-1</sup>(1-p)<sup>(t-k)</sup> * p = (t-1 k-1) p<sup>k</sup>(1-p)<sup>(t-k)</sup>, with 1≤k≤t.  This is called the Pascal PMF.  Notice that P(arrival on trial t|k-1 arrivals in t-1 trials) = P(arrival on trial t) since the two events are independent (imagine the subset of the sample space.  Lastly, we want to know the expected value and variance of this distribution, but this is actually fairly easy going back to the fact that Y<sub>k</sub> is the sum of independent random variables T<sub>i</sub>, so it's expected value is the sum of the expected value of each T<sub>i</sub> and likewise the variance is the sum of the variance of each T<sub>i</sub>.  Each T<sub>i</sub> is a geometric random variable with expected value 1/p and variance (1-p)/p<sup>2</sup>. So E[Y<sub>k</sub>]=k/p, and var(Y<sub>k</sub>)=k(1-p)/p<sup>2</sup>.</li>
                </ul>
            <li><strong>Memorylessness</strong> A bernoulli process is memoryless, which means that what happened in past trials does not impact what happens in future trials.  If you start observing a bernoulli process with success probability p right after some random trial t, and start a new random variable X to record the outcome of each trial starting with trial t+1, then X itself will be a bernoulli process with probability of success p.  (The first trial of X is the t+1 trial of the original bernoulli process).  If you start observing at trial t, but t is not random and instead is chosen somehow based on the outcome of the first t trials (for example you start observing after the first time two successes occur in a row), then the above statement is still true and you could start a new random variable X that is itself a bernoulli process.  This is because past outcomes have no influence on future outcomes.  However, if you start observing based on the outcomes of trials after trial t, then this statement about starting a new random variable X is not true and is not a bernoulli process.  For example, if someone already new the outcome of a future trial somehow, and told you to start observing right before the first two successes in a row, then trial t+1 and t+2 are no longer random, rather they have definite probability of both being successes, so the random variable X that you start is not actually a bernoulli process (perhaps it becomes one after t+1 and t+2, but as a whole it is not because t+1 and t+2 are not actual bernoulli trials with some probability of failure).</li>
            <li><strong>Split Bernoulli Process</strong> Imagine a bernoulli process of arrival of jobs to a main server with probability p of arrival for each trial/time slot.  For each arrival, imagine another bernoulli trial occurs that determines if the job will be sent to server A or server B, with probability q of going to server A and 1-q of going to server B.  Well, the arrival of a job at server A during a given time slot has probability of p*q, and the arrival of a job at server B during a given time  slot has probability of p*(1-q).  The arrival of jobs at servers A and B is a bernoulli process because it is a success/failure event at each trial, with the same probability at each trial of success or failure.  Additionally, to be a bernoulli process each trial must be independent which they are because we assume each arrival to the main server is an independent random variable, and each decision to send to server A or B is also an independent random variable, and a function of two independent random variables is also independent.</li>
            <li><strong>Merged Bernoulli Process</strong> Consider two independent bernoulli processes like one is the arrival of men to a store with probability of arrival p and one is the arrival of women with probability of arrival q, and then a merged process that just counts the arrival of a person regardless of gender.  If a man and a woman arrive during the same time slot, then the merged process just counts that as a single arrival.  The merged process is itself bernoulli, and the probability of no arrival in the merged process is the probability that both no man arrives and no woman arrives, so it is (1-p)(1-q) or 1-q-p+pq.  Thus the probability of an arrival in the merged process is 1-(1-q-p+pq)= p+q-pq.  To be bernoulli each trial of the merged process must be independent, which it is because each one is a function the men arrival random variables and woman arrival random variables.  And if the random variables in that function are independent of the random variables in another merged trials function, then the two merged trial functions are independent.</li>
        </ul>
        <li><strong>Poisson Process</strong> This is the continuous version of the Bernoulli process. It is useful to think of the bernoulli process in terms of arrivals during equal length time slots one after another.  Imagine instead of having time slots where you record whether something has arrived or not, you simply mark the time that each arrival occurs at.  The similarities between the Bernoulli process and poissson process are that the times of arrival for Bernoulli are discretized time, but for Poisson they are continuous time.  For Bernoulli, you have a probability of arrival p per trial, and for Poisson you have probability of λ arrivals per unit time, or λδ probability over δ time as δ approaces 0.  For Bernoulli, the binomial distribution describes the number of successes in n trials, and as you let n go to infinity, you get Poisson distribution which describes the number of successes in a given time period.  For Bernoulli, you have the geometric distribution to describe the number of trials to get a success, and in the limit that n goes to infinity this becomes the Erlang Distribution for the 1st arrival time.  For Bernoulli, you have the Pascal distribution to describe the number of trials until the kth arrival, and for Poisson you have the Erlang distribution to describe the time until the kth arrival.</li>
        <ul>
            <li><strong>Time Homogeneity</strong> We assume that for a given time interval of length τ, the probability of getting k arrivals is a random variable with probability P(k,τ), and that probability is the same no matter when that interval of time occurs.  P(k,τ) itself is going to be a discrete PMF where P(0, τ) has some probability of occuring (no arrivals), P(1, τ) has some probability of occuring (1 arrival), P(2, τ) has some probability of occuring and so on, and the sum over k of P(k,τ) = 1, since the number of arrivals must be some number k greater than or equal to 0.  P(k,τ) may vary depending on how large of an interval τ is chosen.  This is similar to Bernoulli processes in that each time interval is probabilistically identically, i.e. each interval of length τ has the same PMF P(k,τ) as any other time interval of length τ, just as in the bernoulli process each trial/time slot has the same PMF of 1 with probability p and 0 with probability 1-p, and total probability 1.</li>
            <li><strong>Independence</strong> We also assume that any two disjoint time intervals can be given random variables representing the number of arrivals in each time interval, and that these random variables are independent.</li>
            <li><strong>Arrival Rate</strong> If X is a random variable that represents the number of arrivals k during a given interval τ of a poisson process, then X would have a discrete PMF P(k,τ), with respect to k. (Imagine a sample space of all possible arrival times during τ, and then mapping this to the number of arrivals k to get a PMF for different k).  Well, we could take E[X] to get the expected arrivals in τ time.  We want time homogeneity, so if we multiply the interval length by some constant c, we want the expected value to scale by the same constant (double the length of time, expect twice as many arrivals, take 0.65 the length of time, expect 0.65 as many arrivals).  Thus, E[X]/τ must be constant, and we call this constant the arrival rate or intensity, λ, representing the arrivals per unit time.  We can take this further by saying we want that as τ goes to 0, for the distribution P(k,τ) to have less and less probability of k>1, and all probability weight to go to either 0 or 1 arrivals.  As τ goes to 0, we want P(1,τ) to approach some probability p of arrival, and P(0,τ) to approach some probability 1-p of no arrival, with P(k>1,τ) approaching 0.  Well, that means that as τ approaches 0, E[X] = sum over k of k*P(k,τ), approaches 0*P(0,τ)+1*P(1,τ) = P(1,τ), because we want P(k>1,τ) to go to 0.  Thus, as τ goes to 0, E[X]/τ approaches P(1,τ)/τ, and thus P(1,τ) approaches λτ.  Thus the probability of arrival p that we want as τ goes to 0 must be λτ.</li>
            <li><strong>Poisson PMF</strong> For a poisson process, take an interval of time τ, and define a random variable that is the number of successes k during that time interval.  This will be a discrete random variable that can take on values of 0, 1, 2, etc.  We want to figure out the probability distribution, and we call that the Poisson distribution P(k,τ).  To get this, imagine breaking up τ into n tiny intervals of length δ.  So, τ = nδ.  We know in the limit as δ goes to 0, the probability of one arrival during δ is λδ, the probability of no arrival is 1-λδ and there is no probability of 2 or more arrivals.  So, the time period τ can be thought of as a bernoulli process of n trials, where each trial has probability of success p=λδ=λτ/n, in the limit that δ approaches 0 or equivalently as n goes to infinity.  So, τ would have a binomial distribution (n k)(λτ/n)<sup>k</sup>(1-λτ/n)<sup>n-k</sup> as n goes to infinity.  Taking the limit of this as δ goes to 0, or rather as n goes to infinity, this can be show to be P(k,τ) = ((λτ)<sup>k</sup>e<sup>-λτ</sup>)/k! for k=0,1,2,...  If you fix τ, then this formula gives you a PMF that gives us the probability of getting k successes during time interval τ.  We have that for a given τ, the sum of the formula over k equals 1 so it is a valid probability distribution.  The expected value is λτ, which is analagous to np for bernoulli trials if you consider np=n(λτ/n)=λτ, and makes sense considering λ is really defined as expected value per unit time.  Also, the variance coincidentally is also λτ, which is analagous to bernoulli trials variance of np(1-p) if you consider np(1-p)=np-np^2=n(λτ/n)-n(λτ/n)^2=λτ-(λτ)^2/n approaches λτ as n approaches infinity.</li>
            <li><strong>Kth Arrival Time (Erlang Distribution)</strong> We might also want to know when is the expected arrival time t of the kth arrival.  We can define a random variable Y<sub>k</sub> that is the time of the kth arrival, and this arrival can happen at any time t so this is a continuous random variable with some PDF f<sub>Yk</sub>(t).  Since PDF values at a certain point times a small interval equals the probability of getting a value close to that point, we can say as δ goes to zero, f<sub>Yk</sub>(t)*δ = P(t≤Y<sub>k</sub>≤t+δ). For the kth arrival to happen in this little interval, there must have been k-1 arrivals in time t, and then a kth arrival happens in t+δ, in the limit that δ approaches 0.  Well the probability of k-1 arrivals in time t is the Poisson Distribution with time t and k-1 arrivals.  Then the probability of getting the kth arrival in the interval t+δ is λδ.    So,  f<sub>Yk</sub>(t)*δ = P(t≤Y<sub>k</sub>≤t+δ) = ((λt)<sup>k-1</sup>e<sup>-λt</sup>)/(k-1)! * λδ.  You can cancel the δ on both sides to get f<sub>Yk</sub>(t) = λ<sup>k</sup>t<sup>k-1</sup>e<sup>-λt</sup>)/(k-1)!, which is the Erlang Distribution.  You can plot this for different values of k=1,2,3,... and you'll see for k=1, the first arrival time simplifies to λe<sup>-λt</sup>, which is a decreasing exponential function that is very similar to the geometric distribution you get for the 1st arrival time in a bernoulli process, in fact the limit of the geometric distribution as the time slots decrease in size to 0 is the erlang distribution for k=1.  You can also think of the kth arrival time as a sum of random variables Y<sub>k</sub> = T<sub>1</sub>+T<sub>2</sub>+...+T<sub>k</sub>, where each T<sub>i</sub>is the interarrival time, that is the time between arrivals, where T<sub>1</sub> is the time of the first arrival, T<sub>2</sub> is the time after the first arrival that the second arrival occurs, and so on.  Each interarrival time is independent of the others, and each one has a PDF that is the 1st arrival time of a poisson distribution with arrival rate λ. Practically this is helpful when simulating a Poisson process, because you can go one arrival at a time using λte<sup>-λt</sup> as the distribution to determine when each successive arrival should occur.  The expected value of Y<sub>k</sub> with the erlang distribution is E[Y<sub>k</sub>] = k/λ and the variance is var(Y<sub>k</sub>) = k/(λ^2).  This can be shown by doing the integration on the erlang distribution directly, or doing it for Y<sub>1</sub> only and using that Y<sub>k</sub> is the sum of k independent Y<sub>1</sub>, so its expected value and variance is k times those values for Y<sub>1</sub>, which may be a little easier to integrate.</li> 
            <li><strong>Memorylessness</strong> As with bernoulli processes, the poisson process is memoryless, so if you start observing a poisson process at some time t, then what happened before has no impact on what happens next, and what happens next is itself a poisson process, with arrival rate λ.  </li>
            <li><strong>Adding Poisson Random Variables</strong> The addition of two independent poisson random variables is itself poisson.  Consider a random variable X representing the number of arrivals for a poisson process during a given time interval with some arrival rate.  Now consider a random variable Y representing the number of arrials for a poisson process with the same arrival rate, for a different, disjoint time interval perhaps of different length.  If you add X+Y, then this would be the number of arrivals over the two time intervals together, for a poisson process with the given arrival rate.  Well, this must also follow the poisson distribution, just as if you were considering X+Y without knowing it was the sum of two poisson distributions, and you would know that the number of arrivals over some time interval is poisson.</li>
            <li><strong>Merged Poisson Process</strong> The merging of poisson processes is also poisson.  Consider a poisson process that is the random flashes of a red bulb with λ<sub>1</sub> intensity, and an independent process that is the random flashes of a green bulb with λ<sub>2</sub> intensity.  Now consider a person who is color blind who sees the flashes but the color is indistinguishable (the merged process).  Well, for the merged process, consider breaking it up into little intervals δ, and as δ approaches 0, we know the probability of a red flash is λ<sub>1</sub>δ and the probability of a green flash is λ<sub>2</sub>δ. Since the flashes are independent, the probability of both occuring is λ<sub>1</sub>δ*λ<sub>2</sub>δ, the probability of only red flashing is λ<sub>1</sub>δ*(1-λ<sub>2</sub>δ), the probability of only green flashing is λ<sub>2</sub>δ*(1-λ<sub>1</sub>δ), and the probability of neither is (1-λ<sub>1</sub>δ)(1-λ<sub>1</sub>δ).  The sum of all these probabilities must be 1 since one of the four options must occur.  Well, as δ goes to 0, consider that δ^2 goes to 0 even faster and this higher order term becomes negligible.  So, the probability of both flashing goes to 0, the probability of red flashing goes to λ<sub>1</sub>δ, the probability of green flashing goes to λ<sub>2</sub>δ, and the probability of neither goes to 1-λ<sub>1</sub>δ-λ<sub>2</sub>δ.  So, the merged process has a probability of a flash in everly little interval δ of (λ<sub>1</sub>+λ<sub>2</sub>)δ - thus fits the poisson assumption of an arrival rate.  Additionally to show the merged process is poisson, we have each disjoint interval in the merged process is independent since each dsijoint interval within the red process is independent and each disjoint interal within the green process is independent, and finally equal length intervals in the merged process have the same distribution again because this is true of both the red and green processes.  Together, this means the merged process is poisson.  Given an arrival in the merged process, the probability that it was the red bulb is λ<sub>1</sub>δ / (λ<sub>1</sub>δ + λ<sub>2</sub>δ), which is the conditional probability of a red flash given a flash occured in the merged process.  You can cancel out the δ's, so the probability that an arrival came from one of the bulbs is the arrival rate of that bulb over the sum of the arrival rates of both bulbs.  You can think of the origin of the bulb flash (red or green) as a coin flip at any given time, where you have a probability of red or green as above, and each coin flip is independent, i.e. no matter the origin of previous flashes, the origin of the next flash has the exact same probability of being red or green.</li>
            <li><strong>Split Poisson Process</strong> The splitting of a poisson process is also poisson.  Consider arrival of jobs to a main server, where arrivals are a poisson process with arrival rate λ.  Say with probability p, you send a job to Server A when it arrives, and with probability 1-p you send a job to Server B when it arrives.  Then the arrival of jobs to Server A in a little interval δ has probability of occuring of pλδ.  The arrival of jobs to Server B in a little interval is (1-p)λδ.  You can also think through that on Server A and Server B, each disjoint interval is independent and has the same probability distribution.  So, Server A and B are both poisson processes, with arrival rates pλ and (1-p)λ respectively.</li>
            <li><strong>Random Incidence</strong> There is a seeming paradox in the poisson process for measuring interarrival times.  Consider a poisson random variable that represents the arrival times of busses at a particular bus stop, and suppose it has arrival rate of 4 per hour.  Thus, after a bus arrives, you would expect a new poisson process to start due to memorylessness, and the time of arrival of the next bus would have an erlang distribution with expected value 1/λ or 1/4hrs=15 minutes.  So the expected value of interarrival times would be 15 minutes.  However, now consider that you show up at the bus stop at a random moment in time, then the probability from that moment to the next arrival is again poisson and is completely independent of the past (doesn't matter when the last bus came), and the expected arrival time of the next bus is going to be the expected value of the first arrival time of a poisson process with arrival rate 4 per hour, so it is 15 minutes.  Likewise, consider a poisson process running backward in time, it is the same as running it forward in time, as every little interval has the same probability whether you go forward or backward in time, so from the moment you show up at the bus stop, the time since the last bus arrival is also poisson, and the expected value would be that the last bus arrived 15 minutes in the past.  So, from the moment you show up at the bus stop, your total expected value of the time between the previous bus and next bus is 30 minutes.  The reason for the paradox is that the interarrival time is indeed 15 minutes, however that is when you measure the time from the moment of one arrival to the next.  When you show up at a random time, you are more likely to show up during a longer interarrival time than a shorter one, so the expected time between the last and next bus is longer than the actual interarrival time.  Imagine plotting the moment of each arrival on a timeline.  The longer intervals between buses take up more of the length of the timeline than the shorter intervals, so if you pick a time at random, it is more likely to fall in a longer interval than a shorter one.</li>
            <ul>
                <li><strong>Renewal Processes</strong> You can have other types of arrival procesess that model arrivals, the general class is called renewal processes.  The are like poisson where each interval is independent and has the same distribution, but they can have different types of distributions (poisson is a decreasing exponentional on each interval per the erlang distribution for k=1).  For all renewal processes, you have the same random incidence paradox, and the random incidence time is always greater than the actual interarrival time because you are always biased toward picking intervals that are on the longer side if you pick a time at random in the process.  E.g. consider a renewal process where the interarrival time is either 5 or 10 minutes with equal probability.  Then a plot of arrivals on a timeline will have an equal number of 5 minute intervals and 10 minute intervals.  The interarrival time is going to be 5*0.5+10*0.5=7.5 minutes.  However, the 10 minute intervals take up 2/3rds of the timeline and the 5 minute intervals only take up 1/3rd of the timeline, so if you pick a time at random, you have a 2/3 chance of landing in a 10 minute interval and a 1/3 chance of landing in a 5 minute interval.  So the expected value of the length of the interval you land in when picking a random time is going to be 5*1/3 + 10*2/3 = 25/3 = 8.33 minutes.  As another example, consider if you pick a family at random perhaps the average size of the family is 4 people.  However, if you pick a person at random, perhaps the average family size that that person is in is 6.  This isn't necessarily incompatible because when you pick a person at random you are more likely to pick a person from a larger family.  Another extreme example, suppose there are 9 buses with no riders on them, and 1 bus with 10 riders on it.  Well the average crowdedness of the busses is 0*9/10+10*1/10=1 person per bus.  However, if you pick a person at random and ask how crowded their bus is, then the expected value is 0*0/10+10*10/10=100/10=10 people per bus.  </li>
            </ul>
            <li><strong>Examples</strong> Things where a 'success' or 'arrival' occurs at random times, ar an overall regular rate are modelled well as Poisson process.  So, radioactive decay where a particle is emitted at random times, or where a weak light source emits a photon at random times, or the arrival of emails to your inbox, or the occurence of a car accident in  a city.  Note, for the last two examples, you might have to restrict it to certain times of day, like between 1 and 2pm the arrival of emails has a constant arrival rate, but perhaps over the course of a whole day the arrival rate changes.</li>
                <ul>
                    <li><strong>Email arrivals</strong> Say you get emails at an arrival rate of λ=5 per hour. Then the expected value for an interval of time τ=1/2 hour is λτ = 5*1/2 = 2.5 emails.  The probability of getting 0 emails in half an hour is P(k,τ)=P(0, 1/2)=((5*1/2)<sup>0</sup>e<sup>-5*1/2</sup>)/0! = e<sup>-2.5</sup> = 0.08.  Similarly the probability of 1 email in half an hour is P(1, 1/2)=((5*1/2)<sup>1</sup>e<sup>-5*1/2</sup>)/1! = 2.5e<sup>-2.5</sup> = 0.2.</li>
                    <li><strong>Fishing</strong> Imagine catching a fish is a poisson process with arrival rate 0.6 per hour.  Suppose you go home after two hours of fishing if you catch at least one fish during that time, but if you don't catch a fish you keep fishing after two hours until you do and then go home.  Let P(k,t) be the poisson distribution and f<sub>Yk</sub>(t) be the erlang distribution for the problem.  The probability of fishing more than two hours is the probability of catch 0 fish in the first two hours, which is P(0, 2) or integral from 2 to infinity of f<sub>Y1</sub>(t)dt.  The probability of fishing more than 2 but less than 5 hours is P(0,2)(1-P(0,3)) or integral from 2 to 5 of f<sub>Y1</sub>(t)dt. The probability of catching at least two fish is 1-P(0,2)-P(1,2) or sum over k from 2 to infinity of P(k,2), or integral from 0 to 2 of f<sub>Y2</sub>(t)dt.  The E[future fishing time|already fished for 3 hours] is simply E[Y<sub>1</sub>] = 1/λ because of memoryless it doesn't matter that you are starting at 3 hours, you have a fresh poisson process starting at 3 hours waiting for the arrival of the 1st fish.  For E[total time fishing], define a random variable F that is the total fishing time, and a random variable R that is the total fishing time over 2 hours.  For all outcomes in the sample space, F is equal to 2 or more hours, in fact F=2+R.  So, E[F]=E[2+R]=2+E[R], so our problem is to find E[R].  For this we break up the sample space into two disjoint and exhaustive events, A) that of fishing for exactly two hours, and B) that of fishing for more than two hours.  In the event that you fish for exactly two hours, E[R]=0, and in the event that you fish for more than two hours, E[R]=1/λ because this is simply the expected value of the Poisson time until you catch a fish and can stop fishing.  Using total expectation theorem, E[R] = P(A)E[R|A] + P(B)E[R|B] = (1-P(0,2))*0 + P(0,2)*1/λ = P(0,2)/λ, so E[F]=2+P(0,2)/λ. Finally, for the expected value of total fish caught, define a random variable F that is the number of fish caught in the first 2 hours and a random variable S that is the number of fish caught after 2 hours.  Every outcome in the sample space has at least two hours of fishing, and the value of F is discrete with a poisson distribution P(f,2) and thus E[F]=2λ. If F is 1 or more, then S=0, and if F=0, then S=1.  F and S are obviously dependent variables (the distribution of S changes with different values of F), but you can still define the marginal probability distribution for S which is 0 with probability 1-P(0,2) and 1 with probability P(0,2), and thus E[S]=0*(1-P(0,2)) + 1*(P(0,2))=P(0,2).  Finally, define a random variable T=F+S, which for every outcome in the sample space would give you the total fish caught in that outcome.  Even though F and S are dependent, since T is a linear function of them, expected value behaves such that E[T]=E[F+S]=E[F]+E[S]=2λ+P(0,2).</li>
                    <li><strong>Bulb burnouts</strong> Suppose you have three bulbs and you represent the time that they burn out as random variables X, Y, and Z.  Each burnout is modelled as the first arrival in a poisson process with arrival rate λ.  Say you are interested in the expected time that last bulb will burn out.  Well, at first you have a merged poisson process of X, Y, and Z, with arrival rate 3λ.  The expected value of the first burnout is the expected value of the first arrival in the merged process which is 1/3λ.  After the first burnout, you have a merged process with only two bulbs, so the arrival rate is 2λ and the expected value of the time of the first arrival in this process is 1/2λ. Finally, you know have a single bulb remaining and the time of the first arrival in this process is just the single bulb poisson process with arrival rate λ and expected value of the time of first arrival is 1/λ.  So, the expected time that the last bulb burns out is 1/3λ+1/2λ+1/λ.</li>
                </ul>
        </ul>
        <li><strong>Markov Processes</strong> Or Markov Chain is a general class of random processes that describe the evolution of a system in the presence of some noise.  It is not memoryless like Bernoulli and Poisson, but rather, the new state of the system is a function of the old state plus some noise. Pretty much any random process can be modelled by a Markov Process with the correct state construction. The process can either be discrete time slots or continuous time, and it has a state variable that can change with each transition to a value in a state space with some probability that depends only on the current state.  Generally, problems consist of you are given an initial state and you want to know the probability of what the state will be after so many steps in the future.  To make a Markov Process model, you must define the state variable with all possible states, then define the possible transitions between state values at each step, and then define probabilities of each transition.</li>
        <ul>
            <li><strong>State</strong> We say the process starts with an initial given or random state X<sub>0</sub> and after n steps (aka transitions), the state is X<sub>n</sub>.  The state can take on any value in the state space, the set of possible state values.  The simplest state space would be a finite, discrete set of values, but it could be infinite and/or continuous.  We should be able to quantify the probability if the state is a particular state value i, what is the probability that the next state value will be any other state value j.  We define the 1-step transition probability p<sub>ij</sub> = P(X<sub>n+1</sub>=j|X<sub>n</sub>=i) for every combination of values i and j in the state space.  We also require what is called the Markov property, which is that p<sub>ij</sub> is always the same for a given i and j in the state space no matter what step you are on, so that regardless of what happened in the past to get to state i, you have the same probability of getting to state j in the next step.  Said another way, all you need to know is the current state i to predict the future state j, you don't need to know anything about how you got to the current state i.  Markov chains are time-invariant or time-homogeneous in this sense. This is stated mathematically as p<sub>ij</sub> = P(X<sub>n+1</sub>=j|X<sub>n</sub>=i, X<sub>n-1</sub>, ..., X<sub>0</sub>), meaning given the whole history of how you got to the current state i, you still just have a single probability of moving to state j next.  For this to be true, the state must capture all the relevant information that determines the next state, or else you'll have information that could influence the next state j that could be differ every time you have state i.</li>
            <ul>
                <li><strong>Recurrent/Transient States</strong> A recurrent state value is one for which if you start at that state and transition to another state, no matter what happens after that transition, there is always a chance of getting back to the initial state.  A transient state is the opposite, one for which if you start at that state there is a probability of transitioning over one or more steps to a state from which there is not chance of getting back to the initial transient state.  After many steps, the probability that the state will have a transient value goes to 0, because after infinite steps, you will eventually leave the transient state and be unable to return to it.  A recurrence class of states is a set of states where you can start at any state in the class and you'll be able to get to every other state in the class, this can lead to getting stuck in a recurrence class if there are states that are outside the class - once you're in the class there's no getting out.  If a markov process has multiple recurrence classes then you can analyze them independently, finding the steady state probabilities for the states in each recurrence class.</li>
                <li><strong>Periodic States</strong> The states in a recurrent class are periodic if you can divide the class up into two or more groups of states, and on every step you have probability of 1 of the state transitioning to a state in a different group.  If there is a state that has a positive probability of transitioning to itself, then the markov process is definitely not periodic because you'll have to put that state in one group or another and there is a chance the next step will stay on that state and thus in that group.</li>
            </ul>
            <li><strong>n-step Transition Probabilities</strong> The probability of being in state j after n steps given an initial state i is denoted as r<sub>ij</sub>(n) = P(X<sub>n</sub>=j|X<sub>0</sub>=i).  We always have r<sub>ij</sub>(0) = {1 if i=j, 0 if i≠j and r<sub>ij</sub>(1) = p<sub>ij</sub>, the 1-step transition probability defined above.  If you imagine the tree of all possible state transitions over n steps starting with step 0 is i, then the leaves of this tree are the possible states at step n, and the probability of each leaf is a product of all the single step transition probabilities it took to get there.  To find the probability that step n will have state j, you would sum up the probabilities of all the branches that end with state j. Each term of this sum is a product of single step transition probabilities, and each of these products ends with the factor p<sub>kj</sub> where k is state at step n-1 and j is the state at step n.  Group together terms with the same value k, and then factor out p<sub>kj</sub> from each group.  For each k, you will get p<sub>kj</sub> times a sum of products where each product equals the probability of getting a branch that ends in k at step n-1.  In fact, this sum contains the probability of every branch that ends in k at step n-1 and equals r<sub>ik</sub>(n-1).  So for each k, you get the term r<sub>ik</sub>(n-1)*p<sub>kj</sub>.  So, the key recursive result here is that for each j, r<sub>ij</sub>(n) = sum over k of r<sub>ik</sub>(n-1)*p<sub>kj</sub>.</li>
            <ul>
                <li><strong>Variations</strong> By similar logic, imagine instead of grouping by the value of the state at step n-1 and factoring out the last single step transition, you group differently.  You still create a sum of all probabilities ending in state j, but now you group the terms by the value of the first factor in each term, which is the single step probability of going from i at step 0 to k at step 1.  Now, factor out p<sub>ik</sub> for each group, and the remaining sum is the probability of going from state k at step 1 to state j at step n.  This is simply r<sub>kj</sub>(n-1), so for every j, you get r<sub>ij</sub>(n) = sum over k of p<sub>ik</sub>*r<sub>kj</sub>(n-1). Another variation is if the initial state is random, then P(X<sub>n</sub>=j) = sum over i of P(X<sub>0</sub>=i)*r<sub>ij</sub>(n).  If you imagine the tree here, then the first branching is choosing the initial state value, then for each initial state you have tree of n transitions starting with that initial state.  So, adding up all the branches that end in j, you can group the terms by their initial state and factor out the first branch probability of choosing the initial state, so you end up with P(X<sub>0</sub>=i)*r<sub>ij</sub>(n) for each initial state i, and you sum those up to get the total probability of all branches that end in j.</li>
            </ul>
            <li><strong>Steady State Convergence Theorem</strong> The most important theorem for markov chains is that for certain types of markov processes, the probability r<sub>ij</sub>(n) approaches (or actually reaches) a constant value π<sub>j</sub> for each j, and it is independent of the initial state i.  We call this convergence to a steady-state probability.  Stated another way, in the limit that n approaches infinity, P(X<sub>n</sub>=j|X<sub>0</sub>=i) = P(X<sub>n</sub>=j) = π<sub>j</sub> for every i and  j. This implies that in the limit n goes to infintiy, X<sub>n</sub> and X<sub>0</sub> are independent random variables (the conditional probability of X<sub>n</sub>'s value is the same as the unconditioned probability). The theorem is that this happens if and and only if the markov chain has no more than one recurrence class, and the recurrence class is not periodic.  If it is periodic then the state oscillates between values and never approaches a single value. e.g. imagine 3 states, where the first state has an equal chance of transitioning to one of the other two states, but the other two states always transition back to the first state, looking at the first state value it oscillates between probability 1 and 0 with each step and thus never converges to a single value.  If it has 2 or more recurrence classes, then it will matter which if you start in one of the recurrence classes because you will be stuck in that recurrence class, so the probabilities of different state values are very different depending on the initial state.  Intuitively, imagine two copies of the markov process with different intitial states, eventually there will be a step at which these two markov processes have the same state, if there is only one recurrence class and no periodicity, and at that step, the two processes now have identical probability trees determining future outcomes, and thus the initial state has no influence after that point.</li>
            <ul>
                <li><strong>Balance Equations</strong> The steady state probability π<sub>j</sub> for each state j is the limit as n approaches infinity of r<sub>ij</sub>(n) = limit as n approaches infinity of sum over k of r<sub>ik</sub>(n-1)*p<sub>kj</sub> = sum over k of limit as n approaches infinity of r<sub>ik</sub>(n-1)*p<sub>kj</sub> = sum over k of π<sub>k</sub>*p<sub>kj</sub>.  This gives you a system of equations, 1 for each π<sub>j</sub>, which is given in terms of a sum of every π<sub>k</sub> with p<sub>kj</sub> coefficients.  This system of equations has a unique solution if you also add in that the sum over all j of π<sub>j</sub> = 1, capturing that the steady state probabilities of each state value must sum up to 1.</li>
                <li><strong>Frequency interpretation</strong> It turns out it is accurate to think of the steady state probability of each state as the frequency with which the markov process is in each state over many transitions. Consider all the transitions into a state j from other states k, after many transitions have occured.  Consider out of all the transitions that can occur after you are many steps into the process, the frequency of transitions which are into state j.  The balance equations tell us that π<sub>j</sub> = sum over k of π<sub>k</sub>*p<sub>kj</sub>, which you can interpret as the sum of the frequencies of each transition into state j.  Each state k has a probability π<sub>k</sub> of occuring when you are many steps into the process, and at that particular step of the process, you must be in one of those possible states k.  Then from a given state k, there is a probability p<sub>kj</sub> of transitioning to  state j.  So you can think of the frequency of this particular transition out of all the transitions that could happen in the next step, as π<sub>k</sub>*p<sub>kj</sub>.  The total frequency of all transitions into j will be the sum over all k's that transition into j. Since you are only in state j if there was a transition into j, the frequency with which the process is in state j is the frequency of all the possible transitions that take you to state j.</li>
                <li><strong>Time scales </strong> You can often approximate probabilities using steady state probabiliites, for example you might want to know, given that the initial state X<sub>0</sub>=1, what is P(X<sub>100</sub>=1 and X<sub>101</sub>=2)?  Well, this is really asking P(X<sub>100</sub>=1|X<sub>0</sub>=1)*P(X<sub>101</sub>=2|X<sub>100</sub>=1, X<sub>0</sub>=1).  The last condition can omit X<sub>0</sub>=1 because only the current state 100 will determine the probability of the next state 101.  States before 100 have no bearing on that.  Well, the first factor could be approximated by π<sub>1</sub> the steady state probability of 1, since 100 steps is a lot, and the second factor would be p<sub>12</sub>, the single step transition probability of going from 1 to 2 in step 100 to 101.  This type of approximation is dependent on the so called time scale of the markov process.  It would be valid if the transitions between states 1 and 2 had single step transition probabilities between maybe .1 and .9, because on average that means it might take up to 10 steps for a transition to occur from state 1 to 2, or back from state 2 to 1, so over 100 steps, you would have a fair amount of back and forth and perhaps the steady state will have been reached.  However, if you had single step transition probabilities like 0.001 and 0.999, then it would take on average 1000 steps for a transition from state 1 to state 2, or from 2 to 1.  Thus, the steady state won't be reached until maybe 10,000 steps, so you can't use it to approximate a value after 100 steps, and the initial state will still have a lot of influence at that point.</li>
            </ul>
            <li><strong>Absorption Probabilities</strong> If you have a markov process with 2 or more recurrence classes, then once you transition into one of those classes you will not get out, which is called absorption.  You may want to know what is the probability of ending up in each of the recurrence classes.  This depends on the initial state i, for instance if i is already in one of the recurrence classes then all future steps have probability 1 of being in that recurrence class and probability 0 of being in any other recurrence class.  Also, if there are transient states and the initial state is one of them, then if you start in a transient state with high probability of transitioning into a state of a particular recurrence class, then you are more likely to end up in that recurrence class then one that perhaps requires you to transition through many other transient states before having a chance of transitioning to that recurrence class.</li>
            <ul>
                <li><strong>Probability of Absorption</strong> Consider an initial state i that is transient, and can only transition to other transient states.  Define a<sub>i</sub> as the probability of ending up in a particular recurrence class of interest given initial state i (you would have an a<sub>i</sub>s for each recurrence class).  Well if state i can transition to state j, then one way you could eventually reach the recurrence class is transitioning from i to j in one step, with probability p<sub>ij</sub> the single step transition probability, and then from j eventually transitioning into the recurrence class which has probability a<sub>j</sub> by definition, the probability of ending up in the recurrence class if you start at state j.  If you consider every state j that i can transition into, then the probability a<sub>i</sub> is equal to the sum over j of p<sub>ij</sub>a<sub>j</sub>.  This formula is true if i is a transient state that can only transition to other transient state, but is also true if i can transition directly to one or more states in the recurrence class, because those transitions will each have a probability p<sub>ij</sub> of occuring, and a<sub>j</sub> for a state in the recurrence class is 1 since you are guaranteed to be in that recurrence class now for all future states.  Furthermore, the formula is even true for an initial state in the recurrence class, somewhat trivially, because all transitions from i to another state j will be to states inside the recurrence class, so they will all have a<sub>j</sub>=1, and the sum in the formula just adds up the probability of all the transtions out of i which must equal 1, so you get a<sub>i</sub>=1 which makes sense considering you are starting in a recurrence class.  You can go through each i and write down this formula for a<sub>i</sub> and you'll end up with i equations with i unknowns, and it can be show this always has a unique solution, so you can get the probability a<sub>i</sub> of ending up in a given recurrence class for each initial state i.  You would do this for each recurrence class in the markov process.</li>
                <li><strong>Time until Absorption</strong> You may also want to know, what is the average number of steps it will take until the process is absorbed by one of the recurrence classes.  This also depends on the initial state for the same reasons the probability of absorption does above.  Well, define μ<sub>i</sub> as the expected value of the number of steps until absorption by one of the recurrence classes, given the initial state is i.  From i, you will make 1 transition to some state j (which could be i itself if that has nonzero probability) with probability p<sub>ij</sub>, and then from state j, there is an expected value of the number of steps until absorption μ<sub>j</sub>.  Well, by total expectation theorem reasoning, the expected value of steps until absorption after transitioning from i should be sum over j of p<sub>ij</sub>μ<sub>j</sub>.  Just remember that this is the expected value after you make the transition from i, so the total steps from i would be 1 plus that value, so μ<sub>i</sub> = 1 + sum over j of p<sub>ij</sub>μ<sub>j</sub>.  This works if i is a transient state, if i can directly transition to a recurrence class, but not if i is already in a recurrence class, because μ<sub>i</sub> equals 0 for all i in a recurrence class, since no steps are needed to be inside a recurrence class if you're already inside one.  Creating this formula for each i, you will get a system of i equations in i unkowns which can be shown to always have a unique solution.</li>
                <li><strong>Mean First Passage Time</strong> A related problem to absorption time, is in a markov process that is just a single recurrence class, and you want to know what is the expected value of steps t<sub>i</sub> until I get some state s, given that the initial state is i.  In this case, getting to state s doesn't mean there is an absorption, but it is similar in that getting to state s from i means i makes one transition to a state j with probability p<sub>ij</sub> and then state j has some expected value of steps until s, t<sub>j</sub>.  So again by total expectations theorem, the expected value of steps from i to s, t<sub>i</sub> = 1 + sum over j of p<sub>ij</sub>t<sub>j</sub>.  This is true for all i, except if i is s, then t<sub>s</sub>=0.  This gives you a system of i equations in i unkowns, and again this can be shown to have a unique solution.</li>
                <li><strong>Mean Recurrence Time</strong> Yet another related problem, however again with a process that is just a single recurrence class, you want to know if the initial state is s, what is the expected number of steps t<sub>s</sub> until I arrive back at state s again?  Again in this case there is no absorption or multiple recurrence classes, but it is a similar calculation because you know from the initial state s you will take 1 step to some state j, then from j, there is an expected value of steps to get to state s t<sub>j</sub>.  So, again by total expectation theorem, the expected number of steps to get state s again will be t<sub>s</sub> = 1 + sum over j of p<sub>sj</sub>t<sub>j</sub>.  You would then apply the mean first passage time to get a system of equations that can be shown to have a unique solution.</li>
            </ul>
            <li><strong>Examples</strong></li>
            <ul>
                <li><strong>Birth Death Process</strong> TThe state space is 0, 1, 2, ... and may be infinite or may be finite up to an integer m, and transitions can occur on each step that either increment the state up by 1, decrement it by 1, or keeps it the same.  Of course, if the state is 0 it is not possible to decrement, and if there is a maximum state value m then if the state is at the maximum it cannot increment.  The probability of a state i transitioning up to state i+1 is given by p<sub>i</sub>, the probability of the state i transitioning down to state i-1 is given by q<sub>i</sub>, and the probability of the state staying the same is given by the remaining 1-p<sub>i</sub>-q<sub>i</sub>.  The Birth-Death process can track population size as the state with the possibility of a birth incrementing the population upward or a death incrementing it downward, or one of each or neither keeping the population the same.  It could also model jobs in a queue for a server, with jobs arriving and increasing the queue, jobs being completed and leaving the queue.  It could model total active telephone calls in some area, with some probability of a new call occuring and some probability of a current call ending.  It could model number of sick people in a disease outbreak, with some probability of another person contracting the disease and some probability of a sick person getting better (here the probability of getting the disease obviously changes depending on the number of people who have it, the more who have it, the more likely it infects a person who doesn't).  When looking for the steady state probability π<sub>i</sub> of each state, consider that over many transitions, the number of transitions upward from i to i+1 must equal the number of transition downward from i+1 to i.  They could be off by 1, but consider if the transition from i to i+1 has occured, then it cannot occur again until the transition from i+1 to i occurs, and likewise once the transition from i+1 to i occurs, it cannot occur again until the transition from i to i+1 occurs.  So, these transitions from i to i+1 must occur just as frequently as transition from i+1 to i.  So, using the frequency interpretation of balance equations, you can say that the transitions from i to i+1 occur with a frequency of π<sub>i</sub>*p<sub>i</sub>, because for this transition to occur you have to be in state i which has a frequency of π<sub>i</sub> of being the state, and then there is a probability of p<sub>i</sub> of the i to i+1 transition occuring.  Likewise, the transition from i+1 to i occurs with a frequency of π<sub>i+1</sub>*q<sub>i+1</sub>.  By our observation that these two transitions occur equally frequently, we have that π<sub>i</sub>*p<sub>i</sub>=π<sub>i+1</sub>*q<sub>i+1</sub>.  Together with the requirement that sum over i of π<sub>i</sub>=1 to be a valid probability distribution, we get a system of equations that has a unique solution. (Fix π<sub>0</sub> as a symbol, so you can then express π<sub>1</sub> in terms of π<sub>0</sub>, which allows you to express π<sub>2</sub> in terms of π<sub>0</sub> and so on, then use the normalization condition to get a big sum of π<sub>0</sub> terms that must equal 1 and solve for π<sub>0</sub>, allowing you to then solve for all π<sub>i</sub>) </li>
                <li><strong>Checkout Counter Process</strong> A very useful model for a discrete-time, finite-state Markov Process is a checkout counter at a store, where customer are queued in a line to receive a service.  It could also represent arrival of jobs in a server queue and other things.  This is a special case of the birth-death process where the probability of incrementing up is always the same and the probability of decrementing down is always the same.  We can define the load factor ρ = p/q, if ρ=1 then the probability of incrementing up or down is the same so the state tends to stay in one place, but if ρ>1 then there is higher probability of incrementing up and the state tends to increase or if ρ&lt1 then there is a higher probability of decrementing down and the state tends to decrease.  The balance equations for the birth-death process give π<sub>i</sub>*p=π<sub>i+1</sub>*q, so π<sub>i+1</sub>=π<sub>i</sub>*ρ, and thus π<sub>1</sub>=π<sub>0</sub>*ρ, π<sub>2</sub>=π<sub>0</sub>*ρ<sup>2</sup>, and in general π<sub>i</sub>=π<sub>0</sub>*ρ<sup>i</sup>.  Using the normalization condition, we have that sum over i of π<sub>i</sub> = sum over i of π<sub>0</sub>*ρ<sup>i</sup> = 1, so π<sub>0</sub> = 1 / sum over i of ρ<sup>i</sup>, which allows you to solve for π<sub>0</sub> and plug that in to solve for all π<sub>i</sub>.</li>
                <li><strong>Symmetric Random Walk</strong> This is a special case of the Checkout Counter Process where the load factor ρ=1.  In this case, the balance equation gives π<sub>i+1</sub>=π<sub>i</sub>. So, the steady state probability of every state is the same, and after many steps you are equally likely to be in any state i.  If you have finitely many states, that is m+1 total states 0, 1, 2, ..., m, then the normalization condition requires π<sub>0</sub> = 1 / sum over i of ρ<sup>i</sup> = 1 / sum over i of 1 = 1 / (m + 1), since ρ=1.  Since all stead state probabilities are the same, we thus have π<sub>i</sub> = 1 / (m + 1), for all i = 0, 1, ..., m.</li>
                <li><strong>A Stable Queue</strong> Another special case of the checkout counter process is when ρ&lt1 and the state space is very large or infinite.  Then, the normalization condition gives π<sub>0</sub> = 1 / sum over i of ρ<sup>i</sup> = 1 / (1/(1-ρ)) = 1-ρ, because the sum in the denominator is an infinite geometric series that converges to 1/1-ρ.  Thus, π<sub>i</sub>=(1-ρ)ρ<sup>i</sup>.  Thus the probability distribution of being in state i over many steps is a gemoetric series start at 0, and decreasing with i.  This makes sense because the load factor is less than 1, so you expect states to decrement more often than increment.  The expected value of this distribution turns out to be ρ/(1-ρ).  The interesting thing is that this blows up to infinity as ρ approaches 1, so ρ less than but close to 1 actually have fairly high expected values (e.g. long queues).</li>
                <li><strong>Phone Lines</strong> This is an example of the birth-death process.  Imagine a town with B phone lines connecting it to the outside world, and every call outside the town requires a dedicated line.  You want to figure out a reasonable number of phone lines the town should have so that you don't waste money overbuilding, but that there is likely always to be an available line every time a call is made out of town.  We assume initiating calls happens according to a poisson process with some arrival rate λ.  We assume the length of a call is distributed as the 1st arrival in a poissson process, with an arrival rate μ.  We model it overall as a Markov process, with the state being the number of busy phone lines and can be 0, 1, 2,...,B.  We assume the steps are very small in time δ, so we can make things work out nicely with the poisson process.  At any given step, you can have the step increment up by 1 because a new call is initiated, with probability λδ.  You can also have the step increment down by 1 because a call ends.  If you only had one active call, then the probability at any moment of it ending is μδ, since this is the probability of an arrival in the distribution for call length and as soon as that arrival happens the call ends and you should decrement the state.  If you have i active calls (you are in state i), then you can think of it as having i independent poisson processes representing probability of each call ending, merged together, and the rate of the merged process is iμδ, so this is the probability of decrementing the state when you are in state i.  You can also have neither a new call or call ending, but since δ is taken to be small, in the limit there is no chance of an arrival and call ending in the same step because this probability would be a δ<sup>2</sup> term and be negligible. Now, the balance equation for this is π<sub>i</sub>*λδ=π<sub>i+1</sub>*(i+1)μδ, or cancelling out δ, π<sub>i</sub>*λ=π<sub>i+1</sub>*(i+1)μ, and thus π<sub>i+1</sub> = π<sub>i</sub>*λ/(i+1)μ. We get, π<sub>1</sub> = π<sub>0</sub>*λ/μ, π<sub>2</sub> = π<sub>1</sub>*λ/2μ = π<sub>0</sub>*λ^2/2μ^2, π<sub>3</sub> = π<sub>2</sub>*λ/3μ = π<sub>0</sub>*λ^3/6μ^3, and in general π<sub>i</sub> = π<sub>0</sub>*λ^i/(i!μ^i).  The normalization condition thus requires 1 = sum over i from 0 to B of π<sub>i</sub> = sum over i from 0 to B of π<sub>0</sub>*λ^i/(i!μ^i), and thus π<sub>0</sub> = 1 / sum over i from 0 to B of λ^i/(i!μ^i).  Now, for the problem at hand, if someone initiates a call at a random time, then if the state of this process is B, that person will be unable to start the call.  So the steady state probability of B represents the frequency of time the system spends in state B over many transitions, and the goal here is to make this frequency reasonably small, e.g. 0.01 so only 1% of calls would be unable to be initiated due to full phone lines.  This is a real-world example that phone company engineers use, where they would have a list of tabulated values of the steady state frequency for different values of B,λ, and μ.</li>
            </ul>
        </ul>
    </ul>

    <h2>Inferences</h2>
    <ul>
        <li><strong>Inference</strong> An inference problem is finding the probability distribution of an unknown random variable given the value of a known random variable.  Making a measurement of something is an inference problem.  The experiment is that something measurable occurs in the real world and you take a measurement of it using some measuring device, which has some error or noise associated with it.  You define a random variable X that represents the actual state of the world, and a random variable Y that represents your measurement result.  You don't know the value of X but you know or have some beliefs about the probability distribution of X, either a discrete PMF p<sub>X</sub>(x) or a continuous PDF f<sub>X</sub>(x) (called the prior distribution).  You do know the value of Y, and you know the conditional probability distribution of Y given X, which amounts to a model of your measuring device, either a discrete conditional PMF p<sub>Y|X</sub>(y|x) or a continuous PDF f<sub>Y|X</sub>(y|x).  The goal of the inference problem is to figure out the conditional probability of X given the value of Y given by your measurement device, called the posterior distribution and is either a discrete conditional PMF p<sub>X|Y</sub>(x|y) or a continuous PDF f<sub>X|Y</sub>(x|y).  To do this, you use some variation of Bayes' Rule, depending on whether the random variables are discrete or continuous.</li>
        <ul>
            <li><strong>X and Y Discrete</strong> To solve the inference problem for the discrete case, we know p<sub>X,Y</sub>(x,y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) = p<sub>Y</sub>(y) * p<sub>X|Y</sub>(x|y).  Think about this in terms of event subsets of the sample space to convince yourself its true.  Thus we can derive that  p<sub>X|Y</sub>(x|y) = p<sub>X,Y</sub>(x,y) / p<sub>Y</sub>(y) = p<sub>X</sub>(x) * p<sub>Y|X</sub>(y|x) / p<sub>Y</sub>(y).  This could be used in the example of a plane being overhead or not and a radar device that measures if a plane is overhead.  You could say there is a 10% chance of there actually being a plane overhead P(X), and the radar device is wrong 10% of the time P(Y|X), and use bayes rule to find the probability there is a plane overhead given a positive measurement P(X|Y) is (.1 * .9)/(.1*.9 + .9*.1) = 0.5.  To get the denominator, it is useful to know that p<sub>Y</sub>(y) = sum over all x of p<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x).</li>
            <li><strong>X and Y Continuous</strong> This is analagous to the discrete case, where we just replace PMFs with PDFs and sums with integrals.  So we have f<sub>X|Y</sub>(x|y) = f<sub>X,Y</sub>(x,y) / f<sub>Y</sub>(y) = f<sub>X</sub>(x) * f<sub>Y|X</sub>(y|x) / f<sub>Y</sub>(y).  And we may not be given the denominator but we can get it by f<sub>Y</sub>(y) = integral over all x of f<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x). As an example, you might have a current through a resistor represented by X, and a measurement of that current using a device that has some gaussian noise represented by Y.  You get a measurement y, and you want a probability distribution for what X is, the actual current through the resistor.</li>
            <li><strong>X Discrete, Y Continuous</strong> For example, X may be a bit that is sent, either a 0 or 1.  Y is your measurement of that bit and may equal X plus some random gaussian noise W, Y = X+W.  You have a belief about the PMF of X, say 0 or 1 is equally likely, and you have a model for your measurement device, i.e. the conditional PDF of Y given X.  For instance the conditional PDF of Y given X might be the normal distribution centered around X.  To get the conditional PMF of X given Y (the inference), we note that P(X=x, y≤Y≤y+δ) = P(X=x)P(y≤Y≤y+δ|X=x) = P(y≤Y≤y+δ)P(X=x|y≤Y≤y+δ).  (Here we're just thinking in terms of events in the sample space, specifically the event i.e. all the outcomes where X=x, and the event i.e. all the outcomes where y≤Y≤y+δ).  The probability that y≤Y≤y+δ is given by f<sub>Y</sub>(y)*δ, in the limit that δ approaches 0, and we can condition this on X being some value. From this we can get p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x)*δ = f<sub>Y</sub>(y)*δ * p<sub>X|Y</sub>(x,y).  So p<sub>X|Y</sub>(x,y) = (p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x)) / f<sub>Y</sub>(y).  Here again you may not know the denominator, but it can be found with f<sub>Y</sub>(y) = Sum over all x of p<sub>X</sub>(x)*f<sub>Y|X</sub>(y|x). The intuition is the same as the previous cases, just keep in mind you are dealing with probability densities with f, where you get a measurement Y=y, then for every value of X, you need to know the value of the PDF at y for that x.  If you add all that up, you get the denominator, and your numerator can be the PDF value of your y for a particular x of interest.  Its weird because the PDF isn't giving you actual probabilities, it's giving you densities, but all you need is the ratio of the densities to get the probabilities of x that you need (the density units cancel out in the ratio).</li>
            <li><strong>X Continuous, Y Discrete</strong> For example, X may be the magnitude of the current (continuous) through a device that emits photons and Y is a count of the number of photons emitted per second (discrete).  The inference equation you need is gotten to by the same logic as the previous case, just with the roles reversed.  So, f<sub>X|Y</sub>(x,y) = (f<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x)) / p<sub>Y</sub>(y). Where p<sub>Y</sub>(y) = integral over all x of f<sub>X</sub>(x)*p<sub>Y|X</sub>(y|x)dx. </li>
        </ul>
    </ul>

    <h2>Limit Theorems</h2>
    <ul>
        <li><strong>Sample Mean</strong> Suppose you have a large population of penguins and want to know their average height. That is, if the experiment is picking one penguin with uniform probability of which penguin you pick and you have a random variable X for that pick's height, what is E[X]? If you could measure the height of every single penguin in the population, then you could take the average, and that would be equal to E[X]. However, this may not be feasible so you do in experiment where you pick n random penguins, so your sample space is all combinations of n penguins, and the height of each penguin in the n is a random variable X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>.  Then you define a random variable called the Sample Mean, that is the average of these n random variables, and you want to know if this is a good representation of the average height for the whole population.   You can imagine as n gets large you would be approaching the entire population size.</li>
        <li><strong>Markov Inequality</strong> Assume you have a discrete random variable X that is always non-negative, x≥0 for all x.  Then E[X] = Sum over x of x*p<sub>X</sub>(x) is a sum where each term is nonnegative, since x and p<sub>X</sub>(x) are both greater than or equal to zero for every x.  Because of this, if you restrict the values of x you sum over, then the restricted sum removes nonnegative terms from the sum over all x, so the sum over the restricted terms will be less than or equal to the sum over all x.  Specifically, if you restrict x by summing up terms only where x is greater than a constant a>0, then Sum over x of x*p<sub>X</sub>(x) ≥ Sum over x≥a of x*p<sub>X</sub>(x).  Furthermore, if x≥a, then since p<sub>X</sub>(x)≥0, multiplying both sides by it gives you x*p<sub>X</sub>(x)≥a*p<sub>X</sub>(x). Thus, Sum over x≥a of x*p<sub>X</sub>(x) ≥ Sum over x≥a of a*p<sub>X</sub>(x), since every term on the right side has a corresponding term on the left side that is greater than or equal to it.  Furthermore,  Sum over x≥a of a*p<sub>X</sub>(x) = a * Sum over x≥a of p<sub>X</sub>(x) = a * P(X≥a) since a is constant and the sum over probabilities is the total probability of all x≥a.  Altogether we have E[X] ≥ a * P(X≥a), and thus P(X≥a) ≤ E[X]/a.  The important point here is that if E[X] is small, then for large values of a, the probability that X≥a must be small.  Consider all the terms involved in calculating E[X], and the portion of those terms that have large values of x (the portion with some lower bound a).  Well, those terms contribute some small amount to the final value of E[X] (since E[X] is made up of many more terms than just those with lower bound a).  This inequality tells us something about the maximum collective probability those terms greater than a can have.  It can be at most E[X]/a, which means if a is n multiples of E[X], then 1/n ≥ P(X≥a), i.e. the probability that x is greater than a is at most 1 over the number of multiples a is of E[X]. If the probability was any greater than that, then E[X] would have to be bigger.  Another helpful way to think of it, if you write a and each x in terms of multiples of the expected value, cE[X], and say you choose a as 2E[X], then you restrict the terms in the formula for E[X] to just those where x≥2E[X] (e.g. 2.1E[X], 2.4E[X], etc.).  The sum of these restricted terms must be less than E[X] since E[X] includes all those terms and more (and all terms are positive).  Furthermore, if you replace x in each term with a=2E[X], then this sum is less than or equal to the original restricted sum since every x value is at least as big as a.  Then you can factor out a, and you're left with a times the sum of probabilities of all x's greater than a, or 2E[X]*P(X≥2E[X]).  So you know E[X]≥2E[X]*P(X≥2E[X]).  So, at absolute most P(X>2E[X]) could be 1/2.  If it was any bigger, say 3/4, then values greater than or equal to 2E[X] would alone add up to at least 2E[X]*3/4 = 3/2*E[X] in the formula for expected value, so there's no way the expected value could be E[X].</li>
        <li><strong>Chebyshev's Inequality</strong> This is just an application of Markov's inequality using a function of a random variable X, which is a random variable itself: (x-E[X])^2. We know that E[(x-E[X])^2] / a ≥ P((x-E[X])^2 ≥ a), by Markov's inequality for a constant a>0.  Say sqrt(a) = c, so a=c^2, and c>0.  So, E[(x-E[X])^2] / c^2 ≥ P((x-E[X])^2 ≥ c^2).  As usual this just says that the maximum probability that (x-E[X])^2 is greater than some constant c^2, is 1 over the number of multiples c^2 is of the expected value of (x-E[X])^2.  If the probability was any more than this, than those values above c^2 would alone make for a greater expected value than E[(x-E[X])^2]. This takes on new meaning when you see that P((x-E[X])^2 ≥ c^2) = P(abs(x-E[X]) ≥ c) (the probability of the left hand side is the same as the probability of the right hand side) and E[(x-E[X])^2] = var(X).  So, var(X)/c^2 ≥ P(abs(x-E[X]) ≥ c).  This means, when looking at the distribution of the random variable X, the probability that a value is farther than c from the mean of X, is 1 over however many multiples of var(X) that c^2 is.  If the probability was any greater than that, then they would create a greater variance on their own than the variance we actually have.  A more helpful form of the inequality would be to use a constant k such that k = c/σ, so c=kσ, where σ^2=var(X).  Then the inequality becomes 1/k^2 ≥ P(abs(x-E[X]) ≥ kσ), showing that the probability of being k multiples of the standard deviation away from the mean is at most 1/k^2.  It helps to assume var(X) is 1 (and thus standard deviation is 1) to visualize this, so the total probability of X values that are 2 standard deviations away from the mean of X is at most 1/2^2 = 1/4.  The total probability of X values that are 3 standard deviations away from the mean of X is at most 1/3^2 = 1/9, and so on.  All of this holds in the continuous case as well, starting with σ^2 = integral of (x-E[X])^2*f<sub>X</sub>(x)dx ≥ integral from neg infinity to E[X]-c of (x-E[X])^2*f<sub>X</sub>(x)dx + integral from E[X]+c to infinity of (x-E[X])^2*f<sub>X</sub>(x)dx ≥ integral from neg infinity to E[X]-c of (E[X]-c-E[X])^2*f<sub>X</sub>(x)dx + integral from E[X]+c to infinity of (E[X]+c-E[X])^2*f<sub>X</sub>(x)dx = c^2 * integral from neg infinity to E[X]-c of f<sub>X</sub>(x)dx + c^2 * integral from E[X]+c to infinity of f<sub>X</sub>(x)dx = c^2 * P(|x-E[X]|≥c).</li>
    </ul>

    <h2>Sampling Distributions</h2>
    <ul>
        <li>We often cannot calculate statistics for an entire population, and thus calculate a statistic from a randomly selected sample of the population, and want to know how certain we can be that we know the true statistic for the entire population.</li>
        <li><strong>Sampling Distribution</strong> If you take a statistic like the mean, median, the min or max value, variance, etc. for a sample of the population, across many samples, you will create a probability distbution for that value which is called a sampling distribution.</li>
        <li><strong>Unbiased and Biased Estimators</strong> If the mean of a sampling distribution for some statistic is equal (approximately) to the actual value of that statistic for the whole population, then the sample statistic is an unbiased estimator.  This is true for the mean of a sample/population.  If the mean of a sampling distribution for some statistic is not centered around the actual value fo that statistic for the whole population, then it is a biased estimator.  This is true for the max and min value of a sample/population.</li>
        <li><strong>Central Limit Theorem</strong> This states that the sampling distribution of the mean for a population will be normally distributed as long as the population is not too skewed, or the sample size is large enough (at least 30 is a good rule of thumb).  You can have a smaller sample size if the population is normally distributed and the central limit theorem will still hold.  CLT says 1) The sampling distribution of the mean will have a mean x that will be close to the true population mean μ. And 2) The sampling distribution of the mean will have standard deviation equal to the population standard deviation over the square root of the sample size σ / sqrt(n). The standard deviation of a sampling distribution is called the standard error of the estimate of the mean. We may not know the population's standard deviation so we often use the standard deviation of a sample in its place.</li>
        <li><strong>Confidence Intervals</strong> Usually we only have one sample, not many that we can make a sampling distribution with.  We want to say how sure we are that the statistic for that sample that we're interested in is close to the actual population statistic.  We can use the standard deviation of the sample as an estimate for the standard deviation of that population.  Then per CLT, we can calculate the standard error of our sample, using the estimate of the standard deviation divided by the square root of the sample size.  Also using CLT, we know that the sampling distribution is normally distributed and centered on the actual population mean. Since a normal distribution has 95% of its values fall with 1.96 standard deviations of the mean, we know that 95% of sample statistics will thus be less than 1.96 * standard error away from the actual mean.  Thus using our standard error estimate and the mean of our one sample, we can say that the mean of our sample is within 1.96 * standard error of the actual population mean.  This interval is called a 95% confidence interval.</li>
        <li>Python's numpy library has a convenient function for creating a random sample from a population.  It is random.choice(population array, sample size, replace), where pop array is an array to choose from, sample size is an integer, and replace should be FALSE because once an item is added to the sample it should not be able to be picked again.  It returns a list of the sample.</li>
    </ul>
</body>
</html>